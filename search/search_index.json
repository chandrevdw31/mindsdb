{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","social":[{"link":"https://github.com/mindsdb/mindsdb","type":"github"},{"link":"https://twitter.com/mindsdb","type":"twitter"},{"link":"https://www.mindsdb.com","type":"link"}]},"docs":[{"location":"","text":"What is MindsDB? Data is your single most important asset and your data lives in a database. By bringing machine learning to the database, MindsDB accelerates the speed of machine learning development. With MindsDB, you will easily build, train, optimize and deploy your models. Your predictions will be available via the queries you already use.","title":"What is MindsDB"},{"location":"#what-is-mindsdb","text":"Data is your single most important asset and your data lives in a database. By bringing machine learning to the database, MindsDB accelerates the speed of machine learning development. With MindsDB, you will easily build, train, optimize and deploy your models. Your predictions will be available via the queries you already use.","title":"What is MindsDB?"},{"location":"community/","text":"Join our community If you have questions or you want to chat with the MindsDB core team or other community members, you can join our Slack workspace . MindsDB newsletter To get updates on MindsDB\u2019s latest announcements, releases and events, sign up for our newsletter . Become a MindsDB Beta tester If you want to become a part of our product and get first access to all of our latest updates, join our Beta testers community . Talk to our engineers If you want to use MindsDB or you have more questions, you can schedule a call by clicking on Talk to our Engineers button. Get in touch for collaboration Contact us by submitting this form .","title":"Join our community"},{"location":"community/#join-our-community","text":"If you have questions or you want to chat with the MindsDB core team or other community members, you can join our Slack workspace .","title":"Join our community"},{"location":"community/#mindsdb-newsletter","text":"To get updates on MindsDB\u2019s latest announcements, releases and events, sign up for our newsletter .","title":"MindsDB newsletter"},{"location":"community/#become-a-mindsdb-beta-tester","text":"If you want to become a part of our product and get first access to all of our latest updates, join our Beta testers community .","title":"Become a MindsDB Beta tester"},{"location":"community/#talk-to-our-engineers","text":"If you want to use MindsDB or you have more questions, you can schedule a call by clicking on Talk to our Engineers button.","title":"Talk to our engineers"},{"location":"community/#get-in-touch-for-collaboration","text":"Contact us by submitting this form .","title":"Get in touch for collaboration"},{"location":"connect/","text":"Connect your data Connect to database From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select the Supported Database that you want to connect to. Add the Integration name(how you want to name the integration between your database and MindsDB). Add the Database name. Add the Hostname. Add Port. Add the database user. Add Password for the user. Click on CONNECT . Required inputs Note: For different type of database there could be different required inputs you need to provide. After connecting MindsDB and the database, use your SQL client to connect to MindsDB as a database .","title":"Plug your data"},{"location":"connect/#connect-your-data","text":"","title":"Connect your data"},{"location":"connect/#connect-to-database","text":"From the left navigation menu, select Database Integration. Click on the ADD DATABASE button. In the Connect to Database modal window: Select the Supported Database that you want to connect to. Add the Integration name(how you want to name the integration between your database and MindsDB). Add the Database name. Add the Hostname. Add Port. Add the database user. Add Password for the user. Click on CONNECT . Required inputs Note: For different type of database there could be different required inputs you need to provide. After connecting MindsDB and the database, use your SQL client to connect to MindsDB as a database .","title":"Connect to database"},{"location":"contribute/","text":"Contribute to MindsDB Thank you for your interest in contributing to MindsDB. MindsDB is free, open source software, and all types of contributions are welcome, whether they\u2019re documentation changes, bug reports, bug fixes or new source code changes. Report issue We use GitHub issues to track bugs and features. Report them by opening a new issue and complete out all of the required inputs: Your Python version , MindsDB version , Describe the bug and Steps to reproduce . Documentation We are always trying to improve our documentation. All Pull Requests that improve our grammar or docs structure or fix typos are welcomed. Check the MindsDB Docs repository and help us. Easy contribution to issues Most of the issues that are open for contributions will be tagged with good-first-issue or help-wanted . After you find the issue that you want to contribute to, follow the fork-and-pull workflow: Fork the MindsDB repository Clone the repository locally Make changes and commit them Push your local branch to your fork Submit a Pull Request so that we can review your changes Write a commit message Make sure that the CI tests are GREEN NOTE: Be sure to merge the latest from \"upstream\" before making a Pull Request! Pull Request reviews are done on a regular basis. Please make sure you respond to our feedback/questions and sign our CLA. Write for us Do you find MindsDB useful and want to share your story? Make a PR to this repo with your writing in a markdown file, or just post it on Medium, Dev or your own blog post. We would love to hear from you .","title":"How to contribute"},{"location":"contribute/#contribute-to-mindsdb","text":"Thank you for your interest in contributing to MindsDB. MindsDB is free, open source software, and all types of contributions are welcome, whether they\u2019re documentation changes, bug reports, bug fixes or new source code changes.","title":"Contribute to MindsDB"},{"location":"contribute/#report-issue","text":"We use GitHub issues to track bugs and features. Report them by opening a new issue and complete out all of the required inputs: Your Python version , MindsDB version , Describe the bug and Steps to reproduce .","title":"Report issue"},{"location":"contribute/#documentation","text":"We are always trying to improve our documentation. All Pull Requests that improve our grammar or docs structure or fix typos are welcomed. Check the MindsDB Docs repository and help us.","title":"Documentation"},{"location":"contribute/#easy-contribution-to-issues","text":"Most of the issues that are open for contributions will be tagged with good-first-issue or help-wanted . After you find the issue that you want to contribute to, follow the fork-and-pull workflow: Fork the MindsDB repository Clone the repository locally Make changes and commit them Push your local branch to your fork Submit a Pull Request so that we can review your changes Write a commit message Make sure that the CI tests are GREEN NOTE: Be sure to merge the latest from \"upstream\" before making a Pull Request! Pull Request reviews are done on a regular basis. Please make sure you respond to our feedback/questions and sign our CLA.","title":"Easy contribution to issues"},{"location":"contribute/#write-for-us","text":"Do you find MindsDB useful and want to share your story? Make a PR to this repo with your writing in a markdown file, or just post it on Medium, Dev or your own blog post. We would love to hear from you .","title":"Write for us"},{"location":"faq/","text":"What is the roadmap? The MindsDB roadmap is aimed to be aligned with our goals: Success versions 1.0 MindsDB GUI where you can visualize explainability goals Support for images and complex text MindsDB REST API's Success versions 2.0 Run Machine Learning Models as Tables Important versions 3.0 Auto ETL and Data Preparation What type of data can MindsDB learn and predict from? We support tabular data formats as a CSV, Excel, JSON, text files also pandas data frame, URLs, s3 files. We support the following database integration: SQL NoSQL Streams Data Warehouse - - - - - - - - - - - - - - - - - - - How can I help? You can help in the following ways: Trying MindsDB and reporting issues . If you know python, you can also help us debug open issues. Issues labels with the good first issue tag should be the easiest to start with . You can help us with documentation and examples. Tell your friends, write a blog post about MindsDB. Join our team, we are growing fast so we should have a few open positions . Why is it called MindsDB? Well, as most names, we needed one, we like science fiction and the culture series , where there are these AI super smart entities called Minds. How about the DB part? Although in the future we will support all kinds of data, currently our objective is to add intelligence to existing data stores/databases, hence the term DB. As to becoming a Mind to your DB . Why the bear? Who doesn't like bears! Anyway, a bear for UC Berkeley where this all was initially coded. What is the difference between AI and Machine Learning? What is XAI?","title":"FAQ"},{"location":"faq/#what-is-the-roadmap","text":"The MindsDB roadmap is aimed to be aligned with our goals: Success versions 1.0 MindsDB GUI where you can visualize explainability goals Support for images and complex text MindsDB REST API's Success versions 2.0 Run Machine Learning Models as Tables Important versions 3.0 Auto ETL and Data Preparation","title":"What is the roadmap?"},{"location":"faq/#what-type-of-data-can-mindsdb-learn-and-predict-from","text":"We support tabular data formats as a CSV, Excel, JSON, text files also pandas data frame, URLs, s3 files. We support the following database integration: SQL NoSQL Streams Data Warehouse - - - - - - - - - - - - - - - - - - -","title":"What type of data can MindsDB learn and predict from?"},{"location":"faq/#how-can-i-help","text":"You can help in the following ways: Trying MindsDB and reporting issues . If you know python, you can also help us debug open issues. Issues labels with the good first issue tag should be the easiest to start with . You can help us with documentation and examples. Tell your friends, write a blog post about MindsDB. Join our team, we are growing fast so we should have a few open positions .","title":"How can I help?"},{"location":"faq/#why-is-it-called-mindsdb","text":"Well, as most names, we needed one, we like science fiction and the culture series , where there are these AI super smart entities called Minds. How about the DB part? Although in the future we will support all kinds of data, currently our objective is to add intelligence to existing data stores/databases, hence the term DB. As to becoming a Mind to your DB . Why the bear? Who doesn't like bears! Anyway, a bear for UC Berkeley where this all was initially coded.","title":"Why is it called MindsDB?"},{"location":"faq/#what-is-the-difference-between-ai-and-machine-learning","text":"","title":"What is the difference between AI and Machine Learning?"},{"location":"faq/#what-is-xai","text":"","title":"What is XAI?"},{"location":"info/","text":"Follow the below steps to get up and running with MindsDB. Steps: Deploy MindsDB: Docker OR Pip OR MindsDB Cloud Connect your Data to MindsDB: Connect to MindsDB like it was a Database using your preferred DB Client Using MindsDB Cloud OR Using a Local Deployment CREATE a predictor (model) trained on your data Make predictions using your model","title":"Getting Started"},{"location":"info/#steps","text":"Deploy MindsDB: Docker OR Pip OR MindsDB Cloud Connect your Data to MindsDB: Connect to MindsDB like it was a Database using your preferred DB Client Using MindsDB Cloud OR Using a Local Deployment CREATE a predictor (model) trained on your data Make predictions using your model","title":"Steps:"},{"location":"quickstart/","text":"Follow these steps to start predicting with MindsDB straight away. Create MindsDB Cloud Account Create your free MindsDB Cloud account . Note To proceed with a local installation, follow our Docker instructions . Connect SQL Client Open your SQL client and connect to MindsDB using the email and password you used to sign up for MindsDB Cloud. Note If you do not already have a preferred SQL client, we recommend DBeaver Community Edition Connect Example Data We have already prepared some example data for you. To add it to your account, use the CREATE DATASOURCE syntax by copy and pasting this command into your SQL client: CREATE DATASOURCE exampleData WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } #create-datasource code { background-color: #353535; color: #f5f5f5 } mysql> CREATE DATASOURCE exampleData -> WITH ENGINE = \"postgres\", -> PARAMETERS = { -> \"user\": \"demo_user\", -> \"password\": \"demo_password\", -> \"host\": \"3.220.66.106\", -> \"port\": \"5432\", -> \"database\": \"demo\" -> } Query OK, 0 rows affected (3.22 sec) Preview Data You can now preview the available data with a standard SELECT . To preview the Home Rentals dataset, copy and paste this command into your SQL client: SELECT * FROM exampleData.demo_data.home_rentals LIMIT 10; #preview-data code { background-color: #353535; color: #f5f5f5 } mysql> SELECT * -> FROM exampleData.demo_data.home_rentals -> LIMIT 10; +-----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | +-----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | 0.0 | 1.0 | 484 | great | 10 | 2271 | south_side | 2271 | | 1.0 | 1.0 | 674 | good | 1 | 2167 | downtown | 2167 | | 1.0 | 1.0 | 554 | poor | 19 | 1883 | westbrae | 1883 | | 0.0 | 1.0 | 529 | great | 3 | 2431 | south_side | 2431 | | 3.0 | 2.0 | 1219 | great | 3 | 5510 | south_side | 5510 | | 1.0 | 1.0 | 398 | great | 11 | 2272 | south_side | 2272 | | 3.0 | 2.0 | 1190 | poor | 58 | 4463 | westbrae | 4124 | | 1.0 | 1.0 | 730 | good | 0 | 2224 | downtown | 2224 | | 0.0 | 1.0 | 298 | great | 9 | 2104 | south_side | 2104 | | 2.0 | 1.0 | 878 | great | 8 | 3861 | south_side | 3861 | +-----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ 10 rows in set (0.36 sec) Create Predictor Now you are ready to create your first predictor. Use the CREATE PREDICTOR syntax by copy and pasting this command into your SQL client: CREATE PREDICTOR homeRentalsPredictor FROM exampleData (SELECT * FROM demo_data.home_rentals) PREDICT rental_price; #create-predictor code { background-color: #353535; color: #f5f5f5 } mysql> CREATE PREDICTOR homeRentalsPredictor -> FROM exampleData -> (SELECT * FROM demo_data.home_rentals) -> PREDICT rental_price; Query OK, 0 rows affected (9.79 sec) Predictor Status It may take a couple of minutes for training to complete. You can monitor the status of your predictor by copy and pasting this command into your SQL client: SELECT status FROM predictors WHERE name='homeRentalsPredictor'; #predictor-status code { background-color: #353535; color: #f5f5f5 } mysql> SELECT status -> FROM predictors -> WHERE name='homeRentalsPredictor'; +----------+ | status | +----------+ | training | +----------+ 1 row in set (0.19 sec) ... mysql> SELECT status -> FROM predictors -> WHERE name='homeRentalsPredictor'; +----------+ | status | +----------+ | complete | +----------+ 1 row in set (0.31 sec) You should see a status of generating initially, then training , followed by complete once it is done. Make Prediction The SELECT syntax will allow you to make a prediction based on features. Make your first prediction by copy and pasting this command into your SQL client: SELECT rental_price FROM homeRentalsPredictor WHERE number_of_bathrooms=2 AND sqft=1000; #make-prediction code { background-color: #353535; color: #f5f5f5 } mysql> SELECT rental_price -> FROM homeRentalsPredictor -> WHERE number_of_bathrooms=2 AND sqft=1000; +--------------+ | rental_price | +--------------+ | 1130 | +--------------+ 1 row in set (0.38 sec)","title":"Quickstart"},{"location":"quickstart/#create-mindsdb-cloud-account","text":"Create your free MindsDB Cloud account . Note To proceed with a local installation, follow our Docker instructions .","title":"Create MindsDB Cloud Account"},{"location":"quickstart/#connect-sql-client","text":"Open your SQL client and connect to MindsDB using the email and password you used to sign up for MindsDB Cloud. Note If you do not already have a preferred SQL client, we recommend DBeaver Community Edition","title":"Connect SQL Client"},{"location":"quickstart/#connect-example-data","text":"We have already prepared some example data for you. To add it to your account, use the CREATE DATASOURCE syntax by copy and pasting this command into your SQL client: CREATE DATASOURCE exampleData WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"3.220.66.106\" , \"port\" : \"5432\" , \"database\" : \"demo\" } #create-datasource code { background-color: #353535; color: #f5f5f5 } mysql> CREATE DATASOURCE exampleData -> WITH ENGINE = \"postgres\", -> PARAMETERS = { -> \"user\": \"demo_user\", -> \"password\": \"demo_password\", -> \"host\": \"3.220.66.106\", -> \"port\": \"5432\", -> \"database\": \"demo\" -> } Query OK, 0 rows affected (3.22 sec)","title":"Connect Example Data"},{"location":"quickstart/#preview-data","text":"You can now preview the available data with a standard SELECT . To preview the Home Rentals dataset, copy and paste this command into your SQL client: SELECT * FROM exampleData.demo_data.home_rentals LIMIT 10; #preview-data code { background-color: #353535; color: #f5f5f5 } mysql> SELECT * -> FROM exampleData.demo_data.home_rentals -> LIMIT 10; +-----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | +-----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ | 0.0 | 1.0 | 484 | great | 10 | 2271 | south_side | 2271 | | 1.0 | 1.0 | 674 | good | 1 | 2167 | downtown | 2167 | | 1.0 | 1.0 | 554 | poor | 19 | 1883 | westbrae | 1883 | | 0.0 | 1.0 | 529 | great | 3 | 2431 | south_side | 2431 | | 3.0 | 2.0 | 1219 | great | 3 | 5510 | south_side | 5510 | | 1.0 | 1.0 | 398 | great | 11 | 2272 | south_side | 2272 | | 3.0 | 2.0 | 1190 | poor | 58 | 4463 | westbrae | 4124 | | 1.0 | 1.0 | 730 | good | 0 | 2224 | downtown | 2224 | | 0.0 | 1.0 | 298 | great | 9 | 2104 | south_side | 2104 | | 2.0 | 1.0 | 878 | great | 8 | 3861 | south_side | 3861 | +-----------------+---------------------+------+----------+----------------+---------------+--------------+--------------+ 10 rows in set (0.36 sec)","title":"Preview Data"},{"location":"quickstart/#create-predictor","text":"Now you are ready to create your first predictor. Use the CREATE PREDICTOR syntax by copy and pasting this command into your SQL client: CREATE PREDICTOR homeRentalsPredictor FROM exampleData (SELECT * FROM demo_data.home_rentals) PREDICT rental_price; #create-predictor code { background-color: #353535; color: #f5f5f5 } mysql> CREATE PREDICTOR homeRentalsPredictor -> FROM exampleData -> (SELECT * FROM demo_data.home_rentals) -> PREDICT rental_price; Query OK, 0 rows affected (9.79 sec)","title":"Create Predictor"},{"location":"quickstart/#predictor-status","text":"It may take a couple of minutes for training to complete. You can monitor the status of your predictor by copy and pasting this command into your SQL client: SELECT status FROM predictors WHERE name='homeRentalsPredictor'; #predictor-status code { background-color: #353535; color: #f5f5f5 } mysql> SELECT status -> FROM predictors -> WHERE name='homeRentalsPredictor'; +----------+ | status | +----------+ | training | +----------+ 1 row in set (0.19 sec) ... mysql> SELECT status -> FROM predictors -> WHERE name='homeRentalsPredictor'; +----------+ | status | +----------+ | complete | +----------+ 1 row in set (0.31 sec) You should see a status of generating initially, then training , followed by complete once it is done.","title":"Predictor Status"},{"location":"quickstart/#make-prediction","text":"The SELECT syntax will allow you to make a prediction based on features. Make your first prediction by copy and pasting this command into your SQL client: SELECT rental_price FROM homeRentalsPredictor WHERE number_of_bathrooms=2 AND sqft=1000; #make-prediction code { background-color: #353535; color: #f5f5f5 } mysql> SELECT rental_price -> FROM homeRentalsPredictor -> WHERE number_of_bathrooms=2 AND sqft=1000; +--------------+ | rental_price | +--------------+ | 1130 | +--------------+ 1 row in set (0.38 sec)","title":"Make Prediction"},{"location":"deployment/cloud/","text":"MindsDB Cloud For a quick jumpstart into using MindsDB you can sign up for our Cloud. MindsDB Cloud is hosted by the MindsDB team and has all of the latest updates. You can start using it immediately for free by following these steps: Create an account by visiting the cloud.mindsdb.com/signup and filling out the sign-up form. After sign up, you will get a confirmation email to validate your account. Once your account is validated, you can login to MindsDB Cloud. Now, you are ready to use MindsDB Cloud.","title":"MindsDB Cloud"},{"location":"deployment/cloud/#mindsdb-cloud","text":"For a quick jumpstart into using MindsDB you can sign up for our Cloud. MindsDB Cloud is hosted by the MindsDB team and has all of the latest updates. You can start using it immediately for free by following these steps: Create an account by visiting the cloud.mindsdb.com/signup and filling out the sign-up form. After sign up, you will get a confirmation email to validate your account. Once your account is validated, you can login to MindsDB Cloud. Now, you are ready to use MindsDB Cloud.","title":"MindsDB Cloud"},{"location":"deployment/docker/","text":"Deploy using Docker To use MindsDB's Docker Container you first need to have Docker installed on your machine. To make sure Docker is successfully installed on your machine, run: docker run hello-world You should see the Hello from Docker! message displayed. If not, check the Get Started documentation. MindsDB container MindsDB images are uploaded to the MindsDB repo on docker hub after each release. Pull image First, run the below command to pull our latest production image: docker pull mindsdb/mindsdb Or, to try out the latest beta version, pull the beta image: docker pull mindsdb/mindsdb_beta Start container Next, run the below command to start the container: docker run -p 47334:47334 -p 47335:47335 mindsdb/mindsdb That's it. MindsDB GUI should be avaiable on http://127.0.0.1:47334/ on your default browser. Troubleshooting Extend config.json If you want to extend the default configuration, you will be able to send the config.json value as JSON string argument to the MDB_CONFIG_CONTENT as: docker run -e MDB_CONFIG_CONTENT='{\"api\":{\"http\": {\"host\": \"0.0.0.0\",\"port\": \"47334\"}}}' mindsdb/mindsdb Or, you can pipe < the content of the file to the MDB_CONFIG_CONTENT. MKL Issues Note. If you experience any issue related to MKL or if your training process does not complete, please add env var or starter Docker with this command: docker run --env MKL_SERVICE_FORCE_INTEL=1 -it -p 47334:47334 mindsdb/mindsdb Docker for Mac users By default, Docker for Mac allocates 2.00GB of memory . This is insufficient for deploying to docker. We recommend increasing the default memory limit to 4.00GB . Please refer to https://docs.docker.com/desktop/mac/#resources for more information on how to increase the allocated memory.","title":"Docker"},{"location":"deployment/docker/#deploy-using-docker","text":"To use MindsDB's Docker Container you first need to have Docker installed on your machine. To make sure Docker is successfully installed on your machine, run: docker run hello-world You should see the Hello from Docker! message displayed. If not, check the Get Started documentation.","title":"Deploy using Docker"},{"location":"deployment/docker/#mindsdb-container","text":"MindsDB images are uploaded to the MindsDB repo on docker hub after each release.","title":"MindsDB container"},{"location":"deployment/docker/#pull-image","text":"First, run the below command to pull our latest production image: docker pull mindsdb/mindsdb Or, to try out the latest beta version, pull the beta image: docker pull mindsdb/mindsdb_beta","title":"Pull image"},{"location":"deployment/docker/#start-container","text":"Next, run the below command to start the container: docker run -p 47334:47334 -p 47335:47335 mindsdb/mindsdb That's it. MindsDB GUI should be avaiable on http://127.0.0.1:47334/ on your default browser.","title":"Start container"},{"location":"deployment/docker/#_1","text":"","title":""},{"location":"deployment/docker/#troubleshooting","text":"Extend config.json If you want to extend the default configuration, you will be able to send the config.json value as JSON string argument to the MDB_CONFIG_CONTENT as: docker run -e MDB_CONFIG_CONTENT='{\"api\":{\"http\": {\"host\": \"0.0.0.0\",\"port\": \"47334\"}}}' mindsdb/mindsdb Or, you can pipe < the content of the file to the MDB_CONFIG_CONTENT. MKL Issues Note. If you experience any issue related to MKL or if your training process does not complete, please add env var or starter Docker with this command: docker run --env MKL_SERVICE_FORCE_INTEL=1 -it -p 47334:47334 mindsdb/mindsdb Docker for Mac users By default, Docker for Mac allocates 2.00GB of memory . This is insufficient for deploying to docker. We recommend increasing the default memory limit to 4.00GB . Please refer to https://docs.docker.com/desktop/mac/#resources for more information on how to increase the allocated memory.","title":"Troubleshooting"},{"location":"deployment/linux/","text":"Deploy using pip Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version is >=19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages: Deploy using Anaconda Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list Troubleshooting If the installation fails, don't worry, simply follow the below below instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated. Installation fail Note that Python 64 bit version is required. IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies. Installation fail If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon.","title":"Deploy using pip"},{"location":"deployment/linux/#deploy-using-pip","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version is >=19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages:","title":"Deploy using pip"},{"location":"deployment/linux/#deploy-using-anaconda","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list","title":"Deploy using Anaconda"},{"location":"deployment/linux/#troubleshooting","text":"If the installation fails, don't worry, simply follow the below below instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated. Installation fail Note that Python 64 bit version is required. IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies. Installation fail If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon.","title":"Troubleshooting"},{"location":"deployment/macos/","text":"Deploy using pip Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version >= 19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed, run: pip freeze You should see a list with the names of installed packages: Deploy using Anaconda Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and a Python 64bit version. Then open the Anaconda prompt and: Create a new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed, run: conda list You should see a list with the names of installed packages. Troubleshooting If the installation fails, don't worry, simply follow the below below instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. Please downgrade to an older version of Python for now 3.7.x or 3.8.x . We are working on this, and Python 3.9 will be supported soon. Installation fail Note that Python 64 bit version is required. Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt . No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated. IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies.","title":"Deploy using pip"},{"location":"deployment/macos/#deploy-using-pip","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version >= 19.3 . Create and activate venv: python -m venv mindsdb source mindsdb/bin/activate Install MindsDB: pip install mindsdb To verify that mindsdb was installed, run: pip freeze You should see a list with the names of installed packages:","title":"Deploy using pip"},{"location":"deployment/macos/#deploy-using-anaconda","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and a Python 64bit version. Then open the Anaconda prompt and: Create a new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed, run: conda list You should see a list with the names of installed packages.","title":"Deploy using Anaconda"},{"location":"deployment/macos/#troubleshooting","text":"If the installation fails, don't worry, simply follow the below below instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. numpy.distutils.system_info.NotFoundError: No lapack/blas resources found. Note: Accelerate is no longer supported. Please downgrade to an older version of Python for now 3.7.x or 3.8.x . We are working on this, and Python 3.9 will be supported soon. Installation fail Note that Python 64 bit version is required. Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt . No module named mindsdb If you get this error, make sure that your virtual environment (where you installed mindsdb) is activated. IOError: [Errno 28] No space left on device while installing MindsDB MindsDB requires around 3GB of free disk space to install all of its dependencies.","title":"Troubleshooting"},{"location":"deployment/pypi/","text":"Deploy from PyPi The recommended way is to always install inside a virtual environment when using pip . The venv provides an isolated Python environment, which is more practical than installing MindsDB systemwide. Follow below instructions depending on your operating system: Linux Windows MacOS Source code","title":"Pip"},{"location":"deployment/pypi/#deploy-from-pypi","text":"The recommended way is to always install inside a virtual environment when using pip . The venv provides an isolated Python environment, which is more practical than installing MindsDB systemwide. Follow below instructions depending on your operating system: Linux Windows MacOS Source code","title":"Deploy from PyPi"},{"location":"deployment/source/","text":"Deploy from the source code This section describes how to deploy MindsDB from the source code. This is the preferred way to use MindsDB if you want to contribute to our code or simply to debug MindsDB. Prerequisite Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. Python version >=3.7 (64 bit) and pip version >= 19.3. Pip (is usually pre-installed with the latest Python versions). Git . Installation We recommend installing MindsDB inside a virtual environment to avoid dependency issues. Clone the repository: git clone git@github.com:mindsdb/mindsdb.git Create a virtual environment and activate it: python3 -m venv mindsdb-venv source mindsdb-venv/bin/activate Install MindsDB prerequisites: cd mindsdb && pip install -r requirements.txt Install MindsDB: python setup.py develop You're done! To check if everything works, start the MindsDB server: python -m mindsdb To access MindsDB APIs, visit http://127.0.0.1:47334/api . To access MindsDB Studio, visit http://127.0.0.1:47334/ To access MindsDB Studio using mysql: mysql -h 127.0.0.1 --port 3306 -u mindsdb -p Installation troubleshooting No module named mindsdb If you get this error, make sure that your virtual environment is activated. ImportError: No module named {dependency name} This type of error can occur if you skipped the 3rd step. Make sure that you install all of the MindsDB requirements. This site can\u2019t be reached. 127.0.0.1 refused to connect. Please check the MindsDB server console in case the server is still in the starting phase. If the server has started and there is an error displayed, please report it on our GitHub .","title":"Deploy from the source code"},{"location":"deployment/source/#deploy-from-the-source-code","text":"This section describes how to deploy MindsDB from the source code. This is the preferred way to use MindsDB if you want to contribute to our code or simply to debug MindsDB.","title":"Deploy from the source code"},{"location":"deployment/source/#prerequisite","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. Python version >=3.7 (64 bit) and pip version >= 19.3. Pip (is usually pre-installed with the latest Python versions). Git .","title":"Prerequisite"},{"location":"deployment/source/#installation","text":"We recommend installing MindsDB inside a virtual environment to avoid dependency issues. Clone the repository: git clone git@github.com:mindsdb/mindsdb.git Create a virtual environment and activate it: python3 -m venv mindsdb-venv source mindsdb-venv/bin/activate Install MindsDB prerequisites: cd mindsdb && pip install -r requirements.txt Install MindsDB: python setup.py develop You're done! To check if everything works, start the MindsDB server: python -m mindsdb To access MindsDB APIs, visit http://127.0.0.1:47334/api . To access MindsDB Studio, visit http://127.0.0.1:47334/ To access MindsDB Studio using mysql: mysql -h 127.0.0.1 --port 3306 -u mindsdb -p","title":"Installation"},{"location":"deployment/source/#installation-troubleshooting","text":"No module named mindsdb If you get this error, make sure that your virtual environment is activated. ImportError: No module named {dependency name} This type of error can occur if you skipped the 3rd step. Make sure that you install all of the MindsDB requirements. This site can\u2019t be reached. 127.0.0.1 refused to connect. Please check the MindsDB server console in case the server is still in the starting phase. If the server has started and there is an error displayed, please report it on our GitHub .","title":"Installation troubleshooting"},{"location":"deployment/windows/","text":"Deploy using Anaconda Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages. Deploy using pip Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version >= 19.3 . Create new virtual environment called mindsdb: py -m venv mindsdb And, activate it: .\\mindsdb\\Scripts\\activate.bat Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages. Troubleshooting If the installation fails, don't worry, simply follow the below bellow instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. Installation fail Note that Python 64 bit version is required. pip command not found fail Depending on your environment, you might have to use pip3 instead of pip , and python3.x instead of py in the above commands e.g pip3 install mindsdb Installation fail If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. Installation fail If installation fails when installing torch or torchvision try manually installing them following the simple instructions on their official website . Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt .","title":"Deploy using Anaconda"},{"location":"deployment/windows/#deploy-using-anaconda","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. You will need Anaconda or Conda installed and Python 64bit version. Then open Anaconda Prompt and: Create new virtual environment and install mindsdb: conda create -n mindsdb conda activate mindsdb pip install mindsdb To verify that mindsdb was installed run: conda list You should see a list with the names of installed packages.","title":"Deploy using Anaconda"},{"location":"deployment/windows/#deploy-using-pip","text":"Python 3.9 Currently, some of our dependencies have issues with the latest versions of Python 3.9.x. For now, our suggestion is to use Python 3.7.x, or 3.8.x versions. We suggest you to install MindsDB in a virtual environment when using pip to avoid dependency issues. Make sure your Python version is >=3.7 and pip version >= 19.3 . Create new virtual environment called mindsdb: py -m venv mindsdb And, activate it: .\\mindsdb\\Scripts\\activate.bat Install MindsDB: pip install mindsdb To verify that mindsdb was installed run: pip freeze You should see a list with the names of installed packages.","title":"Deploy using pip"},{"location":"deployment/windows/#troubleshooting","text":"If the installation fails, don't worry, simply follow the below bellow instruction which should fix most issues. If none of this works, try using the docker container and create an issue with the installation errors you got on our Github repository . We'll try to review the issue and give you response within a few hours. Installation fail Note that Python 64 bit version is required. pip command not found fail Depending on your environment, you might have to use pip3 instead of pip , and python3.x instead of py in the above commands e.g pip3 install mindsdb Installation fail If you are using Python 3.9 you may get installation errors. Some of the MindsDB's dependencies are not working with Python 3.9 , so please downgrade to older versions for now. We are working on this and Python 3.9 will be supported soon. Installation fail If installation fails when installing torch or torchvision try manually installing them following the simple instructions on their official website . Installation fails because of system dependencies Try installing MindsDB with Anaconda , and run the installation from the anaconda prompt .","title":"Troubleshooting"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/","text":"Forecast Metro Traffic using MindsDB Cloud and MongoDB Atlas In this tutorial, we will be learning to: Connect a MongoDB database to MindsDB. Train a model to predict metro traffic. Get a prediction from the model given certain input parameters. All of that without writing a single line of code and in less than 15 minutes . Yes, you read that right! We will be using the Metro traffic dataset that can be downloaded from here . You are also free to use your own dataset and follow along the tutorial. Pre-requisites This tutorial is primarily going to be about MindsDB so the reader is expected to have some level of familiarity with MongoDB Atlas . In short, MongoDB Atlas is a Database as a Service(DaaS), which we will be using to spin up a MongoDB database cluster and load our dataset. Download a copy of the Metro Traffic dataset from here . You are also expected to have an account on MindsDB Cloud. If not, head over to https://cloud.mindsdb.com/ and create an account. It hardly takes a few minutes. We also need MongoDB Compass to load the dataset into the collection. It can be downloaded from here . Finally, No! You are not required to have any background in programming or machine learning . As mentioned before you wont be writing a single line of code! About MindsDB MindsDB is a predictive platform that makes databases intelligent and machine learning easy to use. It allows data analysts to build and visualize forecasts in BI dashboards without going through the complexity of ML pipelines, all through SQL. It also helps data scientists to streamline MLOps by providing advanced instruments for in-database machine learning and optimize ML workflows through a declarative JSON-AI syntax. Although only SQL is mentioned, MongoDB is also supported. Dataset overview The dataset contains information about the: Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN . More details can be found here . The dataset is a .csv file that contains 9 columns: 1. holiday : Categorical US National holidays plus regional holiday, Minnesota State Fair 2. temp : Numeric Average temp in kelvin 3. rain_1h : Numeric Amount in mm of rain that occurred in the hour 4. snow_1h : Numeric Amount in mm of snow that occurred in the hour 5. clouds_all : Numeric Percentage of cloud cover 6. weather_main : Categorical Short textual description of the current weather 7. weather_description : Categorical Longer textual description of the current weather 8. date_time : DateTime Hour of the data collected in local CST time 9. traffic_volume : Numeric Hourly I-94 ATR 301 reported westbound traffic volume Phew! With all of that out of the way, we can finally get started! Setting up a Cluster on MongoDB Atlas Head over to https://cloud.mongodb.com/ and create a new project named mindsdb and within it a new database cluster named mindsDB . Typically, it takes a minute or two to provision a cluster. Once it is done, you should have something like this: Click on the \"Connect\" button. In the popup modal, you will be asked to add a connection IP address. Although not recommended, for the sake of this tutorial, choose \"Allow access from anywhere\" and then \"Add IP Address\". Next, you will be asked to create a new database user. After providing a username and password, click on the \"Create Database User\" button. In the next step, select \"Connect using MongoDB Compass\". Copy the connection string which should look like this: mongodb+srv://<username>:<password>@mindsdb.htuqc.mongodb.net/ We will now use this connection string to connect to our database from MongoDB Compass and load our dataset. Loading the dataset with MongoDB Compass Open MongoDB Compass. Paste the connection string and click on \"Connect\". On successful authentication, you will be welcomed by this screen. Click on \"Create Database\" and create a database named mindsDB and a collection named data . You will now see mindsDB listed. Click on it and you will see that it contains a collection named data . We will be loading data from the .csv file into this collection. Open the data collection by clicking on it. Click on the \"Import data\" button and load your .csv file. You will now be able to preview your dataset and also assign the data types as shown below. Then, \"import\" the dataset and wait for a few seconds for the import to finish. Connecting MindsDB to MongoDB Database Head over to https://cloud.mindsdb.com/ and click on \"Add Database\". Enter the required details as shown below. The connection string must be similar to: mongodb+srv://<username>:<password>@mindsdb.htuqc.mongodb.net/mindsDB Click on \"Connect\" and that's it! We have successfully linked our Database to MindsDB. Next, head over to the Datasets tab and click on \"From database\". Enter the details as shown below. In the Find field, we can specify a Mongo query using which MindsDB will include only the results of this query in the data source. By specifying {} , we are telling MindsDB to include every single document in the data collection in the data source. 6. Click on \"Create\" and now we will see that our data source named \"Metro Traffic Dataset\" has been added. One can check for the quality and also preview the data source. We are now ready to train an ML model to predict the traffic_volume using MindsDB. Training the ML Model Head over to the Predictors Tab and click on \"Train New\". In the popup modal, give a name to the predictor and select the column that needs to be predicted, which in our case is traffic_volume . After entering the details, click on \"Generate\". That's how simple training an ML model is with MindsDB. Now all you have to do is wait for a few minutes for the model to get trained after which you will be able to run queries and get predictions on the traffic_volume . Running Queries to get predictions Once the status changes to COMPLETE, it means that our model is now ready and we can start getting predictions. We can see that the model has an accuracy of 98.6%, which is impressive! To start getting predictions, click on the \"Query\" button and then \"New Query\". Let's say we wanted to know the traffic_volume for some day and all we know is the following: { temp: 300 , # temperature of 300 Kelvin clouds_all: 10 , # 10% cloud cover weather_main: \"Clouds\" , weather_description: \"few clouds\" , holiday: \"None\" } Enter the above details and \"Run Query\". We can see that the model predicted with 99% confidence that on such a day, the traffic volume would be 832. You can play with the inputs and run a few more queries and observe the results. What Next? This tutorial can be extended to perform lots of awesome things. For example, it would be interesting to see the dependence between the weather and the traffic volume. Some interesting questions that can be asked are: 1. Given a certain traffic_volume what is the probability the sky is clear ? What is the probability that it is raining? \ud83c\udf27\ufe0f 2. Given a certain traffic_volume , how certain can we be that the day is a holiday? \ud83c\udfd6\ufe0f 3. Can we predict more parameters instead of only the traffic_volume ? Apart from these, you can also install MindsDB on your machine and connect with your local databases to get predictions. You can also use a BI tool to visualize these predictions. Apart from SQL and MongoDB, you can explore other data source integrations that MindsDB supports like Oracle and Kafka. If you have made it this far, thank you for your time and hope you found this useful. If you like the article, like it and share it with others. Happy Querying!","title":"Forecast Metro Traffic using MindsDB Cloud and MongoDB Atlas"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#forecast-metro-traffic-using-mindsdb-cloud-and-mongodb-atlas","text":"In this tutorial, we will be learning to: Connect a MongoDB database to MindsDB. Train a model to predict metro traffic. Get a prediction from the model given certain input parameters. All of that without writing a single line of code and in less than 15 minutes . Yes, you read that right! We will be using the Metro traffic dataset that can be downloaded from here . You are also free to use your own dataset and follow along the tutorial.","title":"Forecast Metro Traffic using MindsDB Cloud and MongoDB Atlas"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#pre-requisites","text":"This tutorial is primarily going to be about MindsDB so the reader is expected to have some level of familiarity with MongoDB Atlas . In short, MongoDB Atlas is a Database as a Service(DaaS), which we will be using to spin up a MongoDB database cluster and load our dataset. Download a copy of the Metro Traffic dataset from here . You are also expected to have an account on MindsDB Cloud. If not, head over to https://cloud.mindsdb.com/ and create an account. It hardly takes a few minutes. We also need MongoDB Compass to load the dataset into the collection. It can be downloaded from here . Finally, No! You are not required to have any background in programming or machine learning . As mentioned before you wont be writing a single line of code!","title":"Pre-requisites"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#about-mindsdb","text":"MindsDB is a predictive platform that makes databases intelligent and machine learning easy to use. It allows data analysts to build and visualize forecasts in BI dashboards without going through the complexity of ML pipelines, all through SQL. It also helps data scientists to streamline MLOps by providing advanced instruments for in-database machine learning and optimize ML workflows through a declarative JSON-AI syntax. Although only SQL is mentioned, MongoDB is also supported.","title":"About MindsDB"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#dataset-overview","text":"The dataset contains information about the: Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN . More details can be found here . The dataset is a .csv file that contains 9 columns: 1. holiday : Categorical US National holidays plus regional holiday, Minnesota State Fair 2. temp : Numeric Average temp in kelvin 3. rain_1h : Numeric Amount in mm of rain that occurred in the hour 4. snow_1h : Numeric Amount in mm of snow that occurred in the hour 5. clouds_all : Numeric Percentage of cloud cover 6. weather_main : Categorical Short textual description of the current weather 7. weather_description : Categorical Longer textual description of the current weather 8. date_time : DateTime Hour of the data collected in local CST time 9. traffic_volume : Numeric Hourly I-94 ATR 301 reported westbound traffic volume Phew! With all of that out of the way, we can finally get started!","title":"Dataset overview"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#setting-up-a-cluster-on-mongodb-atlas","text":"Head over to https://cloud.mongodb.com/ and create a new project named mindsdb and within it a new database cluster named mindsDB . Typically, it takes a minute or two to provision a cluster. Once it is done, you should have something like this: Click on the \"Connect\" button. In the popup modal, you will be asked to add a connection IP address. Although not recommended, for the sake of this tutorial, choose \"Allow access from anywhere\" and then \"Add IP Address\". Next, you will be asked to create a new database user. After providing a username and password, click on the \"Create Database User\" button. In the next step, select \"Connect using MongoDB Compass\". Copy the connection string which should look like this: mongodb+srv://<username>:<password>@mindsdb.htuqc.mongodb.net/ We will now use this connection string to connect to our database from MongoDB Compass and load our dataset.","title":"Setting up a Cluster on MongoDB Atlas"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#loading-the-dataset-with-mongodb-compass","text":"Open MongoDB Compass. Paste the connection string and click on \"Connect\". On successful authentication, you will be welcomed by this screen. Click on \"Create Database\" and create a database named mindsDB and a collection named data . You will now see mindsDB listed. Click on it and you will see that it contains a collection named data . We will be loading data from the .csv file into this collection. Open the data collection by clicking on it. Click on the \"Import data\" button and load your .csv file. You will now be able to preview your dataset and also assign the data types as shown below. Then, \"import\" the dataset and wait for a few seconds for the import to finish.","title":"Loading the dataset with MongoDB Compass"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#connecting-mindsdb-to-mongodb-database","text":"Head over to https://cloud.mindsdb.com/ and click on \"Add Database\". Enter the required details as shown below. The connection string must be similar to: mongodb+srv://<username>:<password>@mindsdb.htuqc.mongodb.net/mindsDB Click on \"Connect\" and that's it! We have successfully linked our Database to MindsDB. Next, head over to the Datasets tab and click on \"From database\". Enter the details as shown below. In the Find field, we can specify a Mongo query using which MindsDB will include only the results of this query in the data source. By specifying {} , we are telling MindsDB to include every single document in the data collection in the data source. 6. Click on \"Create\" and now we will see that our data source named \"Metro Traffic Dataset\" has been added. One can check for the quality and also preview the data source. We are now ready to train an ML model to predict the traffic_volume using MindsDB.","title":"Connecting MindsDB to MongoDB Database"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#training-the-ml-model","text":"Head over to the Predictors Tab and click on \"Train New\". In the popup modal, give a name to the predictor and select the column that needs to be predicted, which in our case is traffic_volume . After entering the details, click on \"Generate\". That's how simple training an ML model is with MindsDB. Now all you have to do is wait for a few minutes for the model to get trained after which you will be able to run queries and get predictions on the traffic_volume .","title":"Training the ML Model"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#running-queries-to-get-predictions","text":"Once the status changes to COMPLETE, it means that our model is now ready and we can start getting predictions. We can see that the model has an accuracy of 98.6%, which is impressive! To start getting predictions, click on the \"Query\" button and then \"New Query\". Let's say we wanted to know the traffic_volume for some day and all we know is the following: { temp: 300 , # temperature of 300 Kelvin clouds_all: 10 , # 10% cloud cover weather_main: \"Clouds\" , weather_description: \"few clouds\" , holiday: \"None\" } Enter the above details and \"Run Query\". We can see that the model predicted with 99% confidence that on such a day, the traffic volume would be 832. You can play with the inputs and run a few more queries and observe the results.","title":"Running Queries to get predictions"},{"location":"mongo/tutorials/Forecast-Metro-Traffic/#what-next","text":"This tutorial can be extended to perform lots of awesome things. For example, it would be interesting to see the dependence between the weather and the traffic volume. Some interesting questions that can be asked are: 1. Given a certain traffic_volume what is the probability the sky is clear ? What is the probability that it is raining? \ud83c\udf27\ufe0f 2. Given a certain traffic_volume , how certain can we be that the day is a holiday? \ud83c\udfd6\ufe0f 3. Can we predict more parameters instead of only the traffic_volume ? Apart from these, you can also install MindsDB on your machine and connect with your local databases to get predictions. You can also use a BI tool to visualize these predictions. Apart from SQL and MongoDB, you can explore other data source integrations that MindsDB supports like Oracle and Kafka. If you have made it this far, thank you for your time and hope you found this useful. If you like the article, like it and share it with others. Happy Querying!","title":"What Next?"},{"location":"sql/api/SELECT_Files/","text":"SELECT from Files The SELECT from File statement is to select a file as a datasource. This allows the user to create a predictor from a file that has been uploaded to MindsDB's Cloud/Database. Example This example will show how to upload a file to the database via MindsDB Cloud and select data from a file to create a predictor. We will be using docker to help create a database connection. Docker-compose file For this tutorial Visual studio will be used to execute our docker-compose file and sql code. The first step will be to create a docker-compose file to establish a database connection with the following details: File name: docker-compose.yml version: '3.5' services: postgres: container_name: mindsdb_postgres_diabetic_data image: postgres environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: changeme POSTGRES_DB: diabetic_data PGDATA: /data/postgres volumes: # persist data storage in a docker container - mindsdb_postgres_diabetic_data:/data/postgres ports: - \"5433:5432\" network_mode: bridge # restart: none volumes: mindsdb_postgres_diabetic_data: Open a terminal and run the command: docker-compose up Running Ngrok Tunnel To establish a database connection to MindsDB we will use a tunneling program Ngrok to allow the remote database connection without exposing your public IP. Run the following command and copy the forwarding address: ngrok tcp 5433 Session Status online Account myaccount ( Plan: Free ) Version 2 .3.40 Region United States ( us ) Web Interface http://127.0.0.1:4040 Forwarding tcp://2.tcp.ngrok.io:12320 -> localhost:5433 We will copy the address tcp://2.tcp.ngrok.io:12320 to use to connect our database. Establishing Database Connection via MindsDB Cloud You can easily create a database connection Login to MindsDB Cloud. Select Add Database and enter required details. Connect your database. Upload file to MindsDB Cloud Login to MindsDB Cloud. Navigate to the 'Files' located on the right side bar. Select 'File Upload' next to Your Files. Select 'File' and choose your file which will be used as a datasource. Name your file. Select Upload. Your file will be displayed in the list for your files. Connect to MindsDB using SQL client You can connect to MindsDB SQL Server via Local Deployment or MindsDB Cloud a. Open a terminal and run the following command: mysql - h cloud . mindsdb . com --port 3306 -u cloudusername@mail.com -p The parameters required are: -h: Host name of mindsdbs mysql api (by default takes cloud.mindsdb.com if not specified) --port: TCP/IP port number for connection(3306) -u: MindsDB Cloud username -p: MindsDB Cloud password b. Once connected, run the following command to connect to MindsDB's database: use mindsdb ; Create predictor from your file as a datasource Select the file as your datasource: Query: SELECT * FROM files . file_name ; Example: SELECT * FROM files . diabetes Limit 10 ; Parameters: - .file_name : The name of your file you are using as a datasource(as named when it was uploaded to the database). Results: mysql> select * from files.diabetes LIMIT 10 ; +--------------------------+------------------------------+--------------------------+-----------------------------+----------------------+-----------------+----------------------------+------+----------+ | Number of times pregnant | Plasma glucose concentration | Diastolic blood pressure | Triceps skin fold thickness | 2 -Hour serum insulin | Body mass index | Diabetes pedigree function | Age | Class | +--------------------------+------------------------------+--------------------------+-----------------------------+----------------------+-----------------+----------------------------+------+----------+ | 6 | 148 | 72 | 35 | 0 | 33 .6 | 0 .627 | 50 | positive | | 1 | 85 | 66 | 29 | 0 | 26 .6 | 0 .351 | 31 | negative | | 8 | 183 | 64 | 0 | 0 | 23 .3 | 0 .672 | 32 | positive | | 1 | 89 | 66 | 23 | 94 | 28 .1 | 0 .167 | 21 | negative | | 0 | 137 | 40 | 35 | 168 | 43 .1 | 2 .288 | 33 | positive | | 5 | 116 | 74 | 0 | 0 | 25 .6 | 0 .201 | 30 | negative | | 3 | 78 | 50 | 32 | 88 | 31 .0 | 0 .248 | 26 | positive | | 10 | 115 | 0 | 0 | 0 | 35 .3 | 0 .134 | 29 | negative | | 2 | 197 | 70 | 45 | 543 | 30 .5 | 0 .158 | 53 | positive | | 8 | 125 | 96 | 0 | 0 | 0 .0 | 0 .232 | 54 | positive | +--------------------------+------------------------------+--------------------------+-----------------------------+----------------------+-----------------+----------------------------+------+----------+ 10 rows in set ( 0 .72 sec ) Create a predictor Query: CREATE PREDICTOR predictor_name from files ( SELECT * from file_name ) predict target_variable ; Example CREATE PREDICTOR diabetic_data from files ( SELECT * from diabetes ) predict class ; Parameters: predictor_name: chosen name for your predictor. file_name: name of your file that's being used as a datasource. target_variable: the column you want to predict. Results: mysql> CREATE PREDICTOR diabetic_data from files ( SELECT * from diabetes ) predict class ; Query OK, 0 rows affected ( 5 .16 sec ) Select your predictor You can now select your predictor to check the training status with the following command: Query: SELECT * from mindsdb . predictors where name = 'predictor_name' ; Example SELECT * FROM mindsdb . predictors where name = 'diabetic_data' ; Results: mysql> SELECT * FROM mindsdb.predictors where name = 'diabetic_data' ; +---------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | +---------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | diabetic_data | complete | 0 .6546310832025117 | Class | up_to_date | 22 .2.2.1 | NULL | | | +---------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ 1 row in set ( 0 .63 sec )","title":"SELECT from FILES"},{"location":"sql/api/SELECT_Files/#select-from-files","text":"The SELECT from File statement is to select a file as a datasource. This allows the user to create a predictor from a file that has been uploaded to MindsDB's Cloud/Database.","title":"SELECT from Files"},{"location":"sql/api/SELECT_Files/#example","text":"This example will show how to upload a file to the database via MindsDB Cloud and select data from a file to create a predictor. We will be using docker to help create a database connection.","title":"Example"},{"location":"sql/api/SELECT_Files/#docker-compose-file","text":"For this tutorial Visual studio will be used to execute our docker-compose file and sql code. The first step will be to create a docker-compose file to establish a database connection with the following details: File name: docker-compose.yml version: '3.5' services: postgres: container_name: mindsdb_postgres_diabetic_data image: postgres environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: changeme POSTGRES_DB: diabetic_data PGDATA: /data/postgres volumes: # persist data storage in a docker container - mindsdb_postgres_diabetic_data:/data/postgres ports: - \"5433:5432\" network_mode: bridge # restart: none volumes: mindsdb_postgres_diabetic_data: Open a terminal and run the command: docker-compose up","title":"Docker-compose file"},{"location":"sql/api/SELECT_Files/#running-ngrok-tunnel","text":"To establish a database connection to MindsDB we will use a tunneling program Ngrok to allow the remote database connection without exposing your public IP. Run the following command and copy the forwarding address: ngrok tcp 5433 Session Status online Account myaccount ( Plan: Free ) Version 2 .3.40 Region United States ( us ) Web Interface http://127.0.0.1:4040 Forwarding tcp://2.tcp.ngrok.io:12320 -> localhost:5433 We will copy the address tcp://2.tcp.ngrok.io:12320 to use to connect our database.","title":"Running Ngrok Tunnel"},{"location":"sql/api/SELECT_Files/#establishing-database-connection-via-mindsdb-cloud","text":"You can easily create a database connection Login to MindsDB Cloud. Select Add Database and enter required details. Connect your database.","title":"Establishing Database Connection via MindsDB Cloud"},{"location":"sql/api/SELECT_Files/#upload-file-to-mindsdb-cloud","text":"Login to MindsDB Cloud. Navigate to the 'Files' located on the right side bar. Select 'File Upload' next to Your Files. Select 'File' and choose your file which will be used as a datasource. Name your file. Select Upload. Your file will be displayed in the list for your files.","title":"Upload file to MindsDB Cloud"},{"location":"sql/api/SELECT_Files/#connect-to-mindsdb-using-sql-client","text":"You can connect to MindsDB SQL Server via Local Deployment or MindsDB Cloud a. Open a terminal and run the following command: mysql - h cloud . mindsdb . com --port 3306 -u cloudusername@mail.com -p The parameters required are: -h: Host name of mindsdbs mysql api (by default takes cloud.mindsdb.com if not specified) --port: TCP/IP port number for connection(3306) -u: MindsDB Cloud username -p: MindsDB Cloud password b. Once connected, run the following command to connect to MindsDB's database: use mindsdb ;","title":"Connect to MindsDB using SQL client"},{"location":"sql/api/SELECT_Files/#create-predictor-from-your-file-as-a-datasource","text":"","title":"Create predictor from your file as a datasource"},{"location":"sql/api/SELECT_Files/#select-the-file-as-your-datasource","text":"Query: SELECT * FROM files . file_name ; Example: SELECT * FROM files . diabetes Limit 10 ; Parameters: - .file_name : The name of your file you are using as a datasource(as named when it was uploaded to the database). Results: mysql> select * from files.diabetes LIMIT 10 ; +--------------------------+------------------------------+--------------------------+-----------------------------+----------------------+-----------------+----------------------------+------+----------+ | Number of times pregnant | Plasma glucose concentration | Diastolic blood pressure | Triceps skin fold thickness | 2 -Hour serum insulin | Body mass index | Diabetes pedigree function | Age | Class | +--------------------------+------------------------------+--------------------------+-----------------------------+----------------------+-----------------+----------------------------+------+----------+ | 6 | 148 | 72 | 35 | 0 | 33 .6 | 0 .627 | 50 | positive | | 1 | 85 | 66 | 29 | 0 | 26 .6 | 0 .351 | 31 | negative | | 8 | 183 | 64 | 0 | 0 | 23 .3 | 0 .672 | 32 | positive | | 1 | 89 | 66 | 23 | 94 | 28 .1 | 0 .167 | 21 | negative | | 0 | 137 | 40 | 35 | 168 | 43 .1 | 2 .288 | 33 | positive | | 5 | 116 | 74 | 0 | 0 | 25 .6 | 0 .201 | 30 | negative | | 3 | 78 | 50 | 32 | 88 | 31 .0 | 0 .248 | 26 | positive | | 10 | 115 | 0 | 0 | 0 | 35 .3 | 0 .134 | 29 | negative | | 2 | 197 | 70 | 45 | 543 | 30 .5 | 0 .158 | 53 | positive | | 8 | 125 | 96 | 0 | 0 | 0 .0 | 0 .232 | 54 | positive | +--------------------------+------------------------------+--------------------------+-----------------------------+----------------------+-----------------+----------------------------+------+----------+ 10 rows in set ( 0 .72 sec )","title":"Select the file as your datasource:"},{"location":"sql/api/SELECT_Files/#create-a-predictor","text":"Query: CREATE PREDICTOR predictor_name from files ( SELECT * from file_name ) predict target_variable ; Example CREATE PREDICTOR diabetic_data from files ( SELECT * from diabetes ) predict class ; Parameters: predictor_name: chosen name for your predictor. file_name: name of your file that's being used as a datasource. target_variable: the column you want to predict. Results: mysql> CREATE PREDICTOR diabetic_data from files ( SELECT * from diabetes ) predict class ; Query OK, 0 rows affected ( 5 .16 sec )","title":"Create a predictor"},{"location":"sql/api/SELECT_Files/#select-your-predictor","text":"You can now select your predictor to check the training status with the following command: Query: SELECT * from mindsdb . predictors where name = 'predictor_name' ; Example SELECT * FROM mindsdb . predictors where name = 'diabetic_data' ; Results: mysql> SELECT * FROM mindsdb.predictors where name = 'diabetic_data' ; +---------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | +---------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | diabetic_data | complete | 0 .6546310832025117 | Class | up_to_date | 22 .2.2.1 | NULL | | | +---------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ 1 row in set ( 0 .63 sec )","title":"Select your predictor"},{"location":"sql/api/datasources/","text":"Create Datasource MindsDB enables connections to your favorite databases, data warehouses, data lakes, etc in a simple way. Our SQL API supports creating a datasource connection by passing any credentials needed by each type of system that you are connecting to. Syntax CREATE DATASOURCE datasource_name WITH engine = engine_string , parameters = { \"key\" : \"value\" , ... } ; Example: MySQL Here is a concrete example to connect to a MySQL database. CREATE DATASOURCE mysql_datasource WITH engine = 'mysql' , parameters = { \"user\" : \"root\" , \"port\" : 3307 , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"database\" : \"mysql\" } ; Once a datasource has been correctly created, you will see it registered in mindsdb.datasources , ready for creating and querying predictors with it. select * from mindsdb . datasources ;","title":"Create Datasource"},{"location":"sql/api/datasources/#create-datasource","text":"MindsDB enables connections to your favorite databases, data warehouses, data lakes, etc in a simple way. Our SQL API supports creating a datasource connection by passing any credentials needed by each type of system that you are connecting to.","title":"Create Datasource"},{"location":"sql/api/datasources/#syntax","text":"CREATE DATASOURCE datasource_name WITH engine = engine_string , parameters = { \"key\" : \"value\" , ... } ;","title":"Syntax"},{"location":"sql/api/datasources/#example-mysql","text":"Here is a concrete example to connect to a MySQL database. CREATE DATASOURCE mysql_datasource WITH engine = 'mysql' , parameters = { \"user\" : \"root\" , \"port\" : 3307 , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"database\" : \"mysql\" } ; Once a datasource has been correctly created, you will see it registered in mindsdb.datasources , ready for creating and querying predictors with it. select * from mindsdb . datasources ;","title":"Example: MySQL"},{"location":"sql/api/describe/","text":"DESCRIBE statement The DESCRIBE ... statement is used to display the attributes of an existing model. DESCRIBE FEATURES The DESCRIBE [...].features statement is used to display the way that the model encoded the data prior to training. Syntax: DESCRIBE [ name_of_your_predictor ]. features ; Expected Output column: name of the feature type: binary integer... encoder: name of the encoder used for that column role: describes whether the column is a target or a feature DESCRIBE MODEL DESCRIBE [...].model statement is used to display the performance of the candidate models. Syntax: DESCRIBE [ name_of_your _predictor ]. model ; Expected Output name: name of the model performance : obtained accuracy (from 0.0 to 1.0) for that candidate model selected: the Auto ML pick the best performed cadidate (1) If you're unsure on how to DESCRIBE your model or understand the results feel free to ask us how to do it on the community Slack workspace .","title":"DESCRIBE statement"},{"location":"sql/api/describe/#describe-statement","text":"The DESCRIBE ... statement is used to display the attributes of an existing model.","title":"DESCRIBE statement"},{"location":"sql/api/describe/#describe-features","text":"The DESCRIBE [...].features statement is used to display the way that the model encoded the data prior to training.","title":"DESCRIBE FEATURES"},{"location":"sql/api/describe/#syntax","text":"DESCRIBE [ name_of_your_predictor ]. features ;","title":"Syntax:"},{"location":"sql/api/describe/#expected-output","text":"column: name of the feature type: binary integer... encoder: name of the encoder used for that column role: describes whether the column is a target or a feature","title":"Expected Output"},{"location":"sql/api/describe/#describe-model","text":"DESCRIBE [...].model statement is used to display the performance of the candidate models.","title":"DESCRIBE MODEL"},{"location":"sql/api/describe/#syntax_1","text":"DESCRIBE [ name_of_your _predictor ]. model ;","title":"Syntax:"},{"location":"sql/api/describe/#expected-output_1","text":"name: name of the model performance : obtained accuracy (from 0.0 to 1.0) for that candidate model selected: the Auto ML pick the best performed cadidate (1) If you're unsure on how to DESCRIBE your model or understand the results feel free to ask us how to do it on the community Slack workspace .","title":"Expected Output"},{"location":"sql/api/drop/","text":"DROP statement Work in progress Note this feature is in beta version. If you have additional questions or issues reach out to us on Slack . The DROP statement is used to delete an existing model table. DROP TABLE statement The DROP PREDICTOR statement is used to delete the model table: DROP PREDICTOR table_name ; DROP TABLE example The following SQL statement drops the model table called home_rentals_model : DROP PREDICTOR home_rentals_model ;","title":"DROP"},{"location":"sql/api/drop/#drop-statement","text":"Work in progress Note this feature is in beta version. If you have additional questions or issues reach out to us on Slack . The DROP statement is used to delete an existing model table.","title":"DROP statement"},{"location":"sql/api/drop/#drop-table-statement","text":"The DROP PREDICTOR statement is used to delete the model table: DROP PREDICTOR table_name ;","title":"DROP TABLE statement"},{"location":"sql/api/drop/#drop-table-example","text":"The following SQL statement drops the model table called home_rentals_model : DROP PREDICTOR home_rentals_model ;","title":"DROP TABLE example"},{"location":"sql/api/join/","text":"JOIN clause The JOIN clause is used to combine rows from the database table and the model table on a related column. The basic syntax for joining from the data table and model is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); JOIN example The following SQL statement joins the home_rentals data with the home_rentals_model predicted price: SELECT * FROM db_integration . house_rentals_data AS t JOIN mindsdb . home_rentals_model AS tb WHERE t . neighborhood in ( 'downtown' , 'south_side' ); number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price select_data_query external_datasource when_data rental_price_original rental_price_confidence rental_price_explain rental_price_anomaly rental_price_min rental_price_max 0 1 484 great 10 2271 south_side 2271 nan nan nan nan 0.99 {\"predicted_value\": 2243, \"confidence\": 0.99, \"confidence_lower_bound\": 2200, \"confidence_upper_bound\": 2286, \"anomaly\": null, \"truth\": 2271} nan 2200 2286 1 1 674 good 1 2167 downtown 2167 nan nan nan nan 0.99 {\"predicted_value\": 2197, \"confidence\": 0.99, \"confidence_lower_bound\": 2154, \"confidence_upper_bound\": 2240, \"anomaly\": null, \"truth\": 2167} nan 2154 2240 0 1 529 great 3 2431 south_side 2431 nan nan nan nan 0.99 {\"predicted_value\": 2432, \"confidence\": 0.99, \"confidence_lower_bound\": 2389, \"confidence_upper_bound\": 2475, \"anomaly\": null, \"truth\": 2431} nan 2389 2475 3 2 1219 great 3 5510 south_side 5510 nan nan nan nan 0.99 {\"predicted_value\": 5550, \"confidence\": 0.99, \"confidence_lower_bound\": 5507, \"confidence_upper_bound\": 5593, \"anomaly\": null, \"truth\": 5510} nan 5507 5593 1 1 398 great 11 2272 south_side 2272 nan nan nan nan 0.99 {\"predicted_value\": 2252, \"confidence\": 0.99, \"confidence_lower_bound\": 2209, \"confidence_upper_bound\": 2295, \"anomaly\": null, \"truth\": 2272} nan 2209 2295","title":"JOIN"},{"location":"sql/api/join/#join-clause","text":"The JOIN clause is used to combine rows from the database table and the model table on a related column. The basic syntax for joining from the data table and model is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...);","title":"JOIN clause"},{"location":"sql/api/join/#join-example","text":"The following SQL statement joins the home_rentals data with the home_rentals_model predicted price: SELECT * FROM db_integration . house_rentals_data AS t JOIN mindsdb . home_rentals_model AS tb WHERE t . neighborhood in ( 'downtown' , 'south_side' ); number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price select_data_query external_datasource when_data rental_price_original rental_price_confidence rental_price_explain rental_price_anomaly rental_price_min rental_price_max 0 1 484 great 10 2271 south_side 2271 nan nan nan nan 0.99 {\"predicted_value\": 2243, \"confidence\": 0.99, \"confidence_lower_bound\": 2200, \"confidence_upper_bound\": 2286, \"anomaly\": null, \"truth\": 2271} nan 2200 2286 1 1 674 good 1 2167 downtown 2167 nan nan nan nan 0.99 {\"predicted_value\": 2197, \"confidence\": 0.99, \"confidence_lower_bound\": 2154, \"confidence_upper_bound\": 2240, \"anomaly\": null, \"truth\": 2167} nan 2154 2240 0 1 529 great 3 2431 south_side 2431 nan nan nan nan 0.99 {\"predicted_value\": 2432, \"confidence\": 0.99, \"confidence_lower_bound\": 2389, \"confidence_upper_bound\": 2475, \"anomaly\": null, \"truth\": 2431} nan 2389 2475 3 2 1219 great 3 5510 south_side 5510 nan nan nan nan 0.99 {\"predicted_value\": 5550, \"confidence\": 0.99, \"confidence_lower_bound\": 5507, \"confidence_upper_bound\": 5593, \"anomaly\": null, \"truth\": 5510} nan 5507 5593 1 1 398 great 11 2272 south_side 2272 nan nan nan nan 0.99 {\"predicted_value\": 2252, \"confidence\": 0.99, \"confidence_lower_bound\": 2209, \"confidence_upper_bound\": 2295, \"anomaly\": null, \"truth\": 2272} nan 2209 2295","title":"JOIN example"},{"location":"sql/api/predictor/","text":"CREATE PREDICTOR Statement The CREATE PREDICTOR statement is used to train a new model. The basic syntax for training a model is: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; CREATE PREDICTOR predictor_name - where predictor_name is the name of the model. FROM integration_name (select column_name, column_name2 FROM table_name) - where integration_name is the name of the datasource , where (select column_name, column_name2 FROM table_name) is the SELECT statement for selecting the data. If you want to change the default name of the datasource you can use the alias as ds_name . PREDICT column_name - where column_name is the column name of the target variable. If you want to change the name of the target variable you can use the as column_alias . Example Data The Home Rentals dataset contains prices of properties from a metropolitan area in the US. This table will be used in all of the documentation examples. number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price 0 1 484 great 10 2271 south_side 2271 1 1 674 good 1 2167 downtown 2167 0 1 529 great 3 2431 south_side 2431 3 2 1219 great 3 5510 south_side 5510 1 1 398 great 11 2272 south_side 2272 Create Predictor example This example shows how you can train a Machine Learning model called home_rentals_model to predict the rental prices for real estate properties inside the dataset. CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; With this, we will use all columns from the dataset above to predict the value in rental_price . Create from a file datasource If you have uploaded a file from the GUI, you can specify files as the datasource: CREATE PREDICTOR home_rentals_model FROM files ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; SELECT Predictor status After you run the CREATE Predictor statement, you can check the status of the training model, by selecting from mindsdb.predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'predictor_name' ; SELECT Predictor example To check the training status for the home_rentals_model run: SELECT * FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; Time Series keywords To train a timeseries model, MindsDB provides additional keywords. ORDER BY - keyword is used as the column that defines the time series order by, these can be a date, or anything that defines the sequence of events to order the data by descending (DESC) or ascending (ASC) order. (The default order will always be ASC ). WINDOW - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". HORIZON - (OPTIONAL, default value is 1) keyword specifies the number of future predictions. GROUP BY - (OPTIONAL) keyword is used to group the rows that make a partition, for example, if you want to forecast inventory for all items in a store, you can partition the data by product_id, meaning that each product_id has its own time series. CREATE PREDICTOR predictor_name FROM db_integration ( SELECT sequential_column , partition_column , other_column , target_column FROM table_name ) as ds_name PREDICT target_column AS column_alias ORDER BY sequantial_column GROUP BY partition_column WINDOW 10 HORIZON 5 ; Time Series example The following example trains the new inventory_model model which can predicts the units_in_inventory for the next 7 days, taking into account the historical inventory in the past 20 days for each 'procuct_id' CREATE PREDICTOR inventory_model FROM db_integration ( SELECT * FROM inventory ) as inventory PREDICT units_in_inventory as predicted_units_in_inventory ORDER BY date , GROUP BY product_id , WINDOW 20 HORIZON 7 USING statement In MindsDB, the underlying AutoML models are based on Lightwood. This library generates models automatically based on the data and a declarative problem definition, but the default configuration can be overridden. The USING ... statement provides the option to configure a model to be trained with specific options. Syntax CREATE PREDICTOR [ name_of_your _predictor ] FROM [ name_of_your_integration ] ( SELECT * FROM your_database . your_data_table ) PREDICT [ data_target_column ] USING [ parameter ] = [ 'parameter_value1' ] ; There are several high-level keys that can be specified with USING . Here, we explore a couple of them. Encoder key (USING encoders) Grants access to configure how each column is encoded. By default, the AutoML engine will try to get the best match for the data. ... USING encoders .[ column_name ]. module = 'value' ; To learn more about how encoders work and their options, go here . Model key (USING model) Allows you to specify what type of Machine Learning algorithm to learn from the encoder data. ... USING model . args = '{\"key\": value}' ; To learn more about all the model options, go here . Other keys We support JSON-like syntax as well as a dot access notation for said JSON. In that sense, it is equivalent to write USING a.b.c=5 or USING a = {\"b\": {\"c\": 5}} . You have a lot of options in terms of what you can configure with USING . Since lightwood model are fully-generated by a JSON configuration we call \"JsonAI\", and with the USING statement you can modify all bits of that. The most common usecases for configuring predictors will be listed and explained in the example below. To see all options available in detail, you should checkout the lightwood docs about JsonAI . Example We will use the home rentals dataset, specifying particular encoders for some of the columns and a LightGBM model. CREATE PREDICTOR home_rentals_predictor FROM my_db_integration ( SELECT * FROM home_rentals ) PREDICT rental_price USING /* Change the encoder for a column */ encoders . location . module = 'CategoricalAutoEncoder' , /* Change the encoder for another colum (the target) */ encoders . rental_price . module = 'NumericEncoder' , /* Change the arguments that will be passed to that encoder */ encoders . rental_price . args . positive_domain = 'True' , /* Set the list of models lightwood will try to use to a single one, a Light Gradient Boosting Machine.*/ model . args = '{\"submodels\": [{\"module\": \"LightGBM\", \"args\": {\"stop_after\": 12, \"fit_on_dev\": true}}]}' ; ; If you're unsure how to configure a model to solve your problem, feel free to ask us how to do it on the community Slack workspace .","title":"Predictor"},{"location":"sql/api/predictor/#create-predictor-statement","text":"The CREATE PREDICTOR statement is used to train a new model. The basic syntax for training a model is: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; CREATE PREDICTOR predictor_name - where predictor_name is the name of the model. FROM integration_name (select column_name, column_name2 FROM table_name) - where integration_name is the name of the datasource , where (select column_name, column_name2 FROM table_name) is the SELECT statement for selecting the data. If you want to change the default name of the datasource you can use the alias as ds_name . PREDICT column_name - where column_name is the column name of the target variable. If you want to change the name of the target variable you can use the as column_alias .","title":"CREATE PREDICTOR Statement"},{"location":"sql/api/predictor/#example-data","text":"The Home Rentals dataset contains prices of properties from a metropolitan area in the US. This table will be used in all of the documentation examples. number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price 0 1 484 great 10 2271 south_side 2271 1 1 674 good 1 2167 downtown 2167 0 1 529 great 3 2431 south_side 2431 3 2 1219 great 3 5510 south_side 5510 1 1 398 great 11 2272 south_side 2272","title":"Example Data"},{"location":"sql/api/predictor/#create-predictor-example","text":"This example shows how you can train a Machine Learning model called home_rentals_model to predict the rental prices for real estate properties inside the dataset. CREATE PREDICTOR home_rentals_model FROM db_integration ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; With this, we will use all columns from the dataset above to predict the value in rental_price .","title":"Create Predictor example"},{"location":"sql/api/predictor/#create-from-a-file-datasource","text":"If you have uploaded a file from the GUI, you can specify files as the datasource: CREATE PREDICTOR home_rentals_model FROM files ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ;","title":"Create from a file datasource"},{"location":"sql/api/predictor/#select-predictor-status","text":"After you run the CREATE Predictor statement, you can check the status of the training model, by selecting from mindsdb.predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'predictor_name' ;","title":"SELECT Predictor status"},{"location":"sql/api/predictor/#select-predictor-example","text":"To check the training status for the home_rentals_model run: SELECT * FROM mindsdb . predictors WHERE name = 'home_rentals_model' ;","title":"SELECT Predictor example"},{"location":"sql/api/predictor/#time-series-keywords","text":"To train a timeseries model, MindsDB provides additional keywords. ORDER BY - keyword is used as the column that defines the time series order by, these can be a date, or anything that defines the sequence of events to order the data by descending (DESC) or ascending (ASC) order. (The default order will always be ASC ). WINDOW - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". HORIZON - (OPTIONAL, default value is 1) keyword specifies the number of future predictions. GROUP BY - (OPTIONAL) keyword is used to group the rows that make a partition, for example, if you want to forecast inventory for all items in a store, you can partition the data by product_id, meaning that each product_id has its own time series. CREATE PREDICTOR predictor_name FROM db_integration ( SELECT sequential_column , partition_column , other_column , target_column FROM table_name ) as ds_name PREDICT target_column AS column_alias ORDER BY sequantial_column GROUP BY partition_column WINDOW 10 HORIZON 5 ;","title":"Time Series keywords"},{"location":"sql/api/predictor/#time-series-example","text":"The following example trains the new inventory_model model which can predicts the units_in_inventory for the next 7 days, taking into account the historical inventory in the past 20 days for each 'procuct_id' CREATE PREDICTOR inventory_model FROM db_integration ( SELECT * FROM inventory ) as inventory PREDICT units_in_inventory as predicted_units_in_inventory ORDER BY date , GROUP BY product_id , WINDOW 20 HORIZON 7","title":"Time Series example"},{"location":"sql/api/predictor/#using-statement","text":"In MindsDB, the underlying AutoML models are based on Lightwood. This library generates models automatically based on the data and a declarative problem definition, but the default configuration can be overridden. The USING ... statement provides the option to configure a model to be trained with specific options.","title":"USING statement"},{"location":"sql/api/predictor/#syntax","text":"CREATE PREDICTOR [ name_of_your _predictor ] FROM [ name_of_your_integration ] ( SELECT * FROM your_database . your_data_table ) PREDICT [ data_target_column ] USING [ parameter ] = [ 'parameter_value1' ] ; There are several high-level keys that can be specified with USING . Here, we explore a couple of them.","title":"Syntax"},{"location":"sql/api/predictor/#encoder-key-using-encoders","text":"Grants access to configure how each column is encoded. By default, the AutoML engine will try to get the best match for the data. ... USING encoders .[ column_name ]. module = 'value' ; To learn more about how encoders work and their options, go here .","title":"Encoder key (USING encoders)"},{"location":"sql/api/predictor/#model-key-using-model","text":"Allows you to specify what type of Machine Learning algorithm to learn from the encoder data. ... USING model . args = '{\"key\": value}' ; To learn more about all the model options, go here .","title":"Model key (USING model)"},{"location":"sql/api/predictor/#other-keys","text":"We support JSON-like syntax as well as a dot access notation for said JSON. In that sense, it is equivalent to write USING a.b.c=5 or USING a = {\"b\": {\"c\": 5}} . You have a lot of options in terms of what you can configure with USING . Since lightwood model are fully-generated by a JSON configuration we call \"JsonAI\", and with the USING statement you can modify all bits of that. The most common usecases for configuring predictors will be listed and explained in the example below. To see all options available in detail, you should checkout the lightwood docs about JsonAI .","title":"Other keys"},{"location":"sql/api/predictor/#example","text":"We will use the home rentals dataset, specifying particular encoders for some of the columns and a LightGBM model. CREATE PREDICTOR home_rentals_predictor FROM my_db_integration ( SELECT * FROM home_rentals ) PREDICT rental_price USING /* Change the encoder for a column */ encoders . location . module = 'CategoricalAutoEncoder' , /* Change the encoder for another colum (the target) */ encoders . rental_price . module = 'NumericEncoder' , /* Change the arguments that will be passed to that encoder */ encoders . rental_price . args . positive_domain = 'True' , /* Set the list of models lightwood will try to use to a single one, a Light Gradient Boosting Machine.*/ model . args = '{\"submodels\": [{\"module\": \"LightGBM\", \"args\": {\"stop_after\": 12, \"fit_on_dev\": true}}]}' ; ; If you're unsure how to configure a model to solve your problem, feel free to ask us how to do it on the community Slack workspace .","title":"Example"},{"location":"sql/api/publish/","text":"PUBLISH statement","title":"PUBLISH statement"},{"location":"sql/api/publish/#publish-statement","text":"","title":"PUBLISH statement"},{"location":"sql/api/retrain/","text":"RETRAIN PREDICTOR Statement The RETRAIN statement is used to retrain old predictors. The basic syntax for retraining the predictors is: RETRAIN predictor_name ; The predictor is updated to leverage any new data in optimizing its predictive capabilities, without necessarily taking as long to train as starting from scratch. RETRAIN Predictor example This example shows how you can retrain the predictor called home_rentals_model . RETRAIN home_rentals_model ; SELECT Predictor status To check if the status of the predictor is outdated you can SELECT from predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'predictor_name' ; SELECT Predictor example To check the status of the home_rentals_model run: SELECT * FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; If the status is OUTDATED you can retrain the predictor.","title":"RETRAIN"},{"location":"sql/api/retrain/#retrain-predictor-statement","text":"The RETRAIN statement is used to retrain old predictors. The basic syntax for retraining the predictors is: RETRAIN predictor_name ; The predictor is updated to leverage any new data in optimizing its predictive capabilities, without necessarily taking as long to train as starting from scratch.","title":"RETRAIN PREDICTOR Statement"},{"location":"sql/api/retrain/#retrain-predictor-example","text":"This example shows how you can retrain the predictor called home_rentals_model . RETRAIN home_rentals_model ;","title":"RETRAIN Predictor example"},{"location":"sql/api/retrain/#select-predictor-status","text":"To check if the status of the predictor is outdated you can SELECT from predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'predictor_name' ;","title":"SELECT Predictor status"},{"location":"sql/api/retrain/#select-predictor-example","text":"To check the status of the home_rentals_model run: SELECT * FROM mindsdb . predictors WHERE name = 'home_rentals_model' ; If the status is OUTDATED you can retrain the predictor.","title":"SELECT Predictor example"},{"location":"sql/api/select/","text":"SELECT statement The SELECT statement is used to get a predictions from the model table. The data is not persistent and is returned on the fly as a result-set. The basic syntax for selecting from the model is: SELECT target_variable , target_variable_explain FROM model_table WHERE column3 = \"value\" AND column2 = \"value\" ; Model table columns The below list contains the column names of the model table. Note that target_variable_ will be the name of the target variable column. target_variable_original - The original value of the target variable. target_variable_min - Lower bound of the predicted value. target_variable_max - Upper bound of the predicted value. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound , confidence_upper_bound , anomaly , truth . select_data_query - SQL select query to create the datasource. rental_price number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price_original rental_price_min rental_price_max rental_price_confidence rental_price_explain when_data select_data_query external_datasource 2450 4 2 800 good 12 2222 downtown nan 2407 2493 0.99 {\"predicted_value\": 2450, \"confidence\": 0.99, \"confidence_lower_bound\": 2407, \"confidence_upper_bound\": 2493, \"anomaly\": null, \"truth\": null} {\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"} nan nan SELECT example The following SQL statement selects all information from the home_rentals_model for the property that has \"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\". SELECT * FROM mindsdb . home_rentals_model WHERE when_data = '{\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"}' ; The following SQL statement selects only the target variable rental_price as price and the home_rentals_model confidence as accuracy : SELECT rental_price as price , rental_price_confidence as confidence FROM mindsdb . home_rentals_model WHERE when_data = '{\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"}' ;","title":"SELECT"},{"location":"sql/api/select/#select-statement","text":"The SELECT statement is used to get a predictions from the model table. The data is not persistent and is returned on the fly as a result-set. The basic syntax for selecting from the model is: SELECT target_variable , target_variable_explain FROM model_table WHERE column3 = \"value\" AND column2 = \"value\" ;","title":"SELECT statement"},{"location":"sql/api/select/#model-table-columns","text":"The below list contains the column names of the model table. Note that target_variable_ will be the name of the target variable column. target_variable_original - The original value of the target variable. target_variable_min - Lower bound of the predicted value. target_variable_max - Upper bound of the predicted value. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound , confidence_upper_bound , anomaly , truth . select_data_query - SQL select query to create the datasource. rental_price number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price_original rental_price_min rental_price_max rental_price_confidence rental_price_explain when_data select_data_query external_datasource 2450 4 2 800 good 12 2222 downtown nan 2407 2493 0.99 {\"predicted_value\": 2450, \"confidence\": 0.99, \"confidence_lower_bound\": 2407, \"confidence_upper_bound\": 2493, \"anomaly\": null, \"truth\": null} {\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"} nan nan","title":"Model table columns"},{"location":"sql/api/select/#select-example","text":"The following SQL statement selects all information from the home_rentals_model for the property that has \"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\". SELECT * FROM mindsdb . home_rentals_model WHERE when_data = '{\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"}' ; The following SQL statement selects only the target variable rental_price as price and the home_rentals_model confidence as accuracy : SELECT rental_price as price , rental_price_confidence as confidence FROM mindsdb . home_rentals_model WHERE when_data = '{\"sqft\": 800, \"number_of_rooms\": 4, \"number_of_bathrooms\": 2, \"location\": \"good\", \"days_on_market\" : 12, \"neighborhood\": \"downtown\", \"initial_price\": \"2222\"}' ;","title":"SELECT example"},{"location":"sql/api/stream/","text":"Work in progress This documentation is in progress. If you want to get access to the beta version, reach out to us on Slack .","title":"Stream"},{"location":"sql/api/use/","text":"USE statement The use integration_name statement provides an option to use the connected datasources and SELECT from the database tables. Even if you are connecting to MindsDB as MySQL database, you will still be able to preview or SELECT from your database. If you haven't created a datasource after connecting to your database check out the simple steps explained here . Preview the data To connect to your database use the created datasource: use integration_name Then, simply SELECT from the tables: SELECT * FROM table_name ;","title":"USE"},{"location":"sql/api/use/#use-statement","text":"The use integration_name statement provides an option to use the connected datasources and SELECT from the database tables. Even if you are connecting to MindsDB as MySQL database, you will still be able to preview or SELECT from your database. If you haven't created a datasource after connecting to your database check out the simple steps explained here .","title":"USE statement"},{"location":"sql/api/use/#preview-the-data","text":"To connect to your database use the created datasource: use integration_name Then, simply SELECT from the tables: SELECT * FROM table_name ;","title":"Preview the data"},{"location":"sql/api/view/","text":"CREATE VIEW statement In MindsDB, an AI Table is a virtual table based on the result-set of the SQL Statement that JOINS table data with the predictions of a model. An AI Table can be created using the CREATE AI table ai_table_name statement. Syntax CREATE VIEW ai_table_name as ( SELECT a . column_name , a . column_name2 , a . column_name3 , p . model_column as model_column FROM integration_name . table_name as a JOIN predictor_name as p ); Example We will use the Home Rentals dataset to create an AI Table. number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price 0 1 484 great 10 2271 south_side 2271 1 1 674 good 1 2167 downtown 2167 The first step is to execute a SQL query for creating a home_rentals_model that learns to predict the rental_price value given other features of a real estate listing: CREATE PREDICTOR home_rentals_model FROM integration_name ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; Once trained, we can JOIN any input data with the trained model and store the results as an AI Table. Let's pass some of the expected input columns (in this case, sqft , number_of_bathrooms , location ) to the model and join the predicted rental_price values: CREATE VIEW home_rentals as ( SELECT a . sqft , a . number_of_bathrooms , a . location , p . rental_price as price FROM mysql_db . home_rentals as a JOIN home_rentals_model as p ); Note that in this example, we pass part of the same data that was used to train as a test query, but usually you would create an AI table to store predictions for new data.","title":"View"},{"location":"sql/api/view/#create-view-statement","text":"In MindsDB, an AI Table is a virtual table based on the result-set of the SQL Statement that JOINS table data with the predictions of a model. An AI Table can be created using the CREATE AI table ai_table_name statement.","title":"CREATE VIEW statement"},{"location":"sql/api/view/#syntax","text":"CREATE VIEW ai_table_name as ( SELECT a . column_name , a . column_name2 , a . column_name3 , p . model_column as model_column FROM integration_name . table_name as a JOIN predictor_name as p );","title":"Syntax"},{"location":"sql/api/view/#example","text":"We will use the Home Rentals dataset to create an AI Table. number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price 0 1 484 great 10 2271 south_side 2271 1 1 674 good 1 2167 downtown 2167 The first step is to execute a SQL query for creating a home_rentals_model that learns to predict the rental_price value given other features of a real estate listing: CREATE PREDICTOR home_rentals_model FROM integration_name ( SELECT * FROM house_rentals_data ) as rentals PREDICT rental_price as price ; Once trained, we can JOIN any input data with the trained model and store the results as an AI Table. Let's pass some of the expected input columns (in this case, sqft , number_of_bathrooms , location ) to the model and join the predicted rental_price values: CREATE VIEW home_rentals as ( SELECT a . sqft , a . number_of_bathrooms , a . location , p . rental_price as price FROM mysql_db . home_rentals as a JOIN home_rentals_model as p ); Note that in this example, we pass part of the same data that was used to train as a test query, but usually you would create an AI table to store predictions for new data.","title":"Example"},{"location":"sql/connect/cloud/","text":"MindsDB Cloud as a SQL Database MindsDB Cloud provides a powerful MySQL API that allows cloud users to connect to it. The first step to connect is to use the MindsDB Cloud user. If you haven't signup to the MindsDB Cloud follow the steps explained here . After that you can use one of the below db clients: MySQL Command-Line Client DBeaver MySQL client Open mysql client and run: mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p The required parameters are: -h: Host name of mindsdbs mysql api (by default takes cloud.mindsdb.com if not specified) --port: TCP/IP port number for connection(3306) -u: MindsDB Cloud username -p: MindsDB Cloud password Dbeaver If you are using Dbeaver make sure to select Driver for MySQL 8 or later. If the driver is missing you can download it and add it from the database-drivers section . From the navigation menu, click Connect to database. Search MySQL 8+ . Select the MySQL 8+ or MySQL . Click on Next . Add the Hostname (cloud-mysql.mindsdb.com). Add the Database name (leave empty). Add Port (3306). Add the database user (your MindsDB Cloud username). Add Password for the user (your MindsDB Cloud password). Click on Finish . MindsDB Database At startup mindsdb database will contain 2 tables predictors and commands . All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: name - The name of the model. status - Training status(training, complete, error). accuracy - The model accuracy. predict - The name of the target variable. select_data_query - SQL select query to create the datasource. training options - Additional training parameters. The full list can be found at Predictor Interface docs . Whitelist MindsDB Cloud IP address If you need to whitelist MindsDB Cloud IP address to have access to your database, reach out to MindsDB team so we can share the Cloud static IP with you.","title":"MindsDB Cloud"},{"location":"sql/connect/cloud/#mindsdb-cloud-as-a-sql-database","text":"MindsDB Cloud provides a powerful MySQL API that allows cloud users to connect to it. The first step to connect is to use the MindsDB Cloud user. If you haven't signup to the MindsDB Cloud follow the steps explained here . After that you can use one of the below db clients: MySQL Command-Line Client DBeaver","title":"MindsDB Cloud as a SQL Database"},{"location":"sql/connect/cloud/#mysql-client","text":"Open mysql client and run: mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p The required parameters are: -h: Host name of mindsdbs mysql api (by default takes cloud.mindsdb.com if not specified) --port: TCP/IP port number for connection(3306) -u: MindsDB Cloud username -p: MindsDB Cloud password","title":"MySQL client"},{"location":"sql/connect/cloud/#dbeaver","text":"If you are using Dbeaver make sure to select Driver for MySQL 8 or later. If the driver is missing you can download it and add it from the database-drivers section . From the navigation menu, click Connect to database. Search MySQL 8+ . Select the MySQL 8+ or MySQL . Click on Next . Add the Hostname (cloud-mysql.mindsdb.com). Add the Database name (leave empty). Add Port (3306). Add the database user (your MindsDB Cloud username). Add Password for the user (your MindsDB Cloud password). Click on Finish .","title":"Dbeaver"},{"location":"sql/connect/cloud/#mindsdb-database","text":"At startup mindsdb database will contain 2 tables predictors and commands . All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: name - The name of the model. status - Training status(training, complete, error). accuracy - The model accuracy. predict - The name of the target variable. select_data_query - SQL select query to create the datasource. training options - Additional training parameters. The full list can be found at Predictor Interface docs . Whitelist MindsDB Cloud IP address If you need to whitelist MindsDB Cloud IP address to have access to your database, reach out to MindsDB team so we can share the Cloud static IP with you.","title":"MindsDB Database"},{"location":"sql/connect/local/","text":"MindsDB as a SQL Database MindsDB provides a powerful MySQL API that allows users to connect to it using the MySQL Command-Line Client or DBeaver . By default, MindsDB Server will start the HTTP and MySQL APIs. If you want to run only the MySQL API you can provide that as a parameter on the server start: python3 -m mindsdb --api=http,mysql This will start MySQL API on a 127.0.0.1:47335 with mindsdb as default user and create a mindsdb database. To change the default parameters you need to extend the MindsDBs config.json or create another config and send it as a parameter to the serve start command as: python3 -m mindsdb --api=http,mysql --config=config.json In case you are using Docker, visit the Docker extend config docs . To read more about available config.json options check the configuration docs . Connect Connecting to the localhost Make sure you always use 127.0.0.1 locally instead of localhost as a hostname. Connecting to MySQL API is the same as connecting to a MySQL database. You can use one of the below clients to connect: MySQL Command-Line Client DBeaver MySQL client Open mysql client and run: mysql -h 127.0.0.1 --port 47335 -u mindsdb -p The required parameters are: -h: Host name of mindsdbs mysql api (127.0.0.1). --port: TCP/IP port number for connection(47335). -u: MySQL user name to use when connecting(default mindsdb). -p: Password to use when connecting(default no password). Dbeaver If you are using Dbeaver make sure to select Driver for MySQL 8 or later. If the driver is missing you can download it and add it from the database-drivers section . From the navigation menu, click Connect to database. Search MySQL 8+ . Select the MySQL 8+ or MySQL . Click on Next . Add the Hostname (127.0.0.1). Add the Database name (leave empty). Add Port (47335). Add the database user (default mindsdb). Add Password for the user (default empty). Click on Finish . MindsDB Database On startup the mindsdb database will contain 2 tables predictors and commands . All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: name - The name of the model. status - Training status(training, complete, error). accuracy - The model accuracy. predict - The name of the target variable. select_data_query - SQL select query to create the datasource. training options - Additional training parameters. The full list can be found at Predictor Interface docs .","title":"Local Deployment"},{"location":"sql/connect/local/#mindsdb-as-a-sql-database","text":"MindsDB provides a powerful MySQL API that allows users to connect to it using the MySQL Command-Line Client or DBeaver . By default, MindsDB Server will start the HTTP and MySQL APIs. If you want to run only the MySQL API you can provide that as a parameter on the server start: python3 -m mindsdb --api=http,mysql This will start MySQL API on a 127.0.0.1:47335 with mindsdb as default user and create a mindsdb database. To change the default parameters you need to extend the MindsDBs config.json or create another config and send it as a parameter to the serve start command as: python3 -m mindsdb --api=http,mysql --config=config.json In case you are using Docker, visit the Docker extend config docs . To read more about available config.json options check the configuration docs .","title":"MindsDB as a SQL Database"},{"location":"sql/connect/local/#connect","text":"Connecting to the localhost Make sure you always use 127.0.0.1 locally instead of localhost as a hostname. Connecting to MySQL API is the same as connecting to a MySQL database. You can use one of the below clients to connect: MySQL Command-Line Client DBeaver","title":"Connect"},{"location":"sql/connect/local/#mysql-client","text":"Open mysql client and run: mysql -h 127.0.0.1 --port 47335 -u mindsdb -p The required parameters are: -h: Host name of mindsdbs mysql api (127.0.0.1). --port: TCP/IP port number for connection(47335). -u: MySQL user name to use when connecting(default mindsdb). -p: Password to use when connecting(default no password).","title":"MySQL client"},{"location":"sql/connect/local/#dbeaver","text":"If you are using Dbeaver make sure to select Driver for MySQL 8 or later. If the driver is missing you can download it and add it from the database-drivers section . From the navigation menu, click Connect to database. Search MySQL 8+ . Select the MySQL 8+ or MySQL . Click on Next . Add the Hostname (127.0.0.1). Add the Database name (leave empty). Add Port (47335). Add the database user (default mindsdb). Add Password for the user (default empty). Click on Finish .","title":"Dbeaver"},{"location":"sql/connect/local/#mindsdb-database","text":"On startup the mindsdb database will contain 2 tables predictors and commands . All of the newly trained machine learning models will be visible as a new record inside the predictors table. The predictors columns contains information about each model as: name - The name of the model. status - Training status(training, complete, error). accuracy - The model accuracy. predict - The name of the target variable. select_data_query - SQL select query to create the datasource. training options - Additional training parameters. The full list can be found at Predictor Interface docs .","title":"MindsDB Database"},{"location":"sql/tutorials/ai-tables/","text":"AI Tables Intro There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will we predict can happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models you train this data on. Data Scientists and Data Engineers need best-in-class tools to prepare the data for feature engineering, the best training models, and the best way of deploying, monitoring, and managing these implementations for optimal prediction confidence. Machine Learning (ML) Lifecycle The ML lifecycle can be represented as a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Companies looking to implement machine learning have found their current solutions require substantial amounts of data preparation, cleaning, and labeling, plus hard to find machine learning/AI data scientists to conduct feature engineering; build, train, and optimize models; assemble, verify, and deploy into production; and then monitor in real-time, improve, and refine. Machine learning models require multiple iterations with existing data to train. Additionally, extracting, transforming, and loading (ETL) data from one system to another is complicated, leads to multiple copies of information, and is a compliance and tracking nightmare. A recent study has shown it takes 64% of companies a month, to over a year, to deploy a machine learning model into production\u00b9. Leveraging existing databases and automating the feature engineering, building, training, and optimization of models, assembling them, and deploying them into production is called AutoML and has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications. MindsDB brings machine learning to existing SQL databases with a concept called AI Tables. AI Tables integrate the machine learning models as virtual tables inside a database, create predictions, and can be queried with simple SQL statements. Almost instantly, time series, regression, and classification predictions can be done directly in your database. Deep Dive into the AI Tables Let\u2019s consider the following income table that stores the income and debt values. SELECT income , debt FROM income_table ; A simple visualization of the data present in the income table is as follows. Querying the income table to get the debt value for a particular income value results in the following. SELECT income , debt FROM income WHERE income = 80000 ; But what happens when we query the table for income value that is not present? SELECT income , debt FROM income WHERE income = 90000 ; When a table doesn\u2019t have an exact match the query will return a null value. This is where the AI Tables come into play! Let\u2019s create a debt model that allows us to approximate the debt value for any income value. We\u2019ll train this debt model using the income table\u2019s data. CREATE PREDICTOR debt_model FROM income_table PREDICT debt ; MindsDB provides the CREATE PREDICTOR statement. When we execute this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows. Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the debt_model and not the income table. SELECT income , debt FROM debt_model WHERE income = 90120 ;","title":"AI Tables Intro"},{"location":"sql/tutorials/ai-tables/#ai-tables-intro","text":"There is an ongoing transformational shift within the modern business world from the \u201cwhat happened and why\u201d based on historical data analysis to the \u201cwhat will we predict can happen and how can we make it happen\u201d based on machine learning predictive modeling. The success of your predictions depends both on the data you have available and the models you train this data on. Data Scientists and Data Engineers need best-in-class tools to prepare the data for feature engineering, the best training models, and the best way of deploying, monitoring, and managing these implementations for optimal prediction confidence.","title":"AI Tables Intro"},{"location":"sql/tutorials/ai-tables/#machine-learning-ml-lifecycle","text":"The ML lifecycle can be represented as a process that consists of the data preparation phase, modeling phase, and deployment phase. The diagram below presents all the steps included in each of the stages. Companies looking to implement machine learning have found their current solutions require substantial amounts of data preparation, cleaning, and labeling, plus hard to find machine learning/AI data scientists to conduct feature engineering; build, train, and optimize models; assemble, verify, and deploy into production; and then monitor in real-time, improve, and refine. Machine learning models require multiple iterations with existing data to train. Additionally, extracting, transforming, and loading (ETL) data from one system to another is complicated, leads to multiple copies of information, and is a compliance and tracking nightmare. A recent study has shown it takes 64% of companies a month, to over a year, to deploy a machine learning model into production\u00b9. Leveraging existing databases and automating the feature engineering, building, training, and optimization of models, assembling them, and deploying them into production is called AutoML and has been gaining traction within enterprises for enabling non-experts to use machine learning models for practical applications. MindsDB brings machine learning to existing SQL databases with a concept called AI Tables. AI Tables integrate the machine learning models as virtual tables inside a database, create predictions, and can be queried with simple SQL statements. Almost instantly, time series, regression, and classification predictions can be done directly in your database.","title":"Machine Learning (ML) Lifecycle"},{"location":"sql/tutorials/ai-tables/#deep-dive-into-the-ai-tables","text":"Let\u2019s consider the following income table that stores the income and debt values. SELECT income , debt FROM income_table ; A simple visualization of the data present in the income table is as follows. Querying the income table to get the debt value for a particular income value results in the following. SELECT income , debt FROM income WHERE income = 80000 ; But what happens when we query the table for income value that is not present? SELECT income , debt FROM income WHERE income = 90000 ; When a table doesn\u2019t have an exact match the query will return a null value. This is where the AI Tables come into play! Let\u2019s create a debt model that allows us to approximate the debt value for any income value. We\u2019ll train this debt model using the income table\u2019s data. CREATE PREDICTOR debt_model FROM income_table PREDICT debt ; MindsDB provides the CREATE PREDICTOR statement. When we execute this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows. Let\u2019s now look for the debt value of some random income value. To get the approximated debt value, we query the debt_model and not the income table. SELECT income , debt FROM debt_model WHERE income = 90120 ;","title":"Deep Dive into the AI Tables"},{"location":"sql/tutorials/bitcoin-forecasting/","text":"Forecast Bitcoin price using MindsDB Level: Easy Dataset: Coinbase 2017-2018 Bitcoin data Bitcoin is a digital currency that uses blockchain technology, Bitcoin can be sent from user to user on the peer-to-peer Bitcoin network without the need for intermediaries. Note that this is just a task for fun so use it at your own risk. In this tutorial, you will learn how to forecast Bitcoin using MindsDB. And all you need to know is just SQL. Behind the scenes, MindsDB will create the complete machine learning workflow, like determine, normalize & encode the data, train & test the model, etc. But we don\u2019t need to bother with all this complexity. Of course, if you want to, you can tune things manually inside MindsDB with a declarative syntax caled JSON-AI, but we will not cover it in this article. DISCLAIMER: Please note that predicting Bitcoin price is just an example for showing MindsDB technology and you are solely responsible for any results you may get in real life, if you use this information for real trading purposes. Please note, that you can also follow this tutorial with other data you have. Pre-requisites First, you need MindsDB installed. If you want to use MindsDB locally, you need to install MindsDB with Docker or Python . However, if you want to use MindsDB without installing it locally, you can use Cloud Mindsdb . In this tutorial, I'm using MindsDB Cloud, because it is easy to set up in just 2 minutes and it has a great free tier. Second, you need a MySQL client to connect to MindsDB MYSQL API. Connect your database You must first connect MindsDB to the database where the record is stored. In the left navigation, click Database, click ADD DATABASE. And you must provide all the necessary parameters to connect to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it as you like Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction. Connect to MindsDB\u2019s MySQL API In this tutorial I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. Data Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and in this tutorial, we will use the Bitcoin table. And let's check what is inside this table. These tables have 5 columns: date, open price, the highest price of the day, lowest price of the day, and close price. The column we want to forecast is close price. Create the model Now, to create the model let's move to MindsDB database. and let's see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table, and using this command: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_target as column_alias ORDER BY column_orderby WINDOW num_window HORIZON num_horizon USING { \"is_timeseries\" : \"Yes\" } The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_target (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. column_orderby - The column to order the data, for time series this should be the date/time column. num_window - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". num_horizon - keyword specifies the number of future predictions. So, use this command to create the models: If there's no error, that means your model is created and training. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the model is finished, it will look like this. The model has been created! and the accuracy is 99%! Create the prediction Now you are in the last step of this tutorial, creating the prediction. To create a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ; And you need to set these values: - target_variable - The original value of the target variable. - target_variable_confidence - Model confidence score. - target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. - when_data - The data to make the predictions from(WHERE clause params). Finally, we have created a Bitcoin forecasting model using SQL and MindsDB. Yayyy! Conclusions As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself! MindsDB free cloud account is fast to set up and has more than enough to give it a try. Or use the open source version if you want to.","title":"Forecast Bitcoin price using MindsDB"},{"location":"sql/tutorials/bitcoin-forecasting/#forecast-bitcoin-price-using-mindsdb","text":"Level: Easy Dataset: Coinbase 2017-2018 Bitcoin data Bitcoin is a digital currency that uses blockchain technology, Bitcoin can be sent from user to user on the peer-to-peer Bitcoin network without the need for intermediaries. Note that this is just a task for fun so use it at your own risk. In this tutorial, you will learn how to forecast Bitcoin using MindsDB. And all you need to know is just SQL. Behind the scenes, MindsDB will create the complete machine learning workflow, like determine, normalize & encode the data, train & test the model, etc. But we don\u2019t need to bother with all this complexity. Of course, if you want to, you can tune things manually inside MindsDB with a declarative syntax caled JSON-AI, but we will not cover it in this article. DISCLAIMER: Please note that predicting Bitcoin price is just an example for showing MindsDB technology and you are solely responsible for any results you may get in real life, if you use this information for real trading purposes. Please note, that you can also follow this tutorial with other data you have.","title":"Forecast Bitcoin price using MindsDB"},{"location":"sql/tutorials/bitcoin-forecasting/#pre-requisites","text":"First, you need MindsDB installed. If you want to use MindsDB locally, you need to install MindsDB with Docker or Python . However, if you want to use MindsDB without installing it locally, you can use Cloud Mindsdb . In this tutorial, I'm using MindsDB Cloud, because it is easy to set up in just 2 minutes and it has a great free tier. Second, you need a MySQL client to connect to MindsDB MYSQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/bitcoin-forecasting/#connect-your-database","text":"You must first connect MindsDB to the database where the record is stored. In the left navigation, click Database, click ADD DATABASE. And you must provide all the necessary parameters to connect to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it as you like Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction.","title":"Connect your database"},{"location":"sql/tutorials/bitcoin-forecasting/#connect-to-mindsdbs-mysql-api","text":"In this tutorial I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud.","title":"Connect to MindsDB\u2019s MySQL API"},{"location":"sql/tutorials/bitcoin-forecasting/#data","text":"Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and in this tutorial, we will use the Bitcoin table. And let's check what is inside this table. These tables have 5 columns: date, open price, the highest price of the day, lowest price of the day, and close price. The column we want to forecast is close price.","title":"Data"},{"location":"sql/tutorials/bitcoin-forecasting/#create-the-model","text":"Now, to create the model let's move to MindsDB database. and let's see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table, and using this command: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_target as column_alias ORDER BY column_orderby WINDOW num_window HORIZON num_horizon USING { \"is_timeseries\" : \"Yes\" } The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_target (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. column_orderby - The column to order the data, for time series this should be the date/time column. num_window - keyword specifies the number of rows to \"look back\" into when making a prediction after the rows are ordered by the order_by column and split into groups. This could be used to specify something like \"Always use the previous 10 rows\". num_horizon - keyword specifies the number of future predictions. So, use this command to create the models: If there's no error, that means your model is created and training. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the model is finished, it will look like this. The model has been created! and the accuracy is 99%!","title":"Create the model"},{"location":"sql/tutorials/bitcoin-forecasting/#create-the-prediction","text":"Now you are in the last step of this tutorial, creating the prediction. To create a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ; And you need to set these values: - target_variable - The original value of the target variable. - target_variable_confidence - Model confidence score. - target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. - when_data - The data to make the predictions from(WHERE clause params). Finally, we have created a Bitcoin forecasting model using SQL and MindsDB. Yayyy!","title":"Create the prediction"},{"location":"sql/tutorials/bitcoin-forecasting/#conclusions","text":"As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself! MindsDB free cloud account is fast to set up and has more than enough to give it a try. Or use the open source version if you want to.","title":"Conclusions"},{"location":"sql/tutorials/bodyfat/","text":"Pre-requisites Before you start make sure that you've: Visited Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. The dataset being used for this tutorial. Get it from Kaggle . Determining Body Fat Percentage Machine Learning powered data analysis can be performed quickly and efficiently by MindsDB to enable individuals to make accurate predictions for certain metrics based on a variety of associated values. MindsDB enables you to make predictions automatically using just SQL commands, all the ML workflow is automated, and abstracted as virtual \u201cAI tables\u201d in your database so you may start getting insights from forecasts right away. In this tutorial, we'll be using MindsDB and a MySQL database to predict body fat percentage based on several body part measurement criteria. Data Overview For this tutorial, we'll be using the Body Fat Prediction dataset available at Kaggle . Each row represents one person and we'll train an ML model to help us predict an individual's body fat percentage using MindsDB. Below is a short description of each feature of the data: - Density: Individual's body density as determined by underwater weighing (float) - BodyFat: The individual's determined body fat percentage (float). This is what we want to predict - Age: Age of the individual (int) - Weight: Weight of the individual in pounds (float) - Height: Height of the individual in inches (float) - Neck: Circumference of the individual's neck in cm (float) - Chest: Circumference of the individual's chest in cm (float) - Abdomen: Circumference of the individual's abdomen in cm (float) - Hip: Circumference of the individual's hips in cm (float) - Thigh: Circumference of the individual's thigh in cm (float) - Knee: Circumference of the individual's knee in cm (float) - Ankle: Circumference of the individual's ankle in cm (float) - Biceps: Circumference of the individual's biceps in cm (float) - Forearm: Circumference of the individual's forearm in cm (float) - Wrist: Circumference of the individual's wrist in cm (float) Upload a file Click on Files icon to go to datasets page Click on FILE UPLOAD button to upload file into MindsDB Connect to MindsDB SQL Sever mysql - h cloud . mindsdb . com --port 3306 -u username@email.com -p USE mindsdb ; Create a predictor Now, we have to create a predictor based on the following syntax: CREATE PREDICTOR predictor_name FROM files ( SELECT column_name , column_name2 FROM file_name ) as ds_name PREDICT column_name as column_alias ; For our case, we'll enter the following command: CREATE PREDICTOR bodyfat_predictor FROM files ( SELECT * FROM bodyfat ) PREDICT Bodyfat ; You should see output similar to the following: Query OK, 0 rows affected (3.077 sec) At this point, the predictor will immediately begin training. Check the status of the training by entering the command: SELECT * FROM mindsdb . predictors WHERE name = 'bodyfat_predictor' ; When complete, you should see output similar to the following: +-------------------+----------+--------------------+---------+-------------------+------------------+ | name | status | accuracy | predict | select_data_query | training_options | +-------------------+----------+--------------------+---------+-------------------+------------------+ | bodyfat_predictor | complete | 0.9909730079130395 | BodyFat | | | +-------------------+----------+--------------------+---------+-------------------+------------------+ 1 row in set (0.101 sec) As you can see, the predictor training has completed with an accuracy of approximately 99%. At this point, you have successfully trained an ML model for our Body Fat Prediction dataset! Using SQL Commands to Make Predictions Now, we can query the model and make predictions based on our input data by using SQL statements. Let's imagine an individual aged 25, with a body density of 1.08, a weight of 170lb, a height of 70in, a neck circumference of 38.1cm, a chest circumference of 103.5cm, an abdomen circumference of 85.4cm, a hip circumference of 102.2cm, a thigh circumference of 63.0cm, a knee circumference of 39.4cm, an ankle circumference of 22.8cm, a biceps circumference of 33.3cm, a forearm circumference of 28.7cm, and a wrist circumference of 18.3cm. We can predict this person's body fat percentage by issuing the following command: SELECT BodyFat , BodyFat_confidence , BodyFat_explain AS Info FROM mindsdb . bodyfat_predictor WHERE when_data = '{\"Density\": 1.08, \"Age\": 25, \"Weight\": 170, \"Height\": 70, \"Neck\": 38.1, \"Chest\": 103.5, \"Abdomen\": 85.4, \"Hip\": 102.2, \"Thigh\": 63.0, \"Knee\": 39.4, \"Ankle\": 22.8, \"Biceps\": 33.3, \"Forearm\": 28.7, \"Wrist\": 18.3}' ; This should return output similar to: +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | BodyFat | BodyFat_confidence | Info | +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | 8.968581383955318 | 0.99 | {\"predicted_value\": 8.968581383955318, \"confidence\": 0.99, \"confidence_lower_bound\": 5.758912817402102, \"confidence_upper_bound\": 12.178249950508533, \"anomaly\": null, \"truth\": null} | +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.464 sec) As you can see, with around 99% confidence, MindsDB predicted the body fat percentage for this individual at 8.97%. You can at this point feel free to alter the prospective individual's bodypart measurement parameters and make additional prediction queries if you'd like. Making Batch Predictions using the JOIN Command The above example showed how to make predictions for a single individual's bodyfat, but what if you had a table of bodypart measurements for a number of individuals, and wanted to make predictions for them all? This is possible using the JOIN command , which allows for the combining of rows from a database table and the prediction model table on a related column. The basic syntax to use the JOIN command is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our purposes, we'll re-use the original data set, taking the Age, Density, Weight, Height, Neck circumference, Chest circumference, Abdomen circumference, and Hip circumference fields. We'll also include the original BodyFat percentage to compare our predicted values against the originals. Execute the following command: SELECT t . Age , t . Density , t . Weight , t . Height , t . Neck , t . Chest , t . Abdomen , t . Hip , t . BodyFat , p . BodyFat AS predicted_BodyFat FROM bodyfat_integration . bodyfat AS t JOIN mindsdb . bodyfat_predictor AS p LIMIT 5 ; This should return an output table similar to the following: +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ | Age | Density | Weight | Height | Neck | Chest | Abdomen | Hip | BodyFat | predicted_BodyFat | +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ | 23 | 1.0708 | 154.25 | 67.75 | 36.2 | 93.1 | 85.2 | 94.5 | 12.3 | 12.475132275112655 | | 22 | 1.0853 | 173.25 | 72.25 | 38.5 | 93.6 | 83.0 | 98.7 | 6.1 | 6.07133439184195 | | 22 | 1.0414 | 154.0 | 66.25 | 34.0 | 95.8 | 87.9 | 99.2 | 25.3 | 25.156538398443754 | | 26 | 1.0751 | 184.75 | 72.25 | 37.4 | 101.8 | 86.4 | 101.2 | 10.4 | 10.696461885516461 | | 24 | 1.034 | 184.25 | 71.25 | 34.4 | 97.3 | 100.0 | 101.9 | 28.7 | 28.498772660802427 | +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ 5 rows in set (1.091 sec) As you can see, a prediction has been generated for each row in the input table. Additionally, our predicted bodyfat percentages align closely with the original values! Note that even though we chose only to display the Age, Density, Weight, Height, Neck, Chest, Abdomen, and Hip measurements in this example, the predicted_BodyFat field was determined by taking into consideration all of the data fields in the original bodyfat table (as this table was JOINed with the bodyfat_predictor table, from which we selected the specified fields). In order to make predictions based ONLY on the specified fields, we would have to create a new table containing only those fields, and JOIN that with the bodyfat_predictor table! You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this check out MindsDB documentation .","title":"Body Fat Prediction"},{"location":"sql/tutorials/bodyfat/#pre-requisites","text":"Before you start make sure that you've: Visited Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. The dataset being used for this tutorial. Get it from Kaggle .","title":"Pre-requisites"},{"location":"sql/tutorials/bodyfat/#determining-body-fat-percentage","text":"Machine Learning powered data analysis can be performed quickly and efficiently by MindsDB to enable individuals to make accurate predictions for certain metrics based on a variety of associated values. MindsDB enables you to make predictions automatically using just SQL commands, all the ML workflow is automated, and abstracted as virtual \u201cAI tables\u201d in your database so you may start getting insights from forecasts right away. In this tutorial, we'll be using MindsDB and a MySQL database to predict body fat percentage based on several body part measurement criteria.","title":"Determining Body Fat Percentage"},{"location":"sql/tutorials/bodyfat/#data-overview","text":"For this tutorial, we'll be using the Body Fat Prediction dataset available at Kaggle . Each row represents one person and we'll train an ML model to help us predict an individual's body fat percentage using MindsDB. Below is a short description of each feature of the data: - Density: Individual's body density as determined by underwater weighing (float) - BodyFat: The individual's determined body fat percentage (float). This is what we want to predict - Age: Age of the individual (int) - Weight: Weight of the individual in pounds (float) - Height: Height of the individual in inches (float) - Neck: Circumference of the individual's neck in cm (float) - Chest: Circumference of the individual's chest in cm (float) - Abdomen: Circumference of the individual's abdomen in cm (float) - Hip: Circumference of the individual's hips in cm (float) - Thigh: Circumference of the individual's thigh in cm (float) - Knee: Circumference of the individual's knee in cm (float) - Ankle: Circumference of the individual's ankle in cm (float) - Biceps: Circumference of the individual's biceps in cm (float) - Forearm: Circumference of the individual's forearm in cm (float) - Wrist: Circumference of the individual's wrist in cm (float)","title":"Data Overview"},{"location":"sql/tutorials/bodyfat/#upload-a-file","text":"Click on Files icon to go to datasets page Click on FILE UPLOAD button to upload file into MindsDB","title":"Upload a file"},{"location":"sql/tutorials/bodyfat/#connect-to-mindsdb-sql-sever","text":"mysql - h cloud . mindsdb . com --port 3306 -u username@email.com -p USE mindsdb ;","title":"Connect to MindsDB SQL Sever"},{"location":"sql/tutorials/bodyfat/#create-a-predictor","text":"Now, we have to create a predictor based on the following syntax: CREATE PREDICTOR predictor_name FROM files ( SELECT column_name , column_name2 FROM file_name ) as ds_name PREDICT column_name as column_alias ; For our case, we'll enter the following command: CREATE PREDICTOR bodyfat_predictor FROM files ( SELECT * FROM bodyfat ) PREDICT Bodyfat ; You should see output similar to the following: Query OK, 0 rows affected (3.077 sec) At this point, the predictor will immediately begin training. Check the status of the training by entering the command: SELECT * FROM mindsdb . predictors WHERE name = 'bodyfat_predictor' ; When complete, you should see output similar to the following: +-------------------+----------+--------------------+---------+-------------------+------------------+ | name | status | accuracy | predict | select_data_query | training_options | +-------------------+----------+--------------------+---------+-------------------+------------------+ | bodyfat_predictor | complete | 0.9909730079130395 | BodyFat | | | +-------------------+----------+--------------------+---------+-------------------+------------------+ 1 row in set (0.101 sec) As you can see, the predictor training has completed with an accuracy of approximately 99%. At this point, you have successfully trained an ML model for our Body Fat Prediction dataset!","title":"Create a predictor"},{"location":"sql/tutorials/bodyfat/#using-sql-commands-to-make-predictions","text":"Now, we can query the model and make predictions based on our input data by using SQL statements. Let's imagine an individual aged 25, with a body density of 1.08, a weight of 170lb, a height of 70in, a neck circumference of 38.1cm, a chest circumference of 103.5cm, an abdomen circumference of 85.4cm, a hip circumference of 102.2cm, a thigh circumference of 63.0cm, a knee circumference of 39.4cm, an ankle circumference of 22.8cm, a biceps circumference of 33.3cm, a forearm circumference of 28.7cm, and a wrist circumference of 18.3cm. We can predict this person's body fat percentage by issuing the following command: SELECT BodyFat , BodyFat_confidence , BodyFat_explain AS Info FROM mindsdb . bodyfat_predictor WHERE when_data = '{\"Density\": 1.08, \"Age\": 25, \"Weight\": 170, \"Height\": 70, \"Neck\": 38.1, \"Chest\": 103.5, \"Abdomen\": 85.4, \"Hip\": 102.2, \"Thigh\": 63.0, \"Knee\": 39.4, \"Ankle\": 22.8, \"Biceps\": 33.3, \"Forearm\": 28.7, \"Wrist\": 18.3}' ; This should return output similar to: +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | BodyFat | BodyFat_confidence | Info | +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | 8.968581383955318 | 0.99 | {\"predicted_value\": 8.968581383955318, \"confidence\": 0.99, \"confidence_lower_bound\": 5.758912817402102, \"confidence_upper_bound\": 12.178249950508533, \"anomaly\": null, \"truth\": null} | +-------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.464 sec) As you can see, with around 99% confidence, MindsDB predicted the body fat percentage for this individual at 8.97%. You can at this point feel free to alter the prospective individual's bodypart measurement parameters and make additional prediction queries if you'd like.","title":"Using SQL Commands to Make Predictions"},{"location":"sql/tutorials/bodyfat/#making-batch-predictions-using-the-join-command","text":"The above example showed how to make predictions for a single individual's bodyfat, but what if you had a table of bodypart measurements for a number of individuals, and wanted to make predictions for them all? This is possible using the JOIN command , which allows for the combining of rows from a database table and the prediction model table on a related column. The basic syntax to use the JOIN command is: SELECT t . column_name1 , t . column_name2 , FROM integration_name . table AS t JOIN mindsdb . predictor_name AS p WHERE t . column_name IN ( value1 , value2 , ...); For our purposes, we'll re-use the original data set, taking the Age, Density, Weight, Height, Neck circumference, Chest circumference, Abdomen circumference, and Hip circumference fields. We'll also include the original BodyFat percentage to compare our predicted values against the originals. Execute the following command: SELECT t . Age , t . Density , t . Weight , t . Height , t . Neck , t . Chest , t . Abdomen , t . Hip , t . BodyFat , p . BodyFat AS predicted_BodyFat FROM bodyfat_integration . bodyfat AS t JOIN mindsdb . bodyfat_predictor AS p LIMIT 5 ; This should return an output table similar to the following: +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ | Age | Density | Weight | Height | Neck | Chest | Abdomen | Hip | BodyFat | predicted_BodyFat | +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ | 23 | 1.0708 | 154.25 | 67.75 | 36.2 | 93.1 | 85.2 | 94.5 | 12.3 | 12.475132275112655 | | 22 | 1.0853 | 173.25 | 72.25 | 38.5 | 93.6 | 83.0 | 98.7 | 6.1 | 6.07133439184195 | | 22 | 1.0414 | 154.0 | 66.25 | 34.0 | 95.8 | 87.9 | 99.2 | 25.3 | 25.156538398443754 | | 26 | 1.0751 | 184.75 | 72.25 | 37.4 | 101.8 | 86.4 | 101.2 | 10.4 | 10.696461885516461 | | 24 | 1.034 | 184.25 | 71.25 | 34.4 | 97.3 | 100.0 | 101.9 | 28.7 | 28.498772660802427 | +------+---------+--------+--------+------+-------+---------+-------+---------+--------------------+ 5 rows in set (1.091 sec) As you can see, a prediction has been generated for each row in the input table. Additionally, our predicted bodyfat percentages align closely with the original values! Note that even though we chose only to display the Age, Density, Weight, Height, Neck, Chest, Abdomen, and Hip measurements in this example, the predicted_BodyFat field was determined by taking into consideration all of the data fields in the original bodyfat table (as this table was JOINed with the bodyfat_predictor table, from which we selected the specified fields). In order to make predictions based ONLY on the specified fields, we would have to create a new table containing only those fields, and JOIN that with the bodyfat_predictor table! You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this check out MindsDB documentation .","title":"Making Batch Predictions using the JOIN Command"},{"location":"sql/tutorials/byom/","text":"Bring Your Own Model Mindsdb allows you to integrate your own machine learning models into it. In order to do this your model will require some sort of API wrapper, for now we have 2x API specifications we support, mlflow and ray serve. The former supports importing already trained models and predicting with them from mindsdb. The later supports both training and predicting with external models. In order to use custom models there are three mandatory arguments one must past inside the USING statement: - url.predict , this is the url to call for getting predictions from your model - format , this can be == mlflow or ray_serve - dtype_dict , this is a json specifying all columns expected by their models and their types There's an additional optional argument if you want to train the model via mindsdb: - url.train , which is the endpoint we'll call when training your model Ray Server Ray serve is a simple high-thoroughput service that can wrap over your own ml models. import ray from fastapi import Request, FastAPI from ray import serve import time import pandas as pd import json from sklearn.linear_model import LogisticRegression app = FastAPI() ray.init() serve.start(detached=True) async def parse_req(request: Request): data = await request.json() target = data.get('target', None) di = json.loads(data['df']) df = pd.DataFrame(di) return df, target @serve.deployment(route_prefix=\"/my_model\") @serve.ingress(app) class MyModel: @app.post(\"/train\") async def train(self, request: Request): df, target = await parse_req(request) feature_cols = list(set(list(df.columns)) - set([target])) self.feature_cols = feature_cols X = df.loc[:, self.feature_cols] Y = list(df[target]) self.model = LogisticRegression() self.model.fit(X, Y) return {'status': 'ok'} @app.post(\"/predict\") async def predict(self, request: Request): df, _ = await parse_req(request) X = df.loc[:, self.feature_cols] predictions = self.model.predict(X) pred_dict = {'prediction': [float(x) for x in predictions]} return pred_dict MyModel.deploy() while True: time.sleep(1) The important bits here are having a train and predict endpoint. The train endpoint accept two parameters in the json sent via POST: - df -- a serialized dictionary that can be converted into a pandas dataframe - target -- the name of the target column The predict endpoint needs only one parameter: - df -- a serialized dictionary that can be converted into a pandas dataframe The training endpoints must return a json that contains the keys status set to ok . The predict endpoint must return a dictionayr containing the prediction key, storing the predictions. Additional keys can be returned for confidence and confidence intervals. Once you start this ray serve wrapped model you can train it via the query: CREATE PREDICTOR byom FROM mydb ( SELECT number_of_rooms, initial_price, rental_price FROM test_data.home_rentals ) PREDICT number_of_rooms USING url.train = 'http://127.0.0.1:8000/my_model/train', url.predict = 'http://127.0.0.1:8000/my_model/predict', dtype_dict={\"number_of_rooms\": \"categorical\", \"initial_price\": \"integer\", \"rental_price\": \"integer\"}, format='ray_server'; And you can query predictions as usual: SELECT * FROM byom WHERE initial_price=3000 AND rental_price=3000; or by JOINING SELECT tb.number_of_rooms, t.rental_price FROM mydb.test_data.home_rentals AS t JOIN mindsdb.byom AS tb WHERE t.rental_price > 5300; MLFlow MLFlow is a tool that you can use to train and serve models. As training is done through code rather than the API, that bit you will have to do from outside of mindsdb by pulling your data manually. The first step would be to install mlflow and get a model going, you can use the one in this very simple tutorial: https://github.com/mlflow/mlflow#saving-and-serving-models Next, we're going to add this to mindsdb: CREATE PREDICTOR byom2 PREDICT `1` USING url.predict='http://localhost:5000/invocations', format='mlflow', data_dtype={\"0\": \"integer\", \"1\": \"integer\"} Then we can run predictions as usual, by using the WHERE statement or joining on a data table with an appropriate schema: SELECT y FROM byom2 WHERE `0`=2;","title":"Bring Your Own Model"},{"location":"sql/tutorials/byom/#bring-your-own-model","text":"Mindsdb allows you to integrate your own machine learning models into it. In order to do this your model will require some sort of API wrapper, for now we have 2x API specifications we support, mlflow and ray serve. The former supports importing already trained models and predicting with them from mindsdb. The later supports both training and predicting with external models. In order to use custom models there are three mandatory arguments one must past inside the USING statement: - url.predict , this is the url to call for getting predictions from your model - format , this can be == mlflow or ray_serve - dtype_dict , this is a json specifying all columns expected by their models and their types There's an additional optional argument if you want to train the model via mindsdb: - url.train , which is the endpoint we'll call when training your model","title":"Bring Your Own Model"},{"location":"sql/tutorials/byom/#ray-server","text":"Ray serve is a simple high-thoroughput service that can wrap over your own ml models. import ray from fastapi import Request, FastAPI from ray import serve import time import pandas as pd import json from sklearn.linear_model import LogisticRegression app = FastAPI() ray.init() serve.start(detached=True) async def parse_req(request: Request): data = await request.json() target = data.get('target', None) di = json.loads(data['df']) df = pd.DataFrame(di) return df, target @serve.deployment(route_prefix=\"/my_model\") @serve.ingress(app) class MyModel: @app.post(\"/train\") async def train(self, request: Request): df, target = await parse_req(request) feature_cols = list(set(list(df.columns)) - set([target])) self.feature_cols = feature_cols X = df.loc[:, self.feature_cols] Y = list(df[target]) self.model = LogisticRegression() self.model.fit(X, Y) return {'status': 'ok'} @app.post(\"/predict\") async def predict(self, request: Request): df, _ = await parse_req(request) X = df.loc[:, self.feature_cols] predictions = self.model.predict(X) pred_dict = {'prediction': [float(x) for x in predictions]} return pred_dict MyModel.deploy() while True: time.sleep(1) The important bits here are having a train and predict endpoint. The train endpoint accept two parameters in the json sent via POST: - df -- a serialized dictionary that can be converted into a pandas dataframe - target -- the name of the target column The predict endpoint needs only one parameter: - df -- a serialized dictionary that can be converted into a pandas dataframe The training endpoints must return a json that contains the keys status set to ok . The predict endpoint must return a dictionayr containing the prediction key, storing the predictions. Additional keys can be returned for confidence and confidence intervals. Once you start this ray serve wrapped model you can train it via the query: CREATE PREDICTOR byom FROM mydb ( SELECT number_of_rooms, initial_price, rental_price FROM test_data.home_rentals ) PREDICT number_of_rooms USING url.train = 'http://127.0.0.1:8000/my_model/train', url.predict = 'http://127.0.0.1:8000/my_model/predict', dtype_dict={\"number_of_rooms\": \"categorical\", \"initial_price\": \"integer\", \"rental_price\": \"integer\"}, format='ray_server'; And you can query predictions as usual: SELECT * FROM byom WHERE initial_price=3000 AND rental_price=3000; or by JOINING SELECT tb.number_of_rooms, t.rental_price FROM mydb.test_data.home_rentals AS t JOIN mindsdb.byom AS tb WHERE t.rental_price > 5300;","title":"Ray Server"},{"location":"sql/tutorials/byom/#mlflow","text":"MLFlow is a tool that you can use to train and serve models. As training is done through code rather than the API, that bit you will have to do from outside of mindsdb by pulling your data manually. The first step would be to install mlflow and get a model going, you can use the one in this very simple tutorial: https://github.com/mlflow/mlflow#saving-and-serving-models Next, we're going to add this to mindsdb: CREATE PREDICTOR byom2 PREDICT `1` USING url.predict='http://localhost:5000/invocations', format='mlflow', data_dtype={\"0\": \"integer\", \"1\": \"integer\"} Then we can run predictions as usual, by using the WHERE statement or joining on a data table with an appropriate schema: SELECT y FROM byom2 WHERE `0`=2;","title":"MLFlow"},{"location":"sql/tutorials/crop-prediction/","text":"Crop Recomendation Dataset: Crop recomendation Data Communtiy Author: pixpack Modern agriculture is becoming very dependent on technology. From advanced machinery to specially selected crops. All the technology produces a lot of data that can be used for better adjustment of the farming process. One use case of machine learning in agriculture could be the selection of the best crop for a specific field to maximize the potential yield. Such problems are often called Classification Problems in machine learning. With MindsDB you can easily make automated machine learning predictions straight from your existing database. Even without advanced ML engineering skills, you can start leveraging predictive models that help you make better business decisions. In this tutorial, you will learn how to predict the best crop type based on field parameters using MindsDB and MariaDB . Pre-requisites Before you start make sure you have: Access to MindsDB. In this tutorial, we will use MindsDB Cloud . If you want you can also deploy mindsdb on your premises, Check out the installation guide for Docker or PyPi . Downloaded the dataset. You can get it from Kaggle . Access to a mysql-client. Docker Add your file to MindsDB MindsDB can integrates with many databases, in most scenarios your data will be stored in a database, if you decide to load this dataset into your database of choice, please follow instructions here as to how to connect mindsdb to your database . In this tutorial, you simply upload the kaggle file Crop_recommendation.csv to MindsDB via the MindsDB adming GUI, In this tutorial, we are using cloud.mindsdb.com . Alternatively, remember that if you are using a local deployment you will have to point your browser to 127.0.0.1:47334 . In the main screen, select FILES > FILE UPLOAD . Then add your file and give it a name, in this tutorial we will name it crops . Click Upload , you should now see your file on the list of files. \ud83d\ude80 NOTE: Mindsdb SQL Server allows you to query your uploaded files using SQL, we will get to that right now. Connecting to your MindsDB SQL Server In this section you will connect to MindsDB with the MySQL API and explore what is there for you. First you need to connect to MindsDB through the MySQL API. To do so, use the following command. Remember to change the MindsDB Cloud username for the connection mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p Note: If you are using a local deployment, please review Connect to your Local deployment After that you can list the provided databases. SHOW databases ; You will notice that there is a database called files . USE files ; You can explore your data SELECT * from crops limit 4 ; Create a predictor Now we can create a machine learning model with crops columns serving as features, and MindsDB takes care of the rest of ML workflow automatically. There is a way to get your hands into the insides of the model to fine tune it, but we will not cover it in this tutorial. Switch to the mindsdb database. USE mindsdb ; Use the following query to create a predictor that will predict the label ( crop type ) for the specific field parameters. CREATE PREDICTOR crop_predictor FROM files ( SELECT * FROM crops ) PREDICT label as crop_type ; After creating the predictor you should see a similar output: Query OK, 0 rows affected (11.66 sec) Now the predictor will begin training. You can check the status of the predictor with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'crop_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | crop_predictor | complete | 0.9954545454545454 | label | up_to_date | 2.55.2 | | | | +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ 1 row in set (0.29 sec) You are now done with creating the predictor! \u2728 Make predictions In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT label FROM mindsdb . crop_predictor WHERE N = 77 and P = 52 and K = 17 and temperature = 24 and humidity = 20 . 74 and ph = 5 . 71 and rainfall = 75 . 82 label: maize 1 row in set (0.32 sec) As we have used a real data point from our dataset we can verify the prediction. N, P, K, temperature, humidity, ph, rainfall, label 77, 52, 17, 24.86374934, 65.7420046, 5.714799723, 75.82270467, maize As you can see, the model correctly predicted the most appropriate crop type for our field. OK, we made a prediction using a single query, but what if you want to make a batch prediction for a large set of data in your database? In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how it works. Use the following command to create the batch prediction. SELECT collected_data . N , collected_data . P , collected_data . K , collected_data . temperature , collected_data . humidity , collected_data . ph , collected_data . rainfall , predictions . label as predicted_crop_type FROM crops_integration . crops AS collected_data JOIN mindsdb . crop_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. +------+------+------+-------------+----------+------+----------+---------------------+ | N | P | K | temperature | humidity | ph | rainfall | predicted_crop_type | +------+------+------+-------------+----------+------+----------+---------------------+ | 90 | 42 | 43 | 21 | 82.0 | 6.5 | 202.94 | rice | | 85 | 58 | 41 | 22 | 80.32 | 7.04 | 226.66 | rice | | 60 | 55 | 44 | 23 | 82.32 | 7.84 | 263.96 | rice | | 74 | 35 | 40 | 26 | 80.16 | 6.98 | 242.86 | rice | | 78 | 42 | 42 | 20 | 81.6 | 7.63 | 262.72 | rice | +------+------+------+-------------+----------+------+----------+---------------------+ You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more check out other tutorials and MindsDB documentation .","title":"Crop Recommendation"},{"location":"sql/tutorials/crop-prediction/#crop-recomendation","text":"Dataset: Crop recomendation Data Communtiy Author: pixpack Modern agriculture is becoming very dependent on technology. From advanced machinery to specially selected crops. All the technology produces a lot of data that can be used for better adjustment of the farming process. One use case of machine learning in agriculture could be the selection of the best crop for a specific field to maximize the potential yield. Such problems are often called Classification Problems in machine learning. With MindsDB you can easily make automated machine learning predictions straight from your existing database. Even without advanced ML engineering skills, you can start leveraging predictive models that help you make better business decisions. In this tutorial, you will learn how to predict the best crop type based on field parameters using MindsDB and MariaDB .","title":"Crop Recomendation"},{"location":"sql/tutorials/crop-prediction/#pre-requisites","text":"Before you start make sure you have: Access to MindsDB. In this tutorial, we will use MindsDB Cloud . If you want you can also deploy mindsdb on your premises, Check out the installation guide for Docker or PyPi . Downloaded the dataset. You can get it from Kaggle . Access to a mysql-client. Docker","title":"Pre-requisites"},{"location":"sql/tutorials/crop-prediction/#add-your-file-to-mindsdb","text":"MindsDB can integrates with many databases, in most scenarios your data will be stored in a database, if you decide to load this dataset into your database of choice, please follow instructions here as to how to connect mindsdb to your database . In this tutorial, you simply upload the kaggle file Crop_recommendation.csv to MindsDB via the MindsDB adming GUI, In this tutorial, we are using cloud.mindsdb.com . Alternatively, remember that if you are using a local deployment you will have to point your browser to 127.0.0.1:47334 . In the main screen, select FILES > FILE UPLOAD . Then add your file and give it a name, in this tutorial we will name it crops . Click Upload , you should now see your file on the list of files. \ud83d\ude80 NOTE: Mindsdb SQL Server allows you to query your uploaded files using SQL, we will get to that right now.","title":"Add your file to MindsDB"},{"location":"sql/tutorials/crop-prediction/#connecting-to-your-mindsdb-sql-server","text":"In this section you will connect to MindsDB with the MySQL API and explore what is there for you. First you need to connect to MindsDB through the MySQL API. To do so, use the following command. Remember to change the MindsDB Cloud username for the connection mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p Note: If you are using a local deployment, please review Connect to your Local deployment After that you can list the provided databases. SHOW databases ; You will notice that there is a database called files . USE files ; You can explore your data SELECT * from crops limit 4 ;","title":"Connecting to your MindsDB SQL Server"},{"location":"sql/tutorials/crop-prediction/#create-a-predictor","text":"Now we can create a machine learning model with crops columns serving as features, and MindsDB takes care of the rest of ML workflow automatically. There is a way to get your hands into the insides of the model to fine tune it, but we will not cover it in this tutorial. Switch to the mindsdb database. USE mindsdb ; Use the following query to create a predictor that will predict the label ( crop type ) for the specific field parameters. CREATE PREDICTOR crop_predictor FROM files ( SELECT * FROM crops ) PREDICT label as crop_type ; After creating the predictor you should see a similar output: Query OK, 0 rows affected (11.66 sec) Now the predictor will begin training. You can check the status of the predictor with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'crop_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | crop_predictor | complete | 0.9954545454545454 | label | up_to_date | 2.55.2 | | | | +-----------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ 1 row in set (0.29 sec) You are now done with creating the predictor! \u2728","title":"Create a predictor"},{"location":"sql/tutorials/crop-prediction/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT label FROM mindsdb . crop_predictor WHERE N = 77 and P = 52 and K = 17 and temperature = 24 and humidity = 20 . 74 and ph = 5 . 71 and rainfall = 75 . 82 label: maize 1 row in set (0.32 sec) As we have used a real data point from our dataset we can verify the prediction. N, P, K, temperature, humidity, ph, rainfall, label 77, 52, 17, 24.86374934, 65.7420046, 5.714799723, 75.82270467, maize As you can see, the model correctly predicted the most appropriate crop type for our field. OK, we made a prediction using a single query, but what if you want to make a batch prediction for a large set of data in your database? In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how it works. Use the following command to create the batch prediction. SELECT collected_data . N , collected_data . P , collected_data . K , collected_data . temperature , collected_data . humidity , collected_data . ph , collected_data . rainfall , predictions . label as predicted_crop_type FROM crops_integration . crops AS collected_data JOIN mindsdb . crop_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. +------+------+------+-------------+----------+------+----------+---------------------+ | N | P | K | temperature | humidity | ph | rainfall | predicted_crop_type | +------+------+------+-------------+----------+------+----------+---------------------+ | 90 | 42 | 43 | 21 | 82.0 | 6.5 | 202.94 | rice | | 85 | 58 | 41 | 22 | 80.32 | 7.04 | 226.66 | rice | | 60 | 55 | 44 | 23 | 82.32 | 7.84 | 263.96 | rice | | 74 | 35 | 40 | 26 | 80.16 | 6.98 | 242.86 | rice | | 78 | 42 | 42 | 20 | 81.6 | 7.63 | 262.72 | rice | +------+------+------+-------------+----------+------+----------+---------------------+ You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more check out other tutorials and MindsDB documentation .","title":"Make predictions"},{"location":"sql/tutorials/customer-churn/","text":"MindsDB as a Machine Learning framework can help marketing, sales, and customer retention teams determine the best incentive and the right time to make an offer to minimize customer turnover. In this tutorial you will learn how to use SQL queries to train a machine learning model and make predictions in three simple steps: Connect a database with customer's data to MindsDB. Use an CREATE PREDICTOR statement to train the machine learning model automatically. Query predictions with a simple SELECT statement from MindsDB AI Table (this special table returns data from an ML model upon being queried). Using SQL to perform machine learning at the data layer will bring you many benefits like removing unnecessary ETL-ing, seamless integration with your data, and enabling predictive analytics in your BI tool. Let's see how this works with a real world example to predict the probability of churn for a new customer of a telecom company. Note: You can follow up this tutorial by connecting to your own database and using different data - the same workflow applies to most machine learning use cases. Pre-requisites First, make sure you have successfully installed MindsDB. Check out the installation guide for Docker or PyPi install. Second, you will need to have mysql-client or DBeaver, MySQL WOrkbench etc installed locally to connect to MySQL API. Database Connection First, we need to connect MindsDB to the database where the Customer Churn data is stored. In the left navigation click on Database. Next, click on the ADD DATABASE. Here, we need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration Database - the database name Host - database host name Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API and train a new model that shall predict customer churn. Connect to MindsDB\u2019s MySQL API I will use a mysql command line client in the next part of the tutorial but you can follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MySQL API: mysql -h cloud.mindsdb.com --port 3306 -u theusername@mail.com -p In the above command, we specify the hostname and user name explicitly, as well as a password for connecting. If you got the above screen that means you have successfully connected. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. Data Overview In this tutorial, we will use the customer churn data-set . Each row represents a customer and we will train a machine learning model to help us predict if the customer is going to stop using the company products. Below is a short description of each feature inside the data. CustomerId - Customer ID Gender - Male or Female customer SeniorCitizen - Whether the customer is a senior citizen or not (1, 0) Partner - Whether the customer has a partner or not (Yes, No) Dependents - Whether the customer has dependents or not (Yes, No) Tenure - Number of months the customer has stayed with the company PhoneService - Whether the customer has a phone service or not (Yes, No) MultipleLines - Whether the customer has multiple lines or not (Yes, No, No phone service) InternetService - Customer\u2019s internet service provider (DSL, Fiber optic, No) OnlineSecurity - Whether the customer has online security or not (Yes, No, No internet service) OnlineBackup - Whether the customer has online backup or not (Yes, No, No internet service) DeviceProtection - Whether the customer has device protection or not (Yes, No, No internet service) TechSupport - Whether the customer has tech support or not (Yes, No, No internet service) StreamingTv - Whether the customer has streaming TV or not (Yes, No, No internet service) StreamingMovies - Whether the customer has streaming movies or not (Yes, No, No internet service) Contract - The contract term of the customer (Month-to-month, One year, Two year) PaperlessBilling - Whether the customer has paperless billing or not (Yes, No) PaymentMethod - The customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges - The monthly charge amount TotalCharges - The total amount charged to the customer Churn - Whether the customer churned or not (Yes or No). This is what we want to predict. Using SQL Statements to train/query models Now, we will train a new machine learning model from the datasource we have created using MindsDB Studio. Switch back to mysql-client and run: use mindsdb; show tables; You will notice there are 2 tables available inside the MindsDB database. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table as: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name as column_alias ; The required values that we need to provide are: predictor_name (string) - The name of the model integration_name (string) - The name of connection to your database. column_name (string) - The feature you want to predict. To train the model that will predict customer churn run: CREATE PREDICTOR churn_model FROM demo ( SELECT * FROM CustomerChurnData ) PREDICT Churn as customer_churn USING { \"ignore_columns\" : [ \"gender\" ] } ; What we did here was to create a predictor called customer_churn to predict the Churn and also ignore the gender column as an irrelevant column for the model. Also note that the ID columns in this case customerId will be automatically detected by MindsDB and ignored. The model training has started. To check if the training has finished you can SELECT the model name from the predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'churn_model' ; The complete status means that the model training has successfully finished. The next steps would be to query the model and predict the customer churn. Let\u2019s be creative and imagine a customer. Customer will use only DSL service, no phone service and multiple lines, she was with the company for 1 month and has a partner. Add all of this information to the WHERE clause. SELECT Churn , Churn_confidence , Churn_explain as Info FROM mindsdb . churn_model WHERE when_data = '{\"SeniorCitizen\": 0, \"Partner\": \"Yes\", \"Dependents\": \"No\", \"tenure\": 1, \"PhoneService\": \"No\", \"MultipleLines\": \"No phone service\", \"InternetService\": \"DSL\"}' ; With the confidence of around 82% MindsDB predicted that this customer will churn. One important thing to check here is the important_missing_information value, where MindsDB is pointing to the important missing information for giving a more accurate prediction, in this case, Contract, MonthlyCharges, TotalCharges and OnlineBackup. Let\u2019s include those values in the WHERE clause, and run a new query: SELECT Churn , Churn_confidence , Churn_explain as Info FROM mindsdb . churn_model WHERE when_data = '{\"SeniorCitizen\": 0, \"Partner\": \"Yes\", \"Dependents\": \"No\", \"tenure\": 1, \"PhoneService\": \"No\", \"MultipleLines\": \"No phone service\", \"InternetService\": \"DSL\", \"OnlineSecurity\": \"No\", \"OnlineBackup\": \"Yes\", \"DeviceProtection\": \"No\", \"TechSupport\": \"No\", \"StreamingTV\": \"No\", \"StreamingMovies\": \"No\", \"Contract\": \"Month-to-month\", \"PaperlessBilling\": \"Yes\", \"PaymentMethod\": \"Electronic check\", \"MonthlyCharges\": 29.85, \"TotalCharges\": 29.85}' ;","title":"Customer Churn"},{"location":"sql/tutorials/customer-churn/#pre-requisites","text":"First, make sure you have successfully installed MindsDB. Check out the installation guide for Docker or PyPi install. Second, you will need to have mysql-client or DBeaver, MySQL WOrkbench etc installed locally to connect to MySQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/customer-churn/#database-connection","text":"First, we need to connect MindsDB to the database where the Customer Churn data is stored. In the left navigation click on Database. Next, click on the ADD DATABASE. Here, we need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration Database - the database name Host - database host name Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API and train a new model that shall predict customer churn.","title":"Database Connection"},{"location":"sql/tutorials/customer-churn/#connect-to-mindsdbs-mysql-api","text":"I will use a mysql command line client in the next part of the tutorial but you can follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MySQL API: mysql -h cloud.mindsdb.com --port 3306 -u theusername@mail.com -p In the above command, we specify the hostname and user name explicitly, as well as a password for connecting. If you got the above screen that means you have successfully connected. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud.","title":"Connect to MindsDB\u2019s MySQL API"},{"location":"sql/tutorials/customer-churn/#data-overview","text":"In this tutorial, we will use the customer churn data-set . Each row represents a customer and we will train a machine learning model to help us predict if the customer is going to stop using the company products. Below is a short description of each feature inside the data. CustomerId - Customer ID Gender - Male or Female customer SeniorCitizen - Whether the customer is a senior citizen or not (1, 0) Partner - Whether the customer has a partner or not (Yes, No) Dependents - Whether the customer has dependents or not (Yes, No) Tenure - Number of months the customer has stayed with the company PhoneService - Whether the customer has a phone service or not (Yes, No) MultipleLines - Whether the customer has multiple lines or not (Yes, No, No phone service) InternetService - Customer\u2019s internet service provider (DSL, Fiber optic, No) OnlineSecurity - Whether the customer has online security or not (Yes, No, No internet service) OnlineBackup - Whether the customer has online backup or not (Yes, No, No internet service) DeviceProtection - Whether the customer has device protection or not (Yes, No, No internet service) TechSupport - Whether the customer has tech support or not (Yes, No, No internet service) StreamingTv - Whether the customer has streaming TV or not (Yes, No, No internet service) StreamingMovies - Whether the customer has streaming movies or not (Yes, No, No internet service) Contract - The contract term of the customer (Month-to-month, One year, Two year) PaperlessBilling - Whether the customer has paperless billing or not (Yes, No) PaymentMethod - The customer\u2019s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic)) MonthlyCharges - The monthly charge amount TotalCharges - The total amount charged to the customer Churn - Whether the customer churned or not (Yes or No). This is what we want to predict.","title":"Data Overview"},{"location":"sql/tutorials/customer-churn/#using-sql-statements-to-trainquery-models","text":"Now, we will train a new machine learning model from the datasource we have created using MindsDB Studio. Switch back to mysql-client and run: use mindsdb; show tables; You will notice there are 2 tables available inside the MindsDB database. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table as: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name as column_alias ; The required values that we need to provide are: predictor_name (string) - The name of the model integration_name (string) - The name of connection to your database. column_name (string) - The feature you want to predict. To train the model that will predict customer churn run: CREATE PREDICTOR churn_model FROM demo ( SELECT * FROM CustomerChurnData ) PREDICT Churn as customer_churn USING { \"ignore_columns\" : [ \"gender\" ] } ; What we did here was to create a predictor called customer_churn to predict the Churn and also ignore the gender column as an irrelevant column for the model. Also note that the ID columns in this case customerId will be automatically detected by MindsDB and ignored. The model training has started. To check if the training has finished you can SELECT the model name from the predictors table: SELECT * FROM mindsdb . predictors WHERE name = 'churn_model' ; The complete status means that the model training has successfully finished. The next steps would be to query the model and predict the customer churn. Let\u2019s be creative and imagine a customer. Customer will use only DSL service, no phone service and multiple lines, she was with the company for 1 month and has a partner. Add all of this information to the WHERE clause. SELECT Churn , Churn_confidence , Churn_explain as Info FROM mindsdb . churn_model WHERE when_data = '{\"SeniorCitizen\": 0, \"Partner\": \"Yes\", \"Dependents\": \"No\", \"tenure\": 1, \"PhoneService\": \"No\", \"MultipleLines\": \"No phone service\", \"InternetService\": \"DSL\"}' ; With the confidence of around 82% MindsDB predicted that this customer will churn. One important thing to check here is the important_missing_information value, where MindsDB is pointing to the important missing information for giving a more accurate prediction, in this case, Contract, MonthlyCharges, TotalCharges and OnlineBackup. Let\u2019s include those values in the WHERE clause, and run a new query: SELECT Churn , Churn_confidence , Churn_explain as Info FROM mindsdb . churn_model WHERE when_data = '{\"SeniorCitizen\": 0, \"Partner\": \"Yes\", \"Dependents\": \"No\", \"tenure\": 1, \"PhoneService\": \"No\", \"MultipleLines\": \"No phone service\", \"InternetService\": \"DSL\", \"OnlineSecurity\": \"No\", \"OnlineBackup\": \"Yes\", \"DeviceProtection\": \"No\", \"TechSupport\": \"No\", \"StreamingTV\": \"No\", \"StreamingMovies\": \"No\", \"Contract\": \"Month-to-month\", \"PaperlessBilling\": \"Yes\", \"PaymentMethod\": \"Electronic check\", \"MonthlyCharges\": 29.85, \"TotalCharges\": 29.85}' ;","title":"Using SQL Statements to train/query models"},{"location":"sql/tutorials/diabetes/","text":"Diabetes Dataset: Diabetes Data Communtiy Author: Chandre Tosca Van Der Westhuizen Diabetes is a metabolic disease that causes high blood sugar and if left untreated can damage your nerves, eyes, kidneys and other organs. It is known as a silent killer, as recent studies have shown that by the year 2040 the world's diabetic patients will reach 642 million. The need to analyse vast medical data to assist in the diagnoses, treatment and management of illnesses is increasing for the medical community. With the rapid development of machine learning, it has been applied to many aspects of medical health and is transforming the health care system. The vitality to intelligently transform information into valuable knowledge through machine learning has become more present in biosciences. With the use of predictive models, MindsDB can assist in classifying diabetic and non-diabetic patients or those who pose a high risk. This is just a small showcase on how MindsDB's machine learning will be able to assist in vastly enhancing the reach of illnesses, thereby making it more efficient and can revolutionize businesses and most importantly the health care system. In this tutorial we will be exploring how we can use a machine learning model to classify negative and positive cases for diabetes. MindsDB allows you to train your model from CSV data directly, however for this tutorial you will: Create & setup a new database. Load sample data into a table using the PGAdmin GUI Tool. Allow connections to the database using Ngrok. Setup a MindsDB connection to your local instance. Create a predictor using SQL. Pre-requisites For this tutorial, Docker is highly recommended. A docker-compose.yml file will be provided to get you started quickly. To ensure you can complete all the steps, make sure you have access to the following tools: A MindsDB instance. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Optional: A PostgreSQL Database. You can install it locally or through Docker . Downloaded the dataset. You can get it from here Access to PGAdmin4 (provided with the docker-compose file). Optional: Access to ngrok. You can check the installation details at the ngrok website . Docker Create a new project directory named e.g. mindsdb-tutorial Inside your project directory: 2.1 Create a new file named docker-compose.yml . 2.2 Create another directory named data . 2.3 Download the CSV datafile and store it in your directory Open the docker-compose.yml file with any text-editor, copy the following and save. version: '3.5' services: postgres: container_name: postgres_container image: postgres environment: POSTGRES_USER: ${ POSTGRES_USER :- postgres } POSTGRES_PASSWORD: ${ POSTGRES_PASSWORD :- changeme } PGDATA: /data/postgres volumes: - postgres:/data/postgres ports: - \"5432:5432\" network_mode: bridge restart: unless-stopped pgadmin: container_name: pgadmin_container image: dpage/pgadmin4 environment: PGADMIN_DEFAULT_EMAIL: ${ PGADMIN_DEFAULT_EMAIL :- pgadmin4 @pgadmin.org } PGADMIN_DEFAULT_PASSWORD: ${ PGADMIN_DEFAULT_PASSWORD :- admin } PGADMIN_CONFIG_SERVER_MODE: 'False' volumes: - pgadmin:/var/lib/pgadmin # Mounts local dir to docker container - ./data:/home ports: - \"5050:80\" network_mode: bridge restart: unless-stopped volumes: postgres: pgadmin: This compose stack features both a Postgres database with PGAdmin4 running on ports 5432 and 5050 respectively. For the database to connect to MindsDB we will use a tunneling program Ngrok to allow the remote database connection without exposing your public IP. Data will be loaded from a csv file using the import tool. Docker Volumes are used for persistent storage of data. To run, open a terminal in the project folder and run docker-compose up -d . If you do not have the Docker images locally, it will first download them from docker hub and might take. Setup the database After docker has started all the services In this section, you will create a Postgres database and a table into which you will then load the dataset. First, connect to your Postgres instance. There are cli options like pgcli . We will use a gui manager; pgAdmin The required details are specified within the compose file. Remember to change the username & password if you have a different one set up in. Open your browser and enter localhost:5050 in the Navigation bar. Setup or enter a master password. Create a new server connection by navigating to the right side of the page and right click on 'Server', select 'Create' and enter the required details. 3.1 For the purpose of the exercise, under the General Tab the Name entered is MIndsDB and the host name/address is localhost under the Connection tab. To create a new database, select the dropdown of the Server and navigate to 'Databases', right click on it and select 'Create' and then 'Databases'.Enter the required details. 4.1 For this exercise, the Database was given the name DIABETES_DATA under the General tab. Select the dropdown of the database created, navigate to 'Schema' and right click on it to select 'Create' and then 'Schema'. Enter the required details. Select the dropdown of the schema created and navigate to 'Tables' to right click on it and select 'Query Tool'. This will allow you to add a table with SQL code.The below code was used to create the table. CREATE TABLE Diabetes ( Number_of_times_pregnant INT , Plasma_glucose_concentration DECIMAL ( 6 , 3 ) , Diastolic_blood_pressure DECIMAL ( 6 , 3 ) , Triceps_skin_fold_thickness DECIMAL ( 6 , 3 ) , Two_Hour_serum_insulin DECIMAL ( 6 , 3 ) , Body_mass_index DECIMAL ( 6 , 3 ) , Diabetes_pedigree_function DECIMAL ( 6 , 3 ) , Age INT , Class varchar ( 100 ) ); 7. Import Data Before you import the data, please delete the first row as it is a table header. To import rows into your table, right click on the table name and select Import/Export . Under the Options tab, ensure the slider is set to Import . Select your file name and ensure that the delimiters are set. 8. Select the data By making a select query, the data that has been loaded can be verified. Navigate to the table name to right click and select 'SCRIPTS', then choose 'SELECT scripts'. Run the below query and see the results returned. SELECT * FROM \"DIABETES_DATA\" . diabetes ; You can use the below query structure to show your database SELECT * FROM \"Schema_name\" . table_name ; All the rows imported will be retrieved. Congratulations! You have completed setting up a Postgres database. Connect your database to MindsDB GUI Now we are ready to connect your database to MindsDB's GUI. For this exercise, we will be connecting to MindsDB Cloud as it is much more convenient for most users to use. To make our database available to MindsDB, we will run ngrok as our database instance is local. The following command can be run in docker or a terminal on your device to set up a ngrok tunnel. ngrok tcp [ db-port ] For this example the port number used is 5432. You should see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://x.tcp.ngrok.io:15076 -> localhost:5432 The information required is by the forwarded address, next to 'Forwarding' select and copy x.tcp.ngrok.io:15076 Once you have copied the information, you can add it to the information requested by the MindsDB GUI which we will get to in a moment. For the next steps we will log into the MindsDB cloud interface. MindsDB Cloud is perfect if you are unable to install MindsDB on your device. If you are not registered yet, feel free to follow the below guide. If you have already registered, skip the next steps to connect your database. MindsDB Studio Cloud Set Up You can visit this link and follow the steps in registering a MindsDB Cloud account. On the landing page, navigate to the left corner of the screen and select ADD DATABASE . Enter the required details. Example is below Click Connect , you should now see your Postgres database connection in the main screen. You are now done with connecting MindsDB to your database! \ud83d\ude80 Create a predictor Now we are ready to create our own predictor! We will start by using the MySQL API to connect to MindsDB and with a single SQL command create a predictor. The predictor we will create will be trained to determine negative and positive cases for diabetes. Predictors are great machine learning models when working with large datasets and optimal for determining classifications. Using the following command, you will connect through the MySQL API to MindsDB. Please note that the username and password to use will be the credentials you registered your MindsDB account with. mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p If you are successfully connected, make sure you connect to MindsDB's database. USE mindsdb ; Use the following query to create a predictor that will predict the class ( positive or negative ) for the specific field parameters. CREATE PREDICTOR diabetes_predictor FROM Diabetes ( SELECT * FROM \"DIABETES_DATA\" . diabetes ) PREDICT class ; After creating the predictor you should see a similar output: Query OK, 0 rows affected (8.09 sec) The predictor was created successfully and has started training. To check the status of the model, use the below query. SELECT * FROM mindsdb . predictors WHERE name = 'diabetes_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. mysql> SELECT * FROM mindsdb.predictors WHERE name='diabetes_predictor'; +--------------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | +--------------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | diabetes_predictor | complete | 0.6546310832025117 | class | up_to_date | 2.61.0 | NULL | | | +--------------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ 1 row in set (0.57 sec) The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728 Make predictions In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL query Use the following query using mock data with the predictor. SELECT class FROM mindsdb . diabetes_predictor WHERE when_data = '{\"number_of_times_pregnant\": 0, \"plasma_glucose_concentration\": 135.0, \"diastolic_blood_pressure\": 65.0, \"triceps_skin_fold_thickness\": 30, \"two_Hour_serum_insulin\": 0, \"body_mass_index\": 23.5, \"diabetes_pedigree_function\": 0.366, \"age\": 31}' \\ G The result: class: negative 1 row in set (0.32 sec) *************************** 1. row *************************** Viola! We have successfully created and trained a model and make our own prediction. How easy and amazing is MindsDB? \ud83c\udf89 Want to try it out for yourself? Sign up for a free MindsDB account and join our community! Engage with MindsDB community on Slack or Github to ask questions, share and express ideas and thoughts! For more check out other tutorials and MindsDB documentation .","title":"Diabetes"},{"location":"sql/tutorials/diabetes/#pre-requisites","text":"For this tutorial, Docker is highly recommended. A docker-compose.yml file will be provided to get you started quickly. To ensure you can complete all the steps, make sure you have access to the following tools: A MindsDB instance. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Optional: A PostgreSQL Database. You can install it locally or through Docker . Downloaded the dataset. You can get it from here Access to PGAdmin4 (provided with the docker-compose file). Optional: Access to ngrok. You can check the installation details at the ngrok website .","title":"Pre-requisites"},{"location":"sql/tutorials/diabetes/#docker","text":"Create a new project directory named e.g. mindsdb-tutorial Inside your project directory: 2.1 Create a new file named docker-compose.yml . 2.2 Create another directory named data . 2.3 Download the CSV datafile and store it in your directory Open the docker-compose.yml file with any text-editor, copy the following and save. version: '3.5' services: postgres: container_name: postgres_container image: postgres environment: POSTGRES_USER: ${ POSTGRES_USER :- postgres } POSTGRES_PASSWORD: ${ POSTGRES_PASSWORD :- changeme } PGDATA: /data/postgres volumes: - postgres:/data/postgres ports: - \"5432:5432\" network_mode: bridge restart: unless-stopped pgadmin: container_name: pgadmin_container image: dpage/pgadmin4 environment: PGADMIN_DEFAULT_EMAIL: ${ PGADMIN_DEFAULT_EMAIL :- pgadmin4 @pgadmin.org } PGADMIN_DEFAULT_PASSWORD: ${ PGADMIN_DEFAULT_PASSWORD :- admin } PGADMIN_CONFIG_SERVER_MODE: 'False' volumes: - pgadmin:/var/lib/pgadmin # Mounts local dir to docker container - ./data:/home ports: - \"5050:80\" network_mode: bridge restart: unless-stopped volumes: postgres: pgadmin: This compose stack features both a Postgres database with PGAdmin4 running on ports 5432 and 5050 respectively. For the database to connect to MindsDB we will use a tunneling program Ngrok to allow the remote database connection without exposing your public IP. Data will be loaded from a csv file using the import tool. Docker Volumes are used for persistent storage of data. To run, open a terminal in the project folder and run docker-compose up -d . If you do not have the Docker images locally, it will first download them from docker hub and might take.","title":"Docker"},{"location":"sql/tutorials/diabetes/#setup-the-database","text":"After docker has started all the services In this section, you will create a Postgres database and a table into which you will then load the dataset. First, connect to your Postgres instance. There are cli options like pgcli . We will use a gui manager; pgAdmin The required details are specified within the compose file. Remember to change the username & password if you have a different one set up in. Open your browser and enter localhost:5050 in the Navigation bar. Setup or enter a master password. Create a new server connection by navigating to the right side of the page and right click on 'Server', select 'Create' and enter the required details. 3.1 For the purpose of the exercise, under the General Tab the Name entered is MIndsDB and the host name/address is localhost under the Connection tab. To create a new database, select the dropdown of the Server and navigate to 'Databases', right click on it and select 'Create' and then 'Databases'.Enter the required details. 4.1 For this exercise, the Database was given the name DIABETES_DATA under the General tab. Select the dropdown of the database created, navigate to 'Schema' and right click on it to select 'Create' and then 'Schema'. Enter the required details. Select the dropdown of the schema created and navigate to 'Tables' to right click on it and select 'Query Tool'. This will allow you to add a table with SQL code.The below code was used to create the table. CREATE TABLE Diabetes ( Number_of_times_pregnant INT , Plasma_glucose_concentration DECIMAL ( 6 , 3 ) , Diastolic_blood_pressure DECIMAL ( 6 , 3 ) , Triceps_skin_fold_thickness DECIMAL ( 6 , 3 ) , Two_Hour_serum_insulin DECIMAL ( 6 , 3 ) , Body_mass_index DECIMAL ( 6 , 3 ) , Diabetes_pedigree_function DECIMAL ( 6 , 3 ) , Age INT , Class varchar ( 100 ) ); 7. Import Data Before you import the data, please delete the first row as it is a table header. To import rows into your table, right click on the table name and select Import/Export . Under the Options tab, ensure the slider is set to Import . Select your file name and ensure that the delimiters are set. 8. Select the data By making a select query, the data that has been loaded can be verified. Navigate to the table name to right click and select 'SCRIPTS', then choose 'SELECT scripts'. Run the below query and see the results returned. SELECT * FROM \"DIABETES_DATA\" . diabetes ; You can use the below query structure to show your database SELECT * FROM \"Schema_name\" . table_name ; All the rows imported will be retrieved. Congratulations! You have completed setting up a Postgres database.","title":"Setup the database"},{"location":"sql/tutorials/diabetes/#connect-your-database-to-mindsdb-gui","text":"Now we are ready to connect your database to MindsDB's GUI. For this exercise, we will be connecting to MindsDB Cloud as it is much more convenient for most users to use. To make our database available to MindsDB, we will run ngrok as our database instance is local. The following command can be run in docker or a terminal on your device to set up a ngrok tunnel. ngrok tcp [ db-port ] For this example the port number used is 5432. You should see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://x.tcp.ngrok.io:15076 -> localhost:5432 The information required is by the forwarded address, next to 'Forwarding' select and copy x.tcp.ngrok.io:15076 Once you have copied the information, you can add it to the information requested by the MindsDB GUI which we will get to in a moment. For the next steps we will log into the MindsDB cloud interface. MindsDB Cloud is perfect if you are unable to install MindsDB on your device. If you are not registered yet, feel free to follow the below guide. If you have already registered, skip the next steps to connect your database.","title":"Connect your database to MindsDB GUI"},{"location":"sql/tutorials/diabetes/#mindsdb-studio-cloud-set-up","text":"You can visit this link and follow the steps in registering a MindsDB Cloud account. On the landing page, navigate to the left corner of the screen and select ADD DATABASE . Enter the required details. Example is below Click Connect , you should now see your Postgres database connection in the main screen. You are now done with connecting MindsDB to your database! \ud83d\ude80","title":"MindsDB Studio Cloud Set Up"},{"location":"sql/tutorials/diabetes/#create-a-predictor","text":"Now we are ready to create our own predictor! We will start by using the MySQL API to connect to MindsDB and with a single SQL command create a predictor. The predictor we will create will be trained to determine negative and positive cases for diabetes. Predictors are great machine learning models when working with large datasets and optimal for determining classifications. Using the following command, you will connect through the MySQL API to MindsDB. Please note that the username and password to use will be the credentials you registered your MindsDB account with. mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p If you are successfully connected, make sure you connect to MindsDB's database. USE mindsdb ; Use the following query to create a predictor that will predict the class ( positive or negative ) for the specific field parameters. CREATE PREDICTOR diabetes_predictor FROM Diabetes ( SELECT * FROM \"DIABETES_DATA\" . diabetes ) PREDICT class ; After creating the predictor you should see a similar output: Query OK, 0 rows affected (8.09 sec) The predictor was created successfully and has started training. To check the status of the model, use the below query. SELECT * FROM mindsdb . predictors WHERE name = 'diabetes_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. mysql> SELECT * FROM mindsdb.predictors WHERE name='diabetes_predictor'; +--------------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | +--------------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ | diabetes_predictor | complete | 0.6546310832025117 | class | up_to_date | 2.61.0 | NULL | | | +--------------------+----------+--------------------+---------+---------------+-----------------+-------+-------------------+------------------+ 1 row in set (0.57 sec) The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728","title":"Create a predictor"},{"location":"sql/tutorials/diabetes/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL query Use the following query using mock data with the predictor. SELECT class FROM mindsdb . diabetes_predictor WHERE when_data = '{\"number_of_times_pregnant\": 0, \"plasma_glucose_concentration\": 135.0, \"diastolic_blood_pressure\": 65.0, \"triceps_skin_fold_thickness\": 30, \"two_Hour_serum_insulin\": 0, \"body_mass_index\": 23.5, \"diabetes_pedigree_function\": 0.366, \"age\": 31}' \\ G The result: class: negative 1 row in set (0.32 sec) *************************** 1. row *************************** Viola! We have successfully created and trained a model and make our own prediction. How easy and amazing is MindsDB? \ud83c\udf89 Want to try it out for yourself? Sign up for a free MindsDB account and join our community! Engage with MindsDB community on Slack or Github to ask questions, share and express ideas and thoughts! For more check out other tutorials and MindsDB documentation .","title":"Make predictions"},{"location":"sql/tutorials/heart-disease/","text":"Cardiovascular disease remains the leading cause of morbidity and mortality according to the National Center for Health Statistics in the United States, and consequently, early diagnosis is of paramount importance. Machine learning technology, a subfield of artificial intelligence, is enabling scientists, clinicians and patients to detect it in the earlier stages and therefore save lives. Until now, building, deploying and maintaining applied machine learning solutions was a complicated and expensive task, because it required skilled personnel and expensive tools. But not only that. A traditional machine learning project requires building integrations with data and applications, that is not only a technical but also an organizational challenge. So what if machine learning can become a part of the standard tools that are already in use? This is exactly the problem that MindsDB is solving. It makes machine learning easy to use by automating and integrating it into the mainstream instruments for data and analytics, namely databases and business intelligence software. It adds an AI \u201cbrain\u201d to databases so that they can learn automatically from existing data, allowing you to generate and visualize predictions using standard data query language like SQL. Lastly, MindsDB is open-source, and anyone can use it for free. In this article, we will show step by step how to use MindsDB inside databases to predict the risk of heart disease for patients. You can follow this tutorial by connecting to your own database and using different data - the same workflow applies to most machine learning use cases. Let\u2019s get started! \u200b Pre-requisites \u200b If you want to install MindsDB locally, check out the installation guide for Docker or PyPi and you can follow this tutorial. If you are OK with using MindsDB cloud, then simply create a free account and you will be up and running in just one minute. Second, you will need to have a mysql client like DBeaver, MySQL Workbench etc. installed locally to connect to the MindsDB MySQL API. \u200b Connect to your data \u200b First, we need to connect MindsDB to the database where the Heart Disease data is stored. Open MindsDB GUI and in the left navigation bar click on Database. Next, click on the ADD DATABASE button. Here, we need to provide all of the required parameters for connecting to the database. \u200b * Supported Database - select the database that you want to connect to * Integrations Name - add a name to the integration * Database - the database name * Host - database host name * Port - database port * Username - database user * Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API and train a new model that shall predict the risk of heart disease for a certain patient. \u200b How to use MindsDB \u200b MindsDB allows you to automatically create & train machine learning models from the data in your database that you have connected to in the previous step. MindsDB works via MySQL wire protocol, which means you can do all these steps through SQL commands. When it comes to making predictions, SQL queries become even handier, because you can easily make them straight from your existing applications or Business Intelligence tools that already speak SQL. The ML models are available to use immediately after being trained as if they were virtual database tables (a concept called \u201cAI Tables\u201d). So, let\u2019s see how it works. The first step is to connect to MindsDB\u2019s MySQL API. Go to your MySQL client and execute: \u200b mysql -h cloud.mindsdb.com --port 3306 -u theusername@mail.com -p \u200b In the above command, we specify the hostname and user name, as well as a password for connecting. If you use a local instance of MindsDB you have to specify its parameters. Please refer to the documentation . If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. \u200b Data Overview \u200b For the example of this tutorial, we will use the heart disease dataset available publicly in Kaggle . Each row represents a patient and we will train a machine learning model to help us predict if the patient is classified as a heart disease patient. Below is a short description of each feature inside the data. \u200b * age - In Years * sex - 1 = Male; 0 = Female * cp - chest pain type (4 values) * trestbps - Resting blood pressure (in mm Hg on admission to the hospital) * chol - Serum cholesterol in mg/dl * fbs - Fasting blood sugar > 120 mg/dl (1 = true; 0 = false) * restecg - Resting electrocardiographic results * thalach - Maximum heart rate achieved * exang - Exercise induced angina (1 = yes; 0 = no) * oldpeak - ST depression induced by exercise relative to rest * slope - the slope of the peak exercise ST segment * ca - Number of major vessels (0-3) colored by fluoroscopy * thal - 1 = normal; 2 = fixed defect; 3 = reversible defect * target - 1 or 0 (This is what we will predict) \u200b Using SQL Statements to automatically train ML models \u200b Now, we will train a new machine learning model from the datasource we have created. Go to your mysql-client and run: \u200b use mindsdb; show tables; \u200b You will notice there are 2 tables available inside the MindsDB database. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table as: \u200b CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name as column_alias ; \u200b The required values that we need to provide are: \u200b * predictor_name (string) - The name of the model * integration_name (string) - The name of the connection to your database. * column_name (string) - The feature you want to predict. \u200b To train the model that will predict the risk of heart disease as target run: \u200b CREATE PREDICTOR patients_target FROM db_integration ( SELECT * FROM HeartDiseaseData ) PREDICT target USING { \"ignore_columns\" : [ \"sex\" ] } ; \u200b \u200b What we did here was to create a predictor called patients_target to predict the presence of heart disease as target and also ignore the sex column as an irrelevant column for the model. The model has started training. To check if the training has finished you can SELECT the model name from the predictors table: \u200b SELECT * FROM mindsdb . predictors WHERE name = 'patients_target' ; \u200b The complete status means that the model training has successfully finished. Using SQL Statements to make predictions \u200b \u200bThe next steps would be to query the model and predict the heart disease risk. Let\u2019s imagine a patient. This patient\u2019s age is 30, she has a cholesterol level of 177 mg/dl, with slope of the peak exercise ST segment as 2, and thal as 2. Add all of this information to the WHERE clause. \u200b SELECT target as prediction , target_confidence as confidence , target_explain as info FROM mindsdb . patients_target WHERE when_data = '{\"age\": 30, \"chol\": 177, \"slope\": 2, \"thal\": 2}' ; \u200b With a confidence of around 99%, MindsDB predicted a high risk of heart disease for this patient. The above example shows how you can make predictions for a single patient. But what if you have a table in your database with many patients\u2019 diagnosis data, and you want to make predictions for them in bulk? For this purpose, you can join the predictor with such a table. SELECT * FROM db_integration . HeartDiseaseData AS t JOIN mindsdb . patients_target AS tb WHERE t . thal in ( '2' ); \u200b Now you can even connect the output table to your BI tool and for more convenient visualization of the results using graphs or pivots. Conclusion In this tutorial, you have seen how easy it is to apply machine learning for your predictive needs. MindsDB's innovative open-source technology is making it easy to leverage machine learning for people who are not experts in this field. However, MindsDB is a great tool for ML practitioners as well: if you are a skilled data scientist, you could also benefit from the convenience of deploying custom machine learning solutions within databases by building & configuring models manually through a declarative syntax called JSON-AI . There are other interesting ML use cases where MindsDB is positioned extremely well, like multivariate time-series and real-time data streams, so feel free to check it yourself .","title":"Heart Disease"},{"location":"sql/tutorials/heart-disease/#pre-requisites","text":"\u200b If you want to install MindsDB locally, check out the installation guide for Docker or PyPi and you can follow this tutorial. If you are OK with using MindsDB cloud, then simply create a free account and you will be up and running in just one minute. Second, you will need to have a mysql client like DBeaver, MySQL Workbench etc. installed locally to connect to the MindsDB MySQL API. \u200b","title":"Pre-requisites"},{"location":"sql/tutorials/heart-disease/#connect-to-your-data","text":"\u200b First, we need to connect MindsDB to the database where the Heart Disease data is stored. Open MindsDB GUI and in the left navigation bar click on Database. Next, click on the ADD DATABASE button. Here, we need to provide all of the required parameters for connecting to the database. \u200b * Supported Database - select the database that you want to connect to * Integrations Name - add a name to the integration * Database - the database name * Host - database host name * Port - database port * Username - database user * Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API and train a new model that shall predict the risk of heart disease for a certain patient. \u200b","title":"Connect to your data"},{"location":"sql/tutorials/heart-disease/#how-to-use-mindsdb","text":"\u200b MindsDB allows you to automatically create & train machine learning models from the data in your database that you have connected to in the previous step. MindsDB works via MySQL wire protocol, which means you can do all these steps through SQL commands. When it comes to making predictions, SQL queries become even handier, because you can easily make them straight from your existing applications or Business Intelligence tools that already speak SQL. The ML models are available to use immediately after being trained as if they were virtual database tables (a concept called \u201cAI Tables\u201d). So, let\u2019s see how it works. The first step is to connect to MindsDB\u2019s MySQL API. Go to your MySQL client and execute: \u200b mysql -h cloud.mindsdb.com --port 3306 -u theusername@mail.com -p \u200b In the above command, we specify the hostname and user name, as well as a password for connecting. If you use a local instance of MindsDB you have to specify its parameters. Please refer to the documentation . If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. \u200b","title":"How to use MindsDB"},{"location":"sql/tutorials/heart-disease/#data-overview","text":"\u200b For the example of this tutorial, we will use the heart disease dataset available publicly in Kaggle . Each row represents a patient and we will train a machine learning model to help us predict if the patient is classified as a heart disease patient. Below is a short description of each feature inside the data. \u200b * age - In Years * sex - 1 = Male; 0 = Female * cp - chest pain type (4 values) * trestbps - Resting blood pressure (in mm Hg on admission to the hospital) * chol - Serum cholesterol in mg/dl * fbs - Fasting blood sugar > 120 mg/dl (1 = true; 0 = false) * restecg - Resting electrocardiographic results * thalach - Maximum heart rate achieved * exang - Exercise induced angina (1 = yes; 0 = no) * oldpeak - ST depression induced by exercise relative to rest * slope - the slope of the peak exercise ST segment * ca - Number of major vessels (0-3) colored by fluoroscopy * thal - 1 = normal; 2 = fixed defect; 3 = reversible defect * target - 1 or 0 (This is what we will predict) \u200b","title":"Data Overview"},{"location":"sql/tutorials/heart-disease/#using-sql-statements-to-automatically-train-ml-models","text":"\u200b Now, we will train a new machine learning model from the datasource we have created. Go to your mysql-client and run: \u200b use mindsdb; show tables; \u200b You will notice there are 2 tables available inside the MindsDB database. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table as: \u200b CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) PREDICT column_name as column_alias ; \u200b The required values that we need to provide are: \u200b * predictor_name (string) - The name of the model * integration_name (string) - The name of the connection to your database. * column_name (string) - The feature you want to predict. \u200b To train the model that will predict the risk of heart disease as target run: \u200b CREATE PREDICTOR patients_target FROM db_integration ( SELECT * FROM HeartDiseaseData ) PREDICT target USING { \"ignore_columns\" : [ \"sex\" ] } ; \u200b \u200b What we did here was to create a predictor called patients_target to predict the presence of heart disease as target and also ignore the sex column as an irrelevant column for the model. The model has started training. To check if the training has finished you can SELECT the model name from the predictors table: \u200b SELECT * FROM mindsdb . predictors WHERE name = 'patients_target' ; \u200b The complete status means that the model training has successfully finished.","title":"Using SQL Statements to automatically train ML models"},{"location":"sql/tutorials/heart-disease/#using-sql-statements-to-make-predictions","text":"\u200b \u200bThe next steps would be to query the model and predict the heart disease risk. Let\u2019s imagine a patient. This patient\u2019s age is 30, she has a cholesterol level of 177 mg/dl, with slope of the peak exercise ST segment as 2, and thal as 2. Add all of this information to the WHERE clause. \u200b SELECT target as prediction , target_confidence as confidence , target_explain as info FROM mindsdb . patients_target WHERE when_data = '{\"age\": 30, \"chol\": 177, \"slope\": 2, \"thal\": 2}' ; \u200b With a confidence of around 99%, MindsDB predicted a high risk of heart disease for this patient. The above example shows how you can make predictions for a single patient. But what if you have a table in your database with many patients\u2019 diagnosis data, and you want to make predictions for them in bulk? For this purpose, you can join the predictor with such a table. SELECT * FROM db_integration . HeartDiseaseData AS t JOIN mindsdb . patients_target AS tb WHERE t . thal in ( '2' ); \u200b Now you can even connect the output table to your BI tool and for more convenient visualization of the results using graphs or pivots.","title":"Using SQL Statements to make predictions"},{"location":"sql/tutorials/heart-disease/#conclusion","text":"In this tutorial, you have seen how easy it is to apply machine learning for your predictive needs. MindsDB's innovative open-source technology is making it easy to leverage machine learning for people who are not experts in this field. However, MindsDB is a great tool for ML practitioners as well: if you are a skilled data scientist, you could also benefit from the convenience of deploying custom machine learning solutions within databases by building & configuring models manually through a declarative syntax called JSON-AI . There are other interesting ML use cases where MindsDB is positioned extremely well, like multivariate time-series and real-time data streams, so feel free to check it yourself .","title":"Conclusion"},{"location":"sql/tutorials/insurance-cost-prediction/","text":"Predict Insurance Cost using MindsDB Dataset: Medical Cost Personal Data Communtiy Author: Kinie K Kusuma Pre-requisites First, you need MindsDB installed. So please make sure you've visited Getting Started Guide and Getting Started with Cloud . You may start to use MindsDB by installing it locally or you can use the Cloud service. Let\u2019s use the cloud for this tutorial. Second, you need a MySQL client to connect to the MindsDB MySQL API. Can you accurately predict insurance costs? In this tutorial, you will learn how to predict insurance costs using MindsDB. This tutorial is very easy because you don't need to learn any machine learning algorithms, all you need to know is just SQL. The process looks like the following: First we will connect MindsDB to a database with past data so it can learn from it We will use a single SQL command that will tell MindsDB to train its predictor We will use the standard SQL Select statement to get predictions from AI Tables in MindsDB. Like if this data already exists! MindsDB will execute a complete Machine Learning workflow behind the scenes, it will determine data types for each column, normalize and encode it, train and test ML model. All this happens automatically, so it is very cool! Those who want to get their hands dirty with manual hyperparameters optimization, you can also do that with MindsDB using a declarative syntax called JSON-AI. So let's look at how it works using a real use case. For the demo purpose we will use a public dataset from Kaggle, but you are free to follow this tutorial with your own data. Connect your database First, you need to connect MindsDB to the database where the data is stored. Open MindsDB GUI and in the left navigation click on Database, then click on the ADD DATABASE. Here, you need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it differently Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction. Connect to MindsDB\u2019s MySQL API Here I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud. Data Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and because the tutorial is about insurance cost prediction, we will use the insurance table. Let's check what is inside this table. So, these tables have 7 columns: age: The age of the person (integer) sex: Gender (male or female) bmi: Body mass index is a value derived from the mass and height of a person. The BMI is defined as the body mass divided by the square of the body height, and is expressed in units of kg/m\u00b2, resulting from mass in kilograms and height in meters (float) children: The number of children (integer) smoker: Indicator if the person smoke (yes or no) region: Region where the insured lives (southeast, northeast, southwest or northwest) charges: The insurance cost, this is the target of prediction (float) Create the model Now, to create the model, let's move to the MindsDB database, and see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table, and using this command: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_name (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. So, use this command to create the models: If there's no error, that means your model is created and training has started. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the predictor is ready, it will look like this. The model has been created and trained! The reported accuracy is 75%. If you want to have more control over the model, head to lightwood.io to see how that can be customized. Make prediction Now you are in the last step of this tutorial, making the prediction. To make a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ; You need to set these values: target_variable - The original value of the target variable. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. when_data - The data to make the predictions from(WHERE clause params). Finally, we have trained an insurance model using SQL and MindsDB. Conclusions As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself, MindsDB has an option of a free cloud account that is more than enough to give it a try.","title":"Insurance Cost"},{"location":"sql/tutorials/insurance-cost-prediction/#predict-insurance-cost-using-mindsdb","text":"Dataset: Medical Cost Personal Data Communtiy Author: Kinie K Kusuma","title":"Predict Insurance Cost using MindsDB"},{"location":"sql/tutorials/insurance-cost-prediction/#pre-requisites","text":"First, you need MindsDB installed. So please make sure you've visited Getting Started Guide and Getting Started with Cloud . You may start to use MindsDB by installing it locally or you can use the Cloud service. Let\u2019s use the cloud for this tutorial. Second, you need a MySQL client to connect to the MindsDB MySQL API.","title":"Pre-requisites"},{"location":"sql/tutorials/insurance-cost-prediction/#can-you-accurately-predict-insurance-costs","text":"In this tutorial, you will learn how to predict insurance costs using MindsDB. This tutorial is very easy because you don't need to learn any machine learning algorithms, all you need to know is just SQL. The process looks like the following: First we will connect MindsDB to a database with past data so it can learn from it We will use a single SQL command that will tell MindsDB to train its predictor We will use the standard SQL Select statement to get predictions from AI Tables in MindsDB. Like if this data already exists! MindsDB will execute a complete Machine Learning workflow behind the scenes, it will determine data types for each column, normalize and encode it, train and test ML model. All this happens automatically, so it is very cool! Those who want to get their hands dirty with manual hyperparameters optimization, you can also do that with MindsDB using a declarative syntax called JSON-AI. So let's look at how it works using a real use case. For the demo purpose we will use a public dataset from Kaggle, but you are free to follow this tutorial with your own data.","title":"Can you accurately predict insurance costs?"},{"location":"sql/tutorials/insurance-cost-prediction/#connect-your-database","text":"First, you need to connect MindsDB to the database where the data is stored. Open MindsDB GUI and in the left navigation click on Database, then click on the ADD DATABASE. Here, you need to provide all of the required parameters for connecting to the database. Supported Database - select the database that you want to connect to Integrations Name - add a name to the integration, here I'm using 'mysql' but you can name it differently Database - the database name Host - database hostname Port - database port Username - database user Password - user's password Then, click on CONNECT. The next step is to use the MySQL client to connect to MindsDB\u2019s MySQL API, train a new model, and make a prediction.","title":"Connect your database"},{"location":"sql/tutorials/insurance-cost-prediction/#connect-to-mindsdbs-mysql-api","text":"Here I'm using MySQL command-line client, but you can also follow up with the one that works the best for you, like Dbeaver. The first step is to use the MindsDB Cloud user to connect to the MindsDB MySQL API, using this command: You need to specify the hostname and user name explicitly, as well as a password for connecting. Click enter and you are connected to MindsDB API. If you have an authentication error, please make sure you are providing the email address you have used to create an account on MindsDB Cloud.","title":"Connect to MindsDB\u2019s MySQL API"},{"location":"sql/tutorials/insurance-cost-prediction/#data","text":"Now, let's show the databases. There are 4 databases, and the MySQL database is the database that I've connected to MindsDB. Let's check the MySQL database. There are 3 tables, and because the tutorial is about insurance cost prediction, we will use the insurance table. Let's check what is inside this table. So, these tables have 7 columns: age: The age of the person (integer) sex: Gender (male or female) bmi: Body mass index is a value derived from the mass and height of a person. The BMI is defined as the body mass divided by the square of the body height, and is expressed in units of kg/m\u00b2, resulting from mass in kilograms and height in meters (float) children: The number of children (integer) smoker: Indicator if the person smoke (yes or no) region: Region where the insured lives (southeast, northeast, southwest or northwest) charges: The insurance cost, this is the target of prediction (float)","title":"Data"},{"location":"sql/tutorials/insurance-cost-prediction/#create-the-model","text":"Now, to create the model, let's move to the MindsDB database, and see what's inside. There are 2 tables, predictors, and commands. Predictors contain your predictors record, and commands contain your last commands used. To train a new machine learning model we will need to CREATE Predictor as a new record inside the predictors table, and using this command: CREATE PREDICTOR predictor_name FROM integration_name ( SELECT column_name , column_name2 FROM table_name ) as ds_name PREDICT column_name as column_alias ; The values that we need to provide are: predictor_name (string) - The name of the model. integration_name (string) - The name of the connection to your database. ds_name (string) - the name of the dataset you want to create, it's optional if you don't specify this value MindsDB will generate by itself. column_name (string) - The feature you want to predict. column_alias - Alias name of the feature you want to predict. So, use this command to create the models: If there's no error, that means your model is created and training has started. To see if your model is finished, use this command: SELECT * FROM mindsdb . predictors WHERE name = predictor_name ; And values that we need to provide are: predictor_name (string) - The name of the model. If the predictor is ready, it will look like this. The model has been created and trained! The reported accuracy is 75%. If you want to have more control over the model, head to lightwood.io to see how that can be customized.","title":"Create the model"},{"location":"sql/tutorials/insurance-cost-prediction/#make-prediction","text":"Now you are in the last step of this tutorial, making the prediction. To make a prediction you can use this command: SELECT target_variable , target_variable_explain FROM model_table WHERE when_data = '{\"column3\": \"value\", \"column2\": \"value\"}' ; You need to set these values: target_variable - The original value of the target variable. target_variable_confidence - Model confidence score. target_variable_explain - JSON object that contains additional information as confidence_lower_bound, confidence_upper_bound, anomaly, truth. when_data - The data to make the predictions from(WHERE clause params). Finally, we have trained an insurance model using SQL and MindsDB.","title":"Make prediction"},{"location":"sql/tutorials/insurance-cost-prediction/#conclusions","text":"As you can see it is very easy to start making predictions with machine learning even without being a data scientist! Feel free to check this yourself, MindsDB has an option of a free cloud account that is more than enough to give it a try.","title":"Conclusions"},{"location":"sql/tutorials/mindsdb-superset-snowflake/","text":"Using MindsDB Machine Learning to Solve a Real-World time series Problem Let\u2019s use these powerful AI tables in a real-world scenario. (if you are not familiar with AI-Tables, you can learn about them in here . Imagine that you are a data analyst at the Chicago Transit Authority. Every day, you need to optimize the number of buses per route to avoid overcrowded or empty buses. You need machine learning to forecast the number of rides per bus, per route, and by time of day. The data you have looks like the table below with route_id, timestamp, number of rides, and day-type (W = weekend) This is a difficult machine learning problem that is common in databases. A timestamp indicates that we are dealing with the time-series problem. The data is further complicated by the type of day (day-type) the row contains and this is called multivariate. Additionally, there is high-cardinality as each route will have multiple row entries each with different timestamps, rides, and day types. Let\u2019s see how we can use machine learning with MindsDB to optimize the number of buses per route and visualize the results. Set Up MindsDB First things first! You need to connect your database to MindsDB. One of the easy ways to do so is to create a MindsDB cloud account. If you prefer to deploy MindsDB locally, please refer to installation instructions via Docker or PyPI . Once an account is created you can connect to Snowflake using standard parameters like database name (in this case the Chicago Transit Authority), host, port, username, password, etc. Connect MindsDB to the Data for model training MindsDB works through a MySQL Wire protocol. Therefore, you can connect to it using any MySQL client. Here, we\u2019ll use the DBeaver database client and can see the Snowflake databases we are connected to. Step 1: Getting the Training Data We start by getting the training data from the database that we connected to our MindsDB cloud account. It is always good to first make sure that all the databases are present and the connections correct. show databases ; MindsDB comes with some built-in databases as follows: INFORMATION_SCHEMA stores information about MindsDB, MINDSDB stores metadata about the predictors and allows access to the created predictors as tables, DATASOURCE for connecting to data or uploading files. The SNF database is the database of the Chicago Transit Authority that we connected. It provides us with the training data. Let\u2019s check it. SELECT * FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST LIMIT 100 ; The training data consists of the number of rides per bus route and day. For example, on 2001-07-03, there were 7354 rides on bus route 3. Step 2: Training the Predictive Model Let\u2019s move on to the next step, which is training the predictive model. For that, we\u2019ll use the MINDSDB database. use mindsdb ; show tables MINDSDB database comes with the predictors and commands tables. The predictors table lets us see the status of our predictive models. For example, assuming that we have already trained our predictive model for forecasting the number of rides, we\u2019ll see the following. SELECT name , status FROM MINDSDB . PREDICTORS ; The process of training a predictive model using MindsDB is as simple as creating a view or a table. CREATE PREDICTOR rides_forecaster_demo FROM snf ( SELECT ROUTE , RIDES , DATE FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST WHERE DATE > '2020-01-01' ) PREDICT RIDES ORDER BY DATE GROUP BY ROUTE WINDOW 10 HORIZON 7 ; Let\u2019s discuss the statement above. We create a predictor table using the CREATE PREDICTOR statement and specifying the database from which the training data comes. The code in yellow selects the filtered training data. After that, we use the PREDICT keyword to define the column whose data we want to forecast. Next, there are standard SQL clauses, such as ORDER BY, GROUP BY, WINDOW, and HORIZON . We use the ORDER BY clause and the DATE column as its argument. By doing so, we emphasize that we deal with a time-series problem. We order the rows by date. The GROUP BY clause divides the data into partitions. Here, each of them relates to a particular bus route. We take into account just the last ten rows for every given prediction. Hence, we use WINDOW 10. To prepare the forecast of the number of bus rides for the next week, we define HORIZON 7. Now, you can execute the CREATE PREDICTOR statement and wait until your predictive model is complete. The MINDSDB.PREDICTORS table stores its name as rides_forecaster_demo and its status as training. Once your predictive model is ready, the status changes to complete. Step 3: Getting the Forecasts We are ready to go to the last step, i.e., using the predictive model to get future data. One way is to query the rides_forecaster_demo predictive model directly. Another way is to join this predictive model table to the table with historical data before querying it. We consider a time-series problem. Therefore, it is better to join our predictive model table to the table with historical data. SELECT tb . ROUTE , tb . RIDES AS PREDICTED_RIDES FROM snf . PUBLIC . CTA_BUS_RIDES_LATEST AS ta JOIN mindsdb . rides_forecaster_demo AS tb WHERE ta . ROUTE = \"171\" AND ta . DATE > LATEST LIMIT 7 ; Let\u2019s analyze it. We join the table that stores historical data (i.e., snf.PUBLIC.CTA_BUS_RIDES_LATEST) to our predictive model table (i.e., mindsdb.rides_forecaster_demo). The queried information is the route and the predicted number of rides per route. And the usage of the condition ta.DATE > LATEST (provided by MindsDB) ensures that we get the future number of rides per route. Let\u2019s run the query above to forecast the number of rides for route 171 in the next seven days. Now we know the number of rides for route 171 in the next seven days. We could do it in the same way for all the other routes. Thanks to the special SQL syntax that includes CREATE PREDICTOR, PREDICT, and > LATEST, MindsDB makes it straightforward to run predictors on our chosen data. Now, let\u2019s visualize our predictions. Visualizing the Results using Apache Superset Apache Superset is a modern, open-source data exploration and visualization platform designed for all data personas in an organization. Superset ships with a powerful SQL editor and a no-code chart builder experience. Superset ships with support for most SQL databases out of the box and over 50 visualization types. You can connect to the Snowflake database or your MindsDB database that has a Snowflake connection within. Upon starting up your Superset workspace, your earlier defined database connection is ready to use! So you have access to the Chicago Transit Authority data, as well as to the predictions made by MindsDB. Visualizing Data The two data sets that we are relevant for visualization are the stops_by_route and forecasts data sets. The stops_by_route data set contains the exact location of each bus stop for each bus route. And the forecasts data set stores the actual and predicted number of rides, confidence interval, and lower and upper bounds of prediction, per route and timestamp. Superset lets us visualize the stops_by_route data set as follows. Every bus route has a different color. Also, there is volatility associated with each bus route. Let\u2019s publish this chart to a new dashboard by clicking the +Save button, then switch to the Save as tab, and then type in \u201cRoutes Dashboard\u201d in the Add to Dashboard field. Now, let\u2019s craft a time-series line chart to visualize actual vs predicted riders. Let\u2019s look at the chart that presents the actual number of bus riders (in blue) and the predicted number of bus rides (in purple). Predictions made by MindsDB closely resemble the actual data, except for a short time during March 2020 when the large-scale lockdowns took place. There we see a sudden drop in the number of bus rides. But MindsDB took some time to cope with this new reality and adjust its predictions. Lastly, let\u2019s add a data zoom to this chart for end-users to zoom in on specific date ranges. Click the Customize tab and then click Data Zoom to enable it. Then, click the + Save button and publish to the same \u201cRoutes Dashboard\u201d. Let\u2019s head over to the dashboard now and customize it to make it more dynamic and explorable. Click Dashboards in the top nav bar and then select \u201cRoutes Dashboard\u201d from the list of dashboards. You can rearrange the chart positions by clicking the pencil icon, dragging the corners of the chart objects, and then clicking Save . Let\u2019s add some dashboard filters to this dashboard so dashboard consumers can filter the charts down to specific bus routes and volatility values. Click the right arrow (->) to pop open the filter tray. Then select the pencil icon to start editing this dashboard\u2019s filters. Create the following filters with appropriate filter names: A Value filter on the route column from the forecasts table. A Numerical range filter on the volatility column from the stops_by_route table. Click Save to publish these filters. Let\u2019s give these filters for a test ride! Use the routes filter to only show information for routes 1, 100, and 1001. We could zoom in to see the time during the first large-scale lockdowns in March 2020. For these particular routes, the predictions made by MindsDB are not so far off. Now, let\u2019s use our volatility filter to view only the routes with volatility values greater than 55. Conclusions: Powerful forecasting with MindsDB, your database, and Superset The combination of MindsDB and your database covers all the phases of the ML lifecycle. And Superset helps you to visualize the data in any form of diagrams, charts, or dashboards. MindsDB provides easy-to-use predictive models through AI Tables. You can create these predictive models using SQL statements and feeding the input data. Also, you can query them the same way you query a table. The easiest way to get started with Superset is with the free tier for Preset Cloud , a hassle-free and fully hosted cloud service for Superset. We encourage you to try some predictions with your own data, so please sign up for a free MindsDB cloud account and if you need any help with MindsDB, feel free to ask our Slack and Github communities.","title":"AI powered forecasts with MindsDB"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#using-mindsdb-machine-learning-to-solve-a-real-world-time-series-problem","text":"Let\u2019s use these powerful AI tables in a real-world scenario. (if you are not familiar with AI-Tables, you can learn about them in here . Imagine that you are a data analyst at the Chicago Transit Authority. Every day, you need to optimize the number of buses per route to avoid overcrowded or empty buses. You need machine learning to forecast the number of rides per bus, per route, and by time of day. The data you have looks like the table below with route_id, timestamp, number of rides, and day-type (W = weekend) This is a difficult machine learning problem that is common in databases. A timestamp indicates that we are dealing with the time-series problem. The data is further complicated by the type of day (day-type) the row contains and this is called multivariate. Additionally, there is high-cardinality as each route will have multiple row entries each with different timestamps, rides, and day types. Let\u2019s see how we can use machine learning with MindsDB to optimize the number of buses per route and visualize the results.","title":"Using MindsDB Machine Learning to Solve a Real-World time series Problem"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#set-up-mindsdb","text":"First things first! You need to connect your database to MindsDB. One of the easy ways to do so is to create a MindsDB cloud account. If you prefer to deploy MindsDB locally, please refer to installation instructions via Docker or PyPI . Once an account is created you can connect to Snowflake using standard parameters like database name (in this case the Chicago Transit Authority), host, port, username, password, etc.","title":"Set Up MindsDB"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#connect-mindsdb-to-the-data-for-model-training","text":"MindsDB works through a MySQL Wire protocol. Therefore, you can connect to it using any MySQL client. Here, we\u2019ll use the DBeaver database client and can see the Snowflake databases we are connected to.","title":"Connect MindsDB to the Data for model training"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-1-getting-the-training-data","text":"We start by getting the training data from the database that we connected to our MindsDB cloud account. It is always good to first make sure that all the databases are present and the connections correct. show databases ; MindsDB comes with some built-in databases as follows: INFORMATION_SCHEMA stores information about MindsDB, MINDSDB stores metadata about the predictors and allows access to the created predictors as tables, DATASOURCE for connecting to data or uploading files. The SNF database is the database of the Chicago Transit Authority that we connected. It provides us with the training data. Let\u2019s check it. SELECT * FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST LIMIT 100 ; The training data consists of the number of rides per bus route and day. For example, on 2001-07-03, there were 7354 rides on bus route 3.","title":"Step 1: Getting the Training Data"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-2-training-the-predictive-model","text":"Let\u2019s move on to the next step, which is training the predictive model. For that, we\u2019ll use the MINDSDB database. use mindsdb ; show tables MINDSDB database comes with the predictors and commands tables. The predictors table lets us see the status of our predictive models. For example, assuming that we have already trained our predictive model for forecasting the number of rides, we\u2019ll see the following. SELECT name , status FROM MINDSDB . PREDICTORS ; The process of training a predictive model using MindsDB is as simple as creating a view or a table. CREATE PREDICTOR rides_forecaster_demo FROM snf ( SELECT ROUTE , RIDES , DATE FROM CHICAGO_TRANSIT_AUTHORITY . PUBLIC . CTA_BUS_RIDES_LATEST WHERE DATE > '2020-01-01' ) PREDICT RIDES ORDER BY DATE GROUP BY ROUTE WINDOW 10 HORIZON 7 ; Let\u2019s discuss the statement above. We create a predictor table using the CREATE PREDICTOR statement and specifying the database from which the training data comes. The code in yellow selects the filtered training data. After that, we use the PREDICT keyword to define the column whose data we want to forecast. Next, there are standard SQL clauses, such as ORDER BY, GROUP BY, WINDOW, and HORIZON . We use the ORDER BY clause and the DATE column as its argument. By doing so, we emphasize that we deal with a time-series problem. We order the rows by date. The GROUP BY clause divides the data into partitions. Here, each of them relates to a particular bus route. We take into account just the last ten rows for every given prediction. Hence, we use WINDOW 10. To prepare the forecast of the number of bus rides for the next week, we define HORIZON 7. Now, you can execute the CREATE PREDICTOR statement and wait until your predictive model is complete. The MINDSDB.PREDICTORS table stores its name as rides_forecaster_demo and its status as training. Once your predictive model is ready, the status changes to complete.","title":"Step 2: Training the Predictive Model"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#step-3-getting-the-forecasts","text":"We are ready to go to the last step, i.e., using the predictive model to get future data. One way is to query the rides_forecaster_demo predictive model directly. Another way is to join this predictive model table to the table with historical data before querying it. We consider a time-series problem. Therefore, it is better to join our predictive model table to the table with historical data. SELECT tb . ROUTE , tb . RIDES AS PREDICTED_RIDES FROM snf . PUBLIC . CTA_BUS_RIDES_LATEST AS ta JOIN mindsdb . rides_forecaster_demo AS tb WHERE ta . ROUTE = \"171\" AND ta . DATE > LATEST LIMIT 7 ; Let\u2019s analyze it. We join the table that stores historical data (i.e., snf.PUBLIC.CTA_BUS_RIDES_LATEST) to our predictive model table (i.e., mindsdb.rides_forecaster_demo). The queried information is the route and the predicted number of rides per route. And the usage of the condition ta.DATE > LATEST (provided by MindsDB) ensures that we get the future number of rides per route. Let\u2019s run the query above to forecast the number of rides for route 171 in the next seven days. Now we know the number of rides for route 171 in the next seven days. We could do it in the same way for all the other routes. Thanks to the special SQL syntax that includes CREATE PREDICTOR, PREDICT, and > LATEST, MindsDB makes it straightforward to run predictors on our chosen data. Now, let\u2019s visualize our predictions.","title":"Step 3: Getting the Forecasts"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#visualizing-the-results-using-apache-superset","text":"Apache Superset is a modern, open-source data exploration and visualization platform designed for all data personas in an organization. Superset ships with a powerful SQL editor and a no-code chart builder experience. Superset ships with support for most SQL databases out of the box and over 50 visualization types. You can connect to the Snowflake database or your MindsDB database that has a Snowflake connection within. Upon starting up your Superset workspace, your earlier defined database connection is ready to use! So you have access to the Chicago Transit Authority data, as well as to the predictions made by MindsDB.","title":"Visualizing the Results using Apache Superset"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#visualizing-data","text":"The two data sets that we are relevant for visualization are the stops_by_route and forecasts data sets. The stops_by_route data set contains the exact location of each bus stop for each bus route. And the forecasts data set stores the actual and predicted number of rides, confidence interval, and lower and upper bounds of prediction, per route and timestamp. Superset lets us visualize the stops_by_route data set as follows. Every bus route has a different color. Also, there is volatility associated with each bus route. Let\u2019s publish this chart to a new dashboard by clicking the +Save button, then switch to the Save as tab, and then type in \u201cRoutes Dashboard\u201d in the Add to Dashboard field. Now, let\u2019s craft a time-series line chart to visualize actual vs predicted riders. Let\u2019s look at the chart that presents the actual number of bus riders (in blue) and the predicted number of bus rides (in purple). Predictions made by MindsDB closely resemble the actual data, except for a short time during March 2020 when the large-scale lockdowns took place. There we see a sudden drop in the number of bus rides. But MindsDB took some time to cope with this new reality and adjust its predictions. Lastly, let\u2019s add a data zoom to this chart for end-users to zoom in on specific date ranges. Click the Customize tab and then click Data Zoom to enable it. Then, click the + Save button and publish to the same \u201cRoutes Dashboard\u201d. Let\u2019s head over to the dashboard now and customize it to make it more dynamic and explorable. Click Dashboards in the top nav bar and then select \u201cRoutes Dashboard\u201d from the list of dashboards. You can rearrange the chart positions by clicking the pencil icon, dragging the corners of the chart objects, and then clicking Save . Let\u2019s add some dashboard filters to this dashboard so dashboard consumers can filter the charts down to specific bus routes and volatility values. Click the right arrow (->) to pop open the filter tray. Then select the pencil icon to start editing this dashboard\u2019s filters. Create the following filters with appropriate filter names: A Value filter on the route column from the forecasts table. A Numerical range filter on the volatility column from the stops_by_route table. Click Save to publish these filters. Let\u2019s give these filters for a test ride! Use the routes filter to only show information for routes 1, 100, and 1001. We could zoom in to see the time during the first large-scale lockdowns in March 2020. For these particular routes, the predictions made by MindsDB are not so far off. Now, let\u2019s use our volatility filter to view only the routes with volatility values greater than 55.","title":"Visualizing Data"},{"location":"sql/tutorials/mindsdb-superset-snowflake/#conclusions-powerful-forecasting-with-mindsdb-your-database-and-superset","text":"The combination of MindsDB and your database covers all the phases of the ML lifecycle. And Superset helps you to visualize the data in any form of diagrams, charts, or dashboards. MindsDB provides easy-to-use predictive models through AI Tables. You can create these predictive models using SQL statements and feeding the input data. Also, you can query them the same way you query a table. The easiest way to get started with Superset is with the free tier for Preset Cloud , a hassle-free and fully hosted cloud service for Superset. We encourage you to try some predictions with your own data, so please sign up for a free MindsDB cloud account and if you need any help with MindsDB, feel free to ask our Slack and Github communities.","title":"Conclusions: Powerful forecasting with MindsDB, your database, and Superset"},{"location":"sql/tutorials/mushrooms/","text":"Mushrooms Dataset: Mushrooms Community Author: Chandre Tosca Van Der Westhuizen Mushrooms are a fleshy sporocarp of fungi which can either be edible or poisonous. It's usage dates back centuries with ancient Greek, Chinese and African cultures. They can have high nutritional value and medicinal properties which provides great health benefits. On the other side,some of these fungi can be toxic and consuming the wrong mushroom can have deadly consequences. It is important for industries across the world, like the food and health sector, to identify which type of mushrooms are edible and which are poisonous. We will explore how MindsDB's machine learning predictive model can make it easier classifying mushrooms and predicting which is safe to consume and which can make you ill. Pre-requisites For this tutorial, ensure you have access to docker Docker. A docker-compose.yml file will be provided to get you started quickly. To ensure you can complete all the steps, make sure you have access to the following tools: A MindsDB instance. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Downloaded the dataset. You can get it from Kaggle Optional: Access to ngrok. You can check the installation details at the ngrok website . Optional- Recommended : Visual Studio Code Docker Create a new project directory named e.g. mindsdb-tutorial Inside your project directory: Create a new file named docker-compose.yml . Open the docker-compose.yml file with any text-editor, copy the following and save. version: '3.5' services: postgres: container_name: mindsdb_postgres_mushrooms image: postgres environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: changeme POSTGRES_DB: Mushrooms PGDATA: /data/postgres volumes: # persist data storage in a docker container - mindsdb_postgres_mushrooms:/data/postgres ports: - \"5433:5432\" network_mode: bridge # restart: none volumes: mindsdb_postgres_mushrooms: This compose stack features both a Postgres database running on ports 5433 and 5432 respectively. We will install Python libraries and use python code in a Jupyter Notebook to create a Postgress database and load data into it in Visual Studio. For the database to connect to MindsDB we will use a tunneling program Ngrok to allow the remote database connection without exposing your public IP. Data will be loaded from a csv file using the import tool. Docker Volumes are used for persistent storage of data. To run, open a terminal in the project folder and run docker-compose up -d . If you do not have the Docker images locally, it will first download them from docker hub and might take a while. Creating a database The goal of creating a database in Visual Studio is to simplify the method used. We have two options- to create a python file with the relevant code in it or create a Jupyter Notebook. For the purpose of this exercise we are going to use Jupyter Notebook which will help execute the code. We will use the libraries sqlalchemy and pandas to create the database and load data into tables. To install the required Python libraries In a terminal, run pip3 install sqlalchemy pandas Please change the pip command if required for your system i.e. pip or python3 -m pip In Visual studio, create a new file with the extension '.ipynb' to create a Jupyter Notebook. Alternatively you can open a text editor and save the following code as a python file. from sqlalchemy import create_engine import pandas as pd # engine = create_engine('sqlite:///college.db', echo = True) username = 'postgres' password = 'changeme' db = 'Mushrooms' tablename = 'mushrooms' # database connection engine = create_engine ( f \"postgresql+psycopg2:// { username } : { password } @localhost:5433/ { db } \" , isolation_level = \"SERIALIZABLE\" , ) file_url = './mushrooms.csv' df = pd . read_csv ( file_url ) df . columns Drop rows with nul values and and store data to table. df . dropna ( inplace = True ) df . to_sql ( tablename , con = engine , if_exists = 'replace' , index = False ) If you are using a python file,open a terminal and execute python3 to run the code. If you are using Jupytor Notebook you can run the code by selecting the play buttons next to the cell blocks. Select python3 when prompted. The dataset consists of the following information: Attribute Information: (classes: edible=e, poisonous=p) *cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s *cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s *cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y *bruises: bruises=t,no=f *odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s *gill-attachment: attached=a,descending=d,free=f,notched=n *gill-spacing: close=c,crowded=w,distant=d *gill-size: broad=b,narrow=n *gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y *stalk-shape: enlarging=e,tapering=t *stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=? *stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s *stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s *stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y *stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y *veil-type: partial=p,universal=u *veil-color: brown=n,orange=o,white=w,yellow=y *ring-number: none=n,one=o,two=t *ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z *spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y *population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y *habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d Accessing the Postgres Database To access the postgres database, add the extension Postgres Management Tool by Chris Kolkman v1.1.19 in Visual studio. A shortcut is to select Ctrl+Shift+X and search for the management tool. Connecting to the database On the sidebar of visual studio, select the icon for the Postgress explorer. Select the + button by the option POSTGRESQL EXPLORER:POSTGRESQL to add a database connection. Add the hostname of the database. For this example, 127.0.0.1 was used. Enter the PostgreSQL user to authenticate. For this example it is postgres. Enter the password of the PostgreSQL user. Add the port number. For this example 5433 was used. Select the type of connection to use- use Secure Connection or Standard Connection. For this example we used Standard Connection. Add the name of the Database to connect to. For this example Mushrooms was used. Add the connection name. You can test the database by navigating to the Postgres Management Tool, selecting the hostname and database dropdowns , then right click on the table name and choose between the select options to return the values. Connecting to MindsDB Cloud We will now connect our database to MindsDB Cloud. First, we will run the ngrok command to establish a connection. Open a terminal and run the following command. ngrok tcp [ db-port ] For this example the port number used is 5433. You should see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://6.tcp.ngrok.io:14789 -> localhost:5433 The forwarded address information will be required when connecting to MindsDB's GUI. Select and copy the 'Forwarding' information, in this case it is 6.tcp.ngrok.io:14789 , and keep this information for later. For the next steps we will log into the MindsDB cloud interface. MindsDB Cloud is perfect if you are unable to install MindsDB on your device. If you are not registered yet, feel free to follow the below guide. If you have already registered, skip the next steps to connect your database. MindsDB Studio Cloud Set Up You can visit this link and follow the steps in registering a MindsDB Cloud account. On the landing page, navigate to the left corner of the screen and select ADD DATABASE . Enter the required details. Too see an example, click here . Click Connect , you should now see your Postgres database connection in the main screen. You are now done with connecting MindsDB to your database! Create a predictor Now we are ready to create our own predictor. We will start by using the MySQL API to connect to MindsDB and with a single SQL command create a predictor. The predictor we will create will be trained to determine if a mushroom is edible or poisonous. Using the following command, you will connect through the MySQL API to MindsDB. Please note that the username and password to use will be the credentials you registered your MindsDB account with. mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p If you are successfully connected, make sure you connect to MindsDB's database. USE mindsdb ; Use the following query to create a predictor that will predict the target_class for the specific field parameters. mysql > CREATE PREDICTOR mushroom_predictor FROM Mushrooms ( SELECT * FROM \"mushrooms\" ) as mushrooms_data PREDICT class ; After creating the predictor you should see a similar output: Query OK, 0 rows affected (12.18 sec) The predictor was created successfully and has started training. To check the status of the model, use the below query. mysql > SELECT * FROM mindsdb . predictors WHERE name = 'mushroom_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. +--------------------+----------+----------+---------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | +--------------------+----------+----------+---------+---------------+-----------------+-------+-------------------+------------------+ | mushroom_predictor | complete | 1.0 | class | up_to_date | 2.61.0 | NULL | | | +--------------------+----------+----------+---------+---------------+-----------------+-------+-------------------+------------------+ 1 row in set (0.61 sec) The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728 Make predictions In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL query Use the following query using mock data with the predictor. SELECT class FROM mindsdb . mushroom_predictor WHERE when_data = '{\"cap-shape\": \"x\",\"cap-surface\": \"s\",\"cap-color\": \"n\",\"bruises\": \"t\",\"odor\": \"p\",\"gill-attachment\": \"f\",\"gill-spacing\": \"c\",\"gill-size\": \"n\",\"gill-color\": \"k\",\"stalk-shape\": \"e\",\"stalk-root\": \"e\",\"stalk-surface-above-ring\": \"s\",\"stalk-surface-below-ring\": \"s\",\"stalk-color-above-ring\": \"w\",\"stalk-color-below-ring\": \"w\",\"veil-type\": \"p\",\"veil-color\": \"w\",\"ring-number\": \"o\",\"ring-type\": \"p\",\"spore-print-color\": \"k\",\"population\": \"s\",\"habitat\": \"u\"}' \\ G The result: *************************** 1. row *************************** class: p 1 row in set (1.19 sec) We have successfully created and trained a model that can predict if a model is edible or poisonous. Want to try it out for yourself? Sign up for a free MindsDB account and join our community! Engage with MindsDB community on Slack or Github to ask questions, share and express ideas and thoughts! For more check out other tutorials and MindsDB documentation .","title":"Mushrooms"},{"location":"sql/tutorials/mushrooms/#mushrooms","text":"Dataset: Mushrooms Community Author: Chandre Tosca Van Der Westhuizen Mushrooms are a fleshy sporocarp of fungi which can either be edible or poisonous. It's usage dates back centuries with ancient Greek, Chinese and African cultures. They can have high nutritional value and medicinal properties which provides great health benefits. On the other side,some of these fungi can be toxic and consuming the wrong mushroom can have deadly consequences. It is important for industries across the world, like the food and health sector, to identify which type of mushrooms are edible and which are poisonous. We will explore how MindsDB's machine learning predictive model can make it easier classifying mushrooms and predicting which is safe to consume and which can make you ill.","title":"Mushrooms"},{"location":"sql/tutorials/mushrooms/#pre-requisites","text":"For this tutorial, ensure you have access to docker Docker. A docker-compose.yml file will be provided to get you started quickly. To ensure you can complete all the steps, make sure you have access to the following tools: A MindsDB instance. Check out the installation guide for Docker or PyPi . You can also use MindsDB Cloud . Downloaded the dataset. You can get it from Kaggle Optional: Access to ngrok. You can check the installation details at the ngrok website . Optional- Recommended : Visual Studio Code","title":"Pre-requisites"},{"location":"sql/tutorials/mushrooms/#docker","text":"Create a new project directory named e.g. mindsdb-tutorial Inside your project directory: Create a new file named docker-compose.yml . Open the docker-compose.yml file with any text-editor, copy the following and save. version: '3.5' services: postgres: container_name: mindsdb_postgres_mushrooms image: postgres environment: POSTGRES_USER: postgres POSTGRES_PASSWORD: changeme POSTGRES_DB: Mushrooms PGDATA: /data/postgres volumes: # persist data storage in a docker container - mindsdb_postgres_mushrooms:/data/postgres ports: - \"5433:5432\" network_mode: bridge # restart: none volumes: mindsdb_postgres_mushrooms: This compose stack features both a Postgres database running on ports 5433 and 5432 respectively. We will install Python libraries and use python code in a Jupyter Notebook to create a Postgress database and load data into it in Visual Studio. For the database to connect to MindsDB we will use a tunneling program Ngrok to allow the remote database connection without exposing your public IP. Data will be loaded from a csv file using the import tool. Docker Volumes are used for persistent storage of data. To run, open a terminal in the project folder and run docker-compose up -d . If you do not have the Docker images locally, it will first download them from docker hub and might take a while.","title":"Docker"},{"location":"sql/tutorials/mushrooms/#creating-a-database","text":"The goal of creating a database in Visual Studio is to simplify the method used. We have two options- to create a python file with the relevant code in it or create a Jupyter Notebook. For the purpose of this exercise we are going to use Jupyter Notebook which will help execute the code. We will use the libraries sqlalchemy and pandas to create the database and load data into tables. To install the required Python libraries In a terminal, run pip3 install sqlalchemy pandas Please change the pip command if required for your system i.e. pip or python3 -m pip In Visual studio, create a new file with the extension '.ipynb' to create a Jupyter Notebook. Alternatively you can open a text editor and save the following code as a python file. from sqlalchemy import create_engine import pandas as pd # engine = create_engine('sqlite:///college.db', echo = True) username = 'postgres' password = 'changeme' db = 'Mushrooms' tablename = 'mushrooms' # database connection engine = create_engine ( f \"postgresql+psycopg2:// { username } : { password } @localhost:5433/ { db } \" , isolation_level = \"SERIALIZABLE\" , ) file_url = './mushrooms.csv' df = pd . read_csv ( file_url ) df . columns Drop rows with nul values and and store data to table. df . dropna ( inplace = True ) df . to_sql ( tablename , con = engine , if_exists = 'replace' , index = False ) If you are using a python file,open a terminal and execute python3 to run the code. If you are using Jupytor Notebook you can run the code by selecting the play buttons next to the cell blocks. Select python3 when prompted. The dataset consists of the following information: Attribute Information: (classes: edible=e, poisonous=p) *cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s *cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s *cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y *bruises: bruises=t,no=f *odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s *gill-attachment: attached=a,descending=d,free=f,notched=n *gill-spacing: close=c,crowded=w,distant=d *gill-size: broad=b,narrow=n *gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y *stalk-shape: enlarging=e,tapering=t *stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=? *stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s *stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s *stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y *stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y *veil-type: partial=p,universal=u *veil-color: brown=n,orange=o,white=w,yellow=y *ring-number: none=n,one=o,two=t *ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z *spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y *population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y *habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d","title":"Creating a database"},{"location":"sql/tutorials/mushrooms/#accessing-the-postgres-database","text":"To access the postgres database, add the extension Postgres Management Tool by Chris Kolkman v1.1.19 in Visual studio. A shortcut is to select Ctrl+Shift+X and search for the management tool.","title":"Accessing the Postgres Database"},{"location":"sql/tutorials/mushrooms/#connecting-to-the-database","text":"On the sidebar of visual studio, select the icon for the Postgress explorer. Select the + button by the option POSTGRESQL EXPLORER:POSTGRESQL to add a database connection. Add the hostname of the database. For this example, 127.0.0.1 was used. Enter the PostgreSQL user to authenticate. For this example it is postgres. Enter the password of the PostgreSQL user. Add the port number. For this example 5433 was used. Select the type of connection to use- use Secure Connection or Standard Connection. For this example we used Standard Connection. Add the name of the Database to connect to. For this example Mushrooms was used. Add the connection name. You can test the database by navigating to the Postgres Management Tool, selecting the hostname and database dropdowns , then right click on the table name and choose between the select options to return the values.","title":"Connecting to the database"},{"location":"sql/tutorials/mushrooms/#connecting-to-mindsdb-cloud","text":"We will now connect our database to MindsDB Cloud. First, we will run the ngrok command to establish a connection. Open a terminal and run the following command. ngrok tcp [ db-port ] For this example the port number used is 5433. You should see a similar output: Session Status online Account myaccount (Plan: Free) Version 2.3.40 Region United States (us) Web Interface http://127.0.0.1:4040 Forwarding tcp://6.tcp.ngrok.io:14789 -> localhost:5433 The forwarded address information will be required when connecting to MindsDB's GUI. Select and copy the 'Forwarding' information, in this case it is 6.tcp.ngrok.io:14789 , and keep this information for later. For the next steps we will log into the MindsDB cloud interface. MindsDB Cloud is perfect if you are unable to install MindsDB on your device. If you are not registered yet, feel free to follow the below guide. If you have already registered, skip the next steps to connect your database.","title":"Connecting to MindsDB Cloud"},{"location":"sql/tutorials/mushrooms/#mindsdb-studio-cloud-set-up","text":"You can visit this link and follow the steps in registering a MindsDB Cloud account. On the landing page, navigate to the left corner of the screen and select ADD DATABASE . Enter the required details. Too see an example, click here . Click Connect , you should now see your Postgres database connection in the main screen. You are now done with connecting MindsDB to your database!","title":"MindsDB Studio Cloud Set Up"},{"location":"sql/tutorials/mushrooms/#create-a-predictor","text":"Now we are ready to create our own predictor. We will start by using the MySQL API to connect to MindsDB and with a single SQL command create a predictor. The predictor we will create will be trained to determine if a mushroom is edible or poisonous. Using the following command, you will connect through the MySQL API to MindsDB. Please note that the username and password to use will be the credentials you registered your MindsDB account with. mysql -h cloud.mindsdb.com --port 3306 -u cloudusername@mail.com -p If you are successfully connected, make sure you connect to MindsDB's database. USE mindsdb ; Use the following query to create a predictor that will predict the target_class for the specific field parameters. mysql > CREATE PREDICTOR mushroom_predictor FROM Mushrooms ( SELECT * FROM \"mushrooms\" ) as mushrooms_data PREDICT class ; After creating the predictor you should see a similar output: Query OK, 0 rows affected (12.18 sec) The predictor was created successfully and has started training. To check the status of the model, use the below query. mysql > SELECT * FROM mindsdb . predictors WHERE name = 'mushroom_predictor' ; After the predictor has finished training, you will see a similar output. Note that MindsDB does model testing for you automatically, so you will immediately see if the predictor is accurate enough. +--------------------+----------+----------+---------+---------------+-----------------+-------+-------------------+------------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | +--------------------+----------+----------+---------+---------------+-----------------+-------+-------------------+------------------+ | mushroom_predictor | complete | 1.0 | class | up_to_date | 2.61.0 | NULL | | | +--------------------+----------+----------+---------+---------------+-----------------+-------+-------------------+------------------+ 1 row in set (0.61 sec) The predictor has completed its training, indicated by the status column, and shows the accuracy of the model. You can revisit training new predictors to increase accuracy by changing the query to better suit the dataset i.e. omitting certain columns etc. Good job! We have successfully created and trained a predictive model \u2728","title":"Create a predictor"},{"location":"sql/tutorials/mushrooms/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. Now we will use the trained model to make predictions using a SQL query Use the following query using mock data with the predictor. SELECT class FROM mindsdb . mushroom_predictor WHERE when_data = '{\"cap-shape\": \"x\",\"cap-surface\": \"s\",\"cap-color\": \"n\",\"bruises\": \"t\",\"odor\": \"p\",\"gill-attachment\": \"f\",\"gill-spacing\": \"c\",\"gill-size\": \"n\",\"gill-color\": \"k\",\"stalk-shape\": \"e\",\"stalk-root\": \"e\",\"stalk-surface-above-ring\": \"s\",\"stalk-surface-below-ring\": \"s\",\"stalk-color-above-ring\": \"w\",\"stalk-color-below-ring\": \"w\",\"veil-type\": \"p\",\"veil-color\": \"w\",\"ring-number\": \"o\",\"ring-type\": \"p\",\"spore-print-color\": \"k\",\"population\": \"s\",\"habitat\": \"u\"}' \\ G The result: *************************** 1. row *************************** class: p 1 row in set (1.19 sec) We have successfully created and trained a model that can predict if a model is edible or poisonous. Want to try it out for yourself? Sign up for a free MindsDB account and join our community! Engage with MindsDB community on Slack or Github to ask questions, share and express ideas and thoughts! For more check out other tutorials and MindsDB documentation .","title":"Make predictions"},{"location":"sql/tutorials/process-quality/","text":"Pre-requisites Before you start make sure that you've: Visited Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. You can get it from Kaggle . Manufacturing process quality Predicting process result quality is a common task in manufacturing analytics. Manufacturing plants commonly use quality predictions to gain a competitive edge over their competitors, improve their products or increase their customers satisfaction. MindsDB is a tool that can help you solve quality prediction tasks easily and effectively using machine learning. MindsDB abstracts ML models as virtual \u201cAI Tables\u201d in databases and you can make predictions just using normal SQL commands. In this tutorial you will learn how to predict the quality of a mining process using MindsDB . Upload a file Fix headers: sed -e 's/ /_/g' -e 's/\\(.*\\)/\\L\\1/' -e 's/%_//g' MiningProcess_Flotation_Plant_Database.csv > fixed_headers.csv (for Linux/Unix) edit headers manually: change space to underscore , upper case to lower case, remove % from headers (for Windows) Click on Files icon to go to datasets page Click on FILE UPLOAD button to upload file into MindsDB Connect to MindsDB SQL Sever mysql - h cloud . mindsdb . com --port 3306 -u username@email.com -p USE mindsdb ; Create a predictor In this section you will connect to MindsDB with the MySql API and create a Predictor. It is in MindsDB terms a machine learning model, but all its complexity is automated and abstracted as a virtual \u201cAI Table\u201d. If you are an ML expert and want to tweak the model, MindsDB also allows you that (please refer to documentation). Use the following query to create a Predictor that will foretell the silica_concentrate at the end of our mining process. The row number is limited to 5000 to speed up training but you can keep the whole dataset. CREATE PREDICTOR process_quality_predictor FROM files ( SELECT iron_feed , silica_feed , starch_flow , amina_flow , ore_pulp_flow , ore_pulp_ph , ore_pulp_density , flotation_column_01_air_flow , flotation_column_02_air_flow , flotation_column_03_air_flow , flotation_column_04_air_flow , flotation_column_05_air_flow , flotation_column_06_air_flow , flotation_column_07_air_flow , flotation_column_01_level , flotation_column_02_level , flotation_column_03_level , flotation_column_04_level , flotation_column_05_level , flotation_column_06_level , flotation_column_07_level , iron_concentrate , silica_concentrate from process_quality FROM process_quality LIMIT 5000 ) PREDICT silica_concentrate as quality USING ; After creating the Predictor you should see a similar output: Query OK, 0 rows affected (2 min 27.52 sec) Now the Predictor will begin training. You can check the status with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'process_quality_predictor' ; After the Predictor has finished training, you will see a similar output. +-----------------------------+----------+----------+--------------------+-------------------+------------------+ | name | status | accuracy | predict | select_data_query | training_options | +-----------------------------+----------+----------+--------------------+-------------------+------------------+ | process_quality_predictor | complete | 1 | silica_concentrate | | | +-----------------------------+----------+----------+--------------------+-------------------+------------------+ 1 row in set (0.28 sec) As you can see the accuracy of the model is 1 (i.e. 100%). This is the result of using a limited dataset of 5000 rows. In reality when using the whole dataset, you will probably see a more reasonable accuracy. You are now done with creating the predictor! \u2728 Make predictions In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT silica_concentrate , silica_concentrate_confidence , silica_concentrate_explain as Info FROM mindsdb . process_quality_predictor WHERE when_data = '{\"iron_feed\": 48.81, \"silica_feed\": 25.31, \"starch_flow\": 2504.94, \"amina_flow\": 309.448, \"ore_pulp_flow\": 377.6511682692, \"ore_pulp_ph\": 10.0607, \"ore_pulp_density\": 1.68676}' ; The output should look similar to this. +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | silica_concentrate | silica_concentrate_confidence | Info | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | 1.68 | 0.99 | {\"predicted_value\": \"1.68\", \"confidence\": 0.99, \"confidence_lower_bound\": null, \"confidence_upper_bound\": null, \"anomaly\": null, \"truth\": null} | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.81 sec) As you can see, the model predicted the silica concentrate for our data point. Again we can see a very high confidence due to the limited dataset. When making predictions you can include different fields. As you can notice, we have only included the first 7 fields of our dataset. You are free to test different combinations. In the previous example, we have made a prediction for a single data point. In a real scenario, you might want to make predictions on multiple data points. In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how to make batch predictions. Use the following command to create the batch prediction. SELECT collected_data . iron_feed , collected_data . silica_feed , collected_data . starch_flow , collected_data . amina_flow , collected_data . ore_pulp_flow , collected_data . ore_pulp_ph , collected_data . ore_pulp_density , predictions . silica_concentrate_confidence as confidence , predictions . silica_concentrate as predicted_silica_concentrate FROM process_quality_integration . process_quality AS collected_data JOIN mindsdb . process_quality_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | iron_feed | silica_feed | starch_flow | amina_flow | ore_pulp_flow | ore_pulp_ph | ore_pulp_density | confidence | predicted_silica_concentrate | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | 58.84 | 11.46 | 3277.34 | 564.209 | 403.242 | 9.88472 | 1.76297 | 0.99 | 2.129567174379606 | | 58.84 | 11.46 | 3333.59 | 565.308 | 401.016 | 9.88543 | 1.76331 | 0.99 | 2.129548423407259 | | 58.84 | 11.46 | 3400.39 | 565.674 | 399.551 | 9.88613 | 1.76366 | 0.99 | 2.130100408285386 | | 58.84 | 11.46 | 3410.55 | 563.843 | 397.559 | 9.88684 | 1.764 | 0.99 | 2.1298757513510136 | | 58.84 | 11.46 | 3408.98 | 559.57 | 401.719 | 9.88755 | 1.76434 | 0.99 | 2.130438907683961 | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this, check out MindsDB documentation .","title":"Process Quality"},{"location":"sql/tutorials/process-quality/#pre-requisites","text":"Before you start make sure that you've: Visited Getting Started Guide Visited Getting Started with Cloud Downloaded the dataset. You can get it from Kaggle .","title":"Pre-requisites"},{"location":"sql/tutorials/process-quality/#manufacturing-process-quality","text":"Predicting process result quality is a common task in manufacturing analytics. Manufacturing plants commonly use quality predictions to gain a competitive edge over their competitors, improve their products or increase their customers satisfaction. MindsDB is a tool that can help you solve quality prediction tasks easily and effectively using machine learning. MindsDB abstracts ML models as virtual \u201cAI Tables\u201d in databases and you can make predictions just using normal SQL commands. In this tutorial you will learn how to predict the quality of a mining process using MindsDB .","title":"Manufacturing process quality"},{"location":"sql/tutorials/process-quality/#upload-a-file","text":"Fix headers: sed -e 's/ /_/g' -e 's/\\(.*\\)/\\L\\1/' -e 's/%_//g' MiningProcess_Flotation_Plant_Database.csv > fixed_headers.csv (for Linux/Unix) edit headers manually: change space to underscore , upper case to lower case, remove % from headers (for Windows) Click on Files icon to go to datasets page Click on FILE UPLOAD button to upload file into MindsDB","title":"Upload a file"},{"location":"sql/tutorials/process-quality/#connect-to-mindsdb-sql-sever","text":"mysql - h cloud . mindsdb . com --port 3306 -u username@email.com -p USE mindsdb ;","title":"Connect to MindsDB SQL Sever"},{"location":"sql/tutorials/process-quality/#create-a-predictor","text":"In this section you will connect to MindsDB with the MySql API and create a Predictor. It is in MindsDB terms a machine learning model, but all its complexity is automated and abstracted as a virtual \u201cAI Table\u201d. If you are an ML expert and want to tweak the model, MindsDB also allows you that (please refer to documentation). Use the following query to create a Predictor that will foretell the silica_concentrate at the end of our mining process. The row number is limited to 5000 to speed up training but you can keep the whole dataset. CREATE PREDICTOR process_quality_predictor FROM files ( SELECT iron_feed , silica_feed , starch_flow , amina_flow , ore_pulp_flow , ore_pulp_ph , ore_pulp_density , flotation_column_01_air_flow , flotation_column_02_air_flow , flotation_column_03_air_flow , flotation_column_04_air_flow , flotation_column_05_air_flow , flotation_column_06_air_flow , flotation_column_07_air_flow , flotation_column_01_level , flotation_column_02_level , flotation_column_03_level , flotation_column_04_level , flotation_column_05_level , flotation_column_06_level , flotation_column_07_level , iron_concentrate , silica_concentrate from process_quality FROM process_quality LIMIT 5000 ) PREDICT silica_concentrate as quality USING ; After creating the Predictor you should see a similar output: Query OK, 0 rows affected (2 min 27.52 sec) Now the Predictor will begin training. You can check the status with the following query. SELECT * FROM mindsdb . predictors WHERE name = 'process_quality_predictor' ; After the Predictor has finished training, you will see a similar output. +-----------------------------+----------+----------+--------------------+-------------------+------------------+ | name | status | accuracy | predict | select_data_query | training_options | +-----------------------------+----------+----------+--------------------+-------------------+------------------+ | process_quality_predictor | complete | 1 | silica_concentrate | | | +-----------------------------+----------+----------+--------------------+-------------------+------------------+ 1 row in set (0.28 sec) As you can see the accuracy of the model is 1 (i.e. 100%). This is the result of using a limited dataset of 5000 rows. In reality when using the whole dataset, you will probably see a more reasonable accuracy. You are now done with creating the predictor! \u2728","title":"Create a predictor"},{"location":"sql/tutorials/process-quality/#make-predictions","text":"In this section you will learn how to make predictions using your trained model. To run a prediction against new or existing data, you can use the following query. SELECT silica_concentrate , silica_concentrate_confidence , silica_concentrate_explain as Info FROM mindsdb . process_quality_predictor WHERE when_data = '{\"iron_feed\": 48.81, \"silica_feed\": 25.31, \"starch_flow\": 2504.94, \"amina_flow\": 309.448, \"ore_pulp_flow\": 377.6511682692, \"ore_pulp_ph\": 10.0607, \"ore_pulp_density\": 1.68676}' ; The output should look similar to this. +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | silica_concentrate | silica_concentrate_confidence | Info | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | 1.68 | 0.99 | {\"predicted_value\": \"1.68\", \"confidence\": 0.99, \"confidence_lower_bound\": null, \"confidence_upper_bound\": null, \"anomaly\": null, \"truth\": null} | +--------------------+-------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.81 sec) As you can see, the model predicted the silica concentrate for our data point. Again we can see a very high confidence due to the limited dataset. When making predictions you can include different fields. As you can notice, we have only included the first 7 fields of our dataset. You are free to test different combinations. In the previous example, we have made a prediction for a single data point. In a real scenario, you might want to make predictions on multiple data points. In this case, MindsDB allows you to Join this other table with the Predictor. In result, you will get another table as an output with a predicted value as one of its columns. Let\u2019s see how to make batch predictions. Use the following command to create the batch prediction. SELECT collected_data . iron_feed , collected_data . silica_feed , collected_data . starch_flow , collected_data . amina_flow , collected_data . ore_pulp_flow , collected_data . ore_pulp_ph , collected_data . ore_pulp_density , predictions . silica_concentrate_confidence as confidence , predictions . silica_concentrate as predicted_silica_concentrate FROM process_quality_integration . process_quality AS collected_data JOIN mindsdb . process_quality_predictor AS predictions LIMIT 5 ; As you can see below, the predictor has made multiple predictions for each data point in the collected_data table! You can also try selecting other fields to get more insight on the predictions. See the JOIN clause documentation for more information. +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | iron_feed | silica_feed | starch_flow | amina_flow | ore_pulp_flow | ore_pulp_ph | ore_pulp_density | confidence | predicted_silica_concentrate | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ | 58.84 | 11.46 | 3277.34 | 564.209 | 403.242 | 9.88472 | 1.76297 | 0.99 | 2.129567174379606 | | 58.84 | 11.46 | 3333.59 | 565.308 | 401.016 | 9.88543 | 1.76331 | 0.99 | 2.129548423407259 | | 58.84 | 11.46 | 3400.39 | 565.674 | 399.551 | 9.88613 | 1.76366 | 0.99 | 2.130100408285386 | | 58.84 | 11.46 | 3410.55 | 563.843 | 397.559 | 9.88684 | 1.764 | 0.99 | 2.1298757513510136 | | 58.84 | 11.46 | 3408.98 | 559.57 | 401.719 | 9.88755 | 1.76434 | 0.99 | 2.130438907683961 | +-----------+-------------+-------------+------------+---------------+-------------+------------------+------------+------------------------------+ You are now done with the tutorial! \ud83c\udf89 Please feel free to try it yourself. Sign up for a free MindsDB account to get up and running in 5 minutes, and if you need any help, feel free to ask in Slack or Github . For more tutorials like this, check out MindsDB documentation .","title":"Make predictions"}]}